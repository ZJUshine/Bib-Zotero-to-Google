
@misc{li_towards_2024,
	title = {Towards {Generalist} {Robot} {Policies}: {What} {Matters} in {Building} {Vision}-{Language}-{Action} {Models}},
	shorttitle = {Towards {Generalist} {Robot} {Policies}},
	url = {http://arxiv.org/abs/2412.14058},
	doi = {10.48550/arXiv.2412.14058},
	abstract = {Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.},
	language = {en},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Li, Xinghang and Li, Peiyan and Liu, Minghuan and Wang, Dong and Liu, Jirong and Kang, Bingyi and Ma, Xiao and Kong, Tao and Zhang, Hanbo and Liu, Huaping},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14058 [cs]
TLDR: The key factors that significantly influence the performance of VLA are disclosed and a detailed guidebook for the future design of VLAs is provided, which supports easy integrations of new VLMs and free combinations of various design choices.
remark: 机器人视觉语言行动模型设计的关键因素。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_vision-language_2024,
	title = {Vision-{Language} {Foundation} {Models} as {Effective} {Robot} {Imitators}},
	url = {http://arxiv.org/abs/2311.01378},
	doi = {10.48550/arXiv.2311.01378},
	abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
	language = {en},
	urldate = {2025-05-17},
	publisher = {arXiv},
	author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
	month = feb,
	year = {2024},
	note = {arXiv:2311.01378 [cs]
TLDR: RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.
remark: RoboFlamingo利用VLMs简化机器人操控。},
	keywords = {\# RoboFlamingo, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{cheang_gr-2_2024,
	title = {{GR}-2: {A} {Generative} {Video}-{Language}-{Action} {Model} with {Web}-{Scale} {Knowledge} for {Robot} {Manipulation}},
	shorttitle = {{GR}-2},
	url = {http://arxiv.org/abs/2410.06158},
	doi = {10.48550/arXiv.2410.06158},
	abstract = {We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7\% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: {\textbackslash}url\{https://gr2-manipulation.github.io\}.},
	language = {en-US},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Cheang, Chi-Lam and Chen, Guangzeng and Jing, Ya and Kong, Tao and Li, Hang and Li, Yifeng and Liu, Yuxiao and Wu, Hongtao and Xu, Jiafeng and Yang, Yichu and Zhang, Hanbo and Zhu, Minzhao},
	month = oct,
	year = {2024},
	note = {GSCC: 0000030 2025-03-07T01:48:06.218Z 
arXiv:2410.06158
remark: GR-2实现通用化机器人操控及出色任务泛化。
TLDR: GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning.},
	keywords = {\# GR-2, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{nvidia_gr00t_2025,
	title = {{GR00T} {N1}: {An} {Open} {Foundation} {Model} for {Generalist} {Humanoid} {Robots}},
	shorttitle = {{GR00T} {N1}},
	url = {http://arxiv.org/abs/2503.14734},
	doi = {10.48550/arXiv.2503.14734},
	abstract = {General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.},
	language = {en},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {NVIDIA and Bjorck, Johan and Castañeda, Fernando and Cherniadev, Nikita and Da, Xingye and Ding, Runyu and Fan, Linxi "Jim" and Fang, Yu and Fox, Dieter and Hu, Fengyuan and Huang, Spencer and Jang, Joel and Jiang, Zhenyu and Kautz, Jan and Kundalia, Kaushil and Lao, Lawrence and Li, Zhiqi and Lin, Zongyu and Lin, Kevin and Liu, Guilin and Llontop, Edith and Magne, Loic and Mandlekar, Ajay and Narayan, Avnish and Nasiriany, Soroush and Reed, Scott and Tan, You Liang and Wang, Guanzhi and Wang, Zu and Wang, Jing and Wang, Qi and Xiang, Jiannan and Xie, Yuqi and Xu, Yinzhen and Xu, Zhenjia and Ye, Seonghyeon and Yu, Zhiding and Zhang, Ao and Zhang, Hao and Zhao, Yizhou and Zheng, Ruijie and Zhu, Yuke},
	month = mar,
	year = {2025},
	note = {arXiv:2503.14734 [cs]
remark: GR00T N1是通用人形机器人开放基础模型。
TLDR: This work introduces GR00T N1, an open foundation model for humanoid robots that outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments and deploys the model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks.},
	keywords = {\# N1, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{dorka_what_2024,
	title = {What {Matters} in {Employing} {Vision} {Language} {Models} for {Tokenizing} {Actions} in {Robot} {Control}?},
	url = {https://openreview.net/forum?id=nfm2qcV1S4},
	abstract = {Vision Language Models (VLMs) have demonstrated remarkable proficiency in comprehending images and text, as well as generating textual outputs based on such inputs, owing to their training on web-scale datasets. Their potential for robotics applications is particularly intriguing. One notable example is RT-2, a system capable of generating low-level actions represented in textual format from a given instruction alongside a sequence of historical actions and image observations. To stimulate further research in this domain, we introduce an open-source implementation tailored for utilizing VLMs in instruction-based robot control. This implementation supports a variety of VLM architectures and facilitates straightforward integration of new models. We use our framework to train multiple VLMs and evaluate them on a physical robot. The results validate the practical efficacy of our framework, thus paving the way for enhanced understanding and capabilities in instruction-based robot control systems. The code is available at: https://github.com/Nicolinho/RoboVLM.},
	language = {en},
	urldate = {2024-06-09},
	author = {Dorka, Nicolai and Huang, Chenguang and Welschehold, Tim and Burgard, Wolfram},
	month = apr,
	year = {2024},
	note = {GSCC: 0000001 
remark: 开源的VLM框架复现RT-2},
}

@misc{bu_univla_2025,
	title = {{UniVLA}: {Learning} to {Act} {Anywhere} with {Task}-centric {Latent} {Actions}},
	shorttitle = {{UniVLA}},
	url = {http://arxiv.org/abs/2505.06111},
	doi = {10.48550/arXiv.2505.06111},
	abstract = {A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.},
	language = {en},
	urldate = {2025-05-13},
	publisher = {arXiv},
	author = {Bu, Qingwen and Yang, Yanting and Cai, Jisong and Gao, Shenyuan and Ren, Guanghui and Yao, Maoqing and Luo, Ping and Li, Hongyang},
	month = may,
	year = {2025},
	note = {arXiv:2505.06111 [cs]
remark: UniVLA提升机器人跨环境任务学习能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kim_fine-tuning_2025,
	title = {Fine-{Tuning} {Vision}-{Language}-{Action} {Models}: {Optimizing} {Speed} and {Success}},
	shorttitle = {Fine-{Tuning} {Vision}-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2502.19645},
	doi = {10.48550/arXiv.2502.19645},
	abstract = {Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5\% to 97.1\% while increasing action generation throughput by 26\${\textbackslash}times\$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs (\${\textbackslash}pi\_0\$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15\% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Finn, Chelsea and Liang, Percy},
	month = feb,
	year = {2025},
	note = {arXiv:2502.19645 [cs]
TLDR: This work proposes an Optimized Fine-Tuning (OFT) recipe, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5\% to 97.1\% while increasing action generation throughput by 26\${\textbackslash}times\$.
remark: 优化视觉语言动作模型以提升任务成功率。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Fine-Tuning Strategies, Machine Learning Optimization, Robot Fine-Tuning, Robotic Control Tasks, Robotic Task Execution, Robotics, Vision-Language-Action Models},
}

@misc{intelligence__05_2025,
	title = {\$π\_\{0.5\}\$: a {Vision}-{Language}-{Action} {Model} with {Open}-{World} {Generalization}},
	shorttitle = {\$π\_\{0.5\}\$},
	url = {http://arxiv.org/abs/2504.16054},
	doi = {10.48550/arXiv.2504.16054},
	abstract = {In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe \${\textbackslash}pi\_\{0.5\}\$, a new model based on \${\textbackslash}pi\_\{0\}\$ that uses co-training on heterogeneous tasks to enable broad generalization. \${\textbackslash}pi\_\{0.5\}\${\textbackslash} uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Intelligence, Physical and Black, Kevin and Brown, Noah and Darpinian, James and Dhabalia, Karan and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Galliker, Manuel Y. and Ghosh, Dibya and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and LeBlanc, Devin and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Ren, Allen Z. and Shi, Lucy Xiaoyang and Smith, Laura and Springenberg, Jost Tobias and Stachowicz, Kyle and Tanner, James and Vuong, Quan and Walke, Homer and Walling, Anna and Wang, Haohuan and Yu, Lili and Zhilinsky, Ury},
	month = apr,
	year = {2025},
	note = {arXiv:2504.16054 [cs]
remark: 新模型\${\textbackslash}pi\_\{0.5\}\$实现机器人广泛任务泛化。},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{qu_spatialvla_2025,
	title = {{SpatialVLA}: {Exploring} {Spatial} {Representations} for {Visual}-{Language}-{Action} {Model}},
	shorttitle = {{SpatialVLA}},
	url = {http://arxiv.org/abs/2501.15830},
	doi = {10.48550/arXiv.2501.15830},
	abstract = {In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced.},
	language = {en},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Qu, Delin and Song, Haoming and Chen, Qizhi and Yao, Yuanqi and Ye, Xinyi and Ding, Yan and Wang, Zhigang and Gu, JiaYuan and Zhao, Bin and Wang, Dong and Li, Xuelong},
	month = jan,
	year = {2025},
	note = {arXiv:2501.15830 [cs]
TLDR: Ego3D Position Encoding is introduced to inject 3D information into the input observations of the visual-language-action model, and Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids are proposed, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control.
remark: SpatialVLA提升机器人操控的空间理解能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{team_gemini_2025,
	title = {Gemini {Robotics}: {Bringing} {AI} into the {Physical} {World}},
	shorttitle = {Gemini {Robotics}},
	url = {http://arxiv.org/abs/2503.20020},
	doi = {10.48550/arXiv.2503.20020},
	abstract = {Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.},
	language = {en},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {Team, Gemini Robotics and Abeyruwan, Saminda and Ainslie, Joshua and Alayrac, Jean-Baptiste and Arenas, Montserrat Gonzalez and Armstrong, Travis and Balakrishna, Ashwin and Baruch, Robert and Bauza, Maria and Blokzijl, Michiel and Bohez, Steven and Bousmalis, Konstantinos and Brohan, Anthony and Buschmann, Thomas and Byravan, Arunkumar and Cabi, Serkan and Caluwaerts, Ken and Casarini, Federico and Chang, Oscar and Chen, Jose Enrique and Chen, Xi and Chiang, Hao-Tien Lewis and Choromanski, Krzysztof and D'Ambrosio, David and Dasari, Sudeep and Davchev, Todor and Devin, Coline and Palo, Norman Di and Ding, Tianli and Dostmohamed, Adil and Driess, Danny and Du, Yilun and Dwibedi, Debidatta and Elabd, Michael and Fantacci, Claudio and Fong, Cody and Frey, Erik and Fu, Chuyuan and Giustina, Marissa and Gopalakrishnan, Keerthana and Graesser, Laura and Hasenclever, Leonard and Heess, Nicolas and Hernaez, Brandon and Herzog, Alexander and Hofer, R. Alex and Humplik, Jan and Iscen, Atil and Jacob, Mithun George and Jain, Deepali and Julian, Ryan and Kalashnikov, Dmitry and Karagozler, M. Emre and Karp, Stefani and Kew, Chase and Kirkland, Jerad and Kirmani, Sean and Kuang, Yuheng and Lampe, Thomas and Laurens, Antoine and Leal, Isabel and Lee, Alex X. and Lee, Tsang-Wei Edward and Liang, Jacky and Lin, Yixin and Maddineni, Sharath and Majumdar, Anirudha and Michaely, Assaf Hurwitz and Moreno, Robert and Neunert, Michael and Nori, Francesco and Parada, Carolina and Parisotto, Emilio and Pastor, Peter and Pooley, Acorn and Rao, Kanishka and Reymann, Krista and Sadigh, Dorsa and Saliceti, Stefano and Sanketi, Pannag and Sermanet, Pierre and Shah, Dhruv and Sharma, Mohit and Shea, Kathryn and Shu, Charles and Sindhwani, Vikas and Singh, Sumeet and Soricut, Radu and Springenberg, Jost Tobias and Sterneck, Rachel and Surdulescu, Razvan and Tan, Jie and Tompson, Jonathan and Vanhoucke, Vincent and Varley, Jake and Vesom, Grace and Vezzani, Giulia and Vinyals, Oriol and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Xia, Fei and Xiao, Ted and Xie, Annie and Xie, Jinyu and Xu, Peng and Xu, Sichun and Xu, Ying and Xu, Zhuo and Yang, Yuxiang and Yao, Rui and Yaroshenko, Sergey and Yu, Wenhao and Yuan, Wentao and Zhang, Jingwei and Zhang, Tingnan and Zhou, Allan and Zhou, Yuxiang},
	month = mar,
	year = {2025},
	note = {arXiv:2503.20020 [cs]
remark: Gemini Robotics实现AI在机器人中的应用。
TLDR: A new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0 is introduced, marking a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.},
	keywords = {Computer Science - Robotics},
}

@misc{zhao_cot-vla_2025,
	title = {{CoT}-{VLA}: {Visual} {Chain}-of-{Thought} {Reasoning} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{CoT}-{VLA}},
	url = {http://arxiv.org/abs/2503.22020},
	doi = {10.48550/arXiv.2503.22020},
	abstract = {Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17\% in real-world manipulation tasks and 6\% in simulation benchmarks. Project website: https://cot-vla.github.io/},
	language = {en-US},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Zhao, Qingqing and Lu, Yao and Kim, Moo Jin and Fu, Zipeng and Zhang, Zhuoyang and Wu, Yecheng and Li, Zhaoshuo and Ma, Qianli and Han, Song and Finn, Chelsea and Handa, Ankur and Liu, Ming-Yu and Xiang, Donglai and Wetzstein, Gordon and Lin, Tsung-Yi},
	month = mar,
	year = {2025},
	note = {arXiv:2503.22020 [cs]
TLDR: This paper introduces CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens and achieves strong performance, outperforming the state-of-the-art VLA model by 17\% in real-world manipulation tasks and 6\% in simulation benchmarks.
remark: 视觉链式推理提升机器人操控效果。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_chatvla_2025,
	title = {{ChatVLA}: {Unified} {Multimodal} {Understanding} and {Robot} {Control} with {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{ChatVLA}},
	url = {http://arxiv.org/abs/2502.14420},
	doi = {10.48550/arXiv.2502.14420},
	abstract = {Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2\% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.},
	language = {en},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Zhou, Zhongyi and Zhu, Yichen and Zhu, Minjie and Wen, Junjie and Liu, Ning and Xu, Zhiyuan and Meng, Weibin and Cheng, Ran and Peng, Yaxin and Shen, Chaomin and Feng, Feifei},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14420 [cs]
remark: ChatVLA提升多模态理解和机器人控制性能。
TLDR: ChatVLA is a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference, which demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action methods on multimodal understanding benchmarks.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xing_towards_2025,
	title = {Towards {Robust} and {Secure} {Embodied} {AI}: {A} {Survey} on {Vulnerabilities} and {Attacks}},
	shorttitle = {Towards {Robust} and {Secure} {Embodied} {AI}},
	url = {http://arxiv.org/abs/2502.13175},
	doi = {10.48550/arXiv.2502.13175},
	abstract = {Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.},
	language = {en},
	urldate = {2025-05-16},
	publisher = {arXiv},
	author = {Xing, Wenpeng and Li, Minghao and Li, Mohan and Han, Meng},
	month = feb,
	year = {2025},
	note = {arXiv:2502.13175 [cs]
TLDR: A comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI is provided, categorizing vulnerabilities specific to embodied AI into exogenous and endogenous origins and proposing targeted strategies to enhance the safety and reliability of embodied AI systems.
remark: 总结了体感AI系统的安全漏洞及增强策略。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Robotics},
}

@misc{jiao_can_2025,
	title = {Can {We} {Trust} {Embodied} {Agents}? {Exploring} {Backdoor} {Attacks} against {Embodied} {LLM}-based {Decision}-{Making} {Systems}},
	shorttitle = {Can {We} {Trust} {Embodied} {Agents}?},
	url = {http://arxiv.org/abs/2405.20774},
	doi = {10.48550/arXiv.2405.20774},
	abstract = {Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100\% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65\%, reaching up to 90\%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Jiao, Ruochen and Xie, Shaoyuan and Yue, Justin and Sato, Takami and Wang, Lixu and Wang, Yixuan and Chen, Qi Alfred and Zhu, Qi},
	month = apr,
	year = {2025},
	note = {arXiv:2405.20774 [cs]
remark: 探讨LLM决策系统的后门攻击框架。},
	keywords = {\# BALD, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{black__0_2024,
	title = {\$π\_0\$: {A} {Vision}-{Language}-{Action} {Flow} {Model} for {General} {Robot} {Control}},
	shorttitle = {\$π\_0\$},
	url = {http://arxiv.org/abs/2410.24164},
	doi = {10.48550/arXiv.2410.24164},
	abstract = {Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.},
	language = {en-US},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
	month = nov,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2410.24164
TLDR: A novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge is proposed and evaluated in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning.
remark: 视觉语言行动模型提升机器人控制能力。},
	keywords = {\# π0, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_mirage_2025,
	title = {Mirage in the {Eyes}: {Hallucination} {Attack} on {Multi}-modal {Large} {Language} {Models} with {Only} {Attention} {Sink}},
	shorttitle = {Mirage in the {Eyes}},
	url = {http://arxiv.org/abs/2501.15269},
	doi = {10.48550/arXiv.2501.15269},
	abstract = {Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Wang, Yining and Zhang, Mi and Sun, Junjie and Wang, Chenyue and Yang, Min and Xue, Hui and Tao, Jialing and Duan, Ranjie and Liu, Jiexi},
	month = jan,
	year = {2025},
	note = {arXiv:2501.15269 [cs]
TLDR: This work proposes a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications.
remark: 多模态大模型幻觉攻击研究。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{shafran_machine_2025,
	title = {Machine {Against} the {RAG}: {Jamming} {Retrieval}-{Augmented} {Generation} with {Blocker} {Documents}},
	shorttitle = {Machine {Against} the {RAG}},
	url = {http://arxiv.org/abs/2406.05870},
	doi = {10.48550/arXiv.2406.05870},
	abstract = {Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query, ostensibly because it lacks relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. Our method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not employ an auxiliary LLM. We evaluate jamming attacks on several embeddings and LLMs and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Shafran, Avital and Schuster, Roei and Shmatikov, Vitaly},
	month = mar,
	year = {2025},
	note = {arXiv:2406.05870 [cs]
TLDR: This work describes and measures the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization that does not rely on instruction injection, does not require the adversary to know the embedding or LLM used by the target RAG system, and does not employ an auxiliary LLM.
remark: RAG系统易受阻塞文档攻击。},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zou_poisonedrag_2024,
	title = {{PoisonedRAG}: {Knowledge} {Corruption} {Attacks} to {Retrieval}-{Augmented} {Generation} of {Large} {Language} {Models}},
	shorttitle = {{PoisonedRAG}},
	url = {http://arxiv.org/abs/2402.07867},
	doi = {10.48550/arXiv.2402.07867},
	abstract = {Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90\% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
	month = aug,
	year = {2024},
	note = {arXiv:2402.07867 [cs]
TLDR: This work proposes PoisonedRAG, a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question.
remark: 提出针对RAG系统的知识腐败攻击方法。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{liu_formalizing_2024,
	title = {Formalizing and {Benchmarking} {Prompt} {Injection} {Attacks} and {Defenses}},
	url = {http://arxiv.org/abs/2310.12815},
	doi = {10.48550/arXiv.2310.12815},
	abstract = {A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
	month = nov,
	year = {2024},
	note = {arXiv:2310.12815 [cs]
TLDR: This work proposes a general framework to formalize prompt injection attacks, and proposes a framework to systematize defenses against prompt injection attacks.
remark: 提出框架系统评估提示注入攻击防御。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{shi_optimization-based_2025,
	title = {Optimization-based {Prompt} {Injection} {Attack} to {LLM}-as-a-{Judge}},
	url = {http://arxiv.org/abs/2403.17710},
	doi = {10.48550/arXiv.2403.17710},
	abstract = {LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies. Our implementation is available at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang},
	month = mar,
	year = {2025},
	note = {arXiv:2403.17710 [cs]
remark: 优化攻击模型欺骗LLM选择攻击者答案。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{wu_image-perfect_2024,
	title = {Image-{Perfect} {Imperfections}: {Safety}, {Bias}, and {Authenticity} in the {Shadow} of {Text}-{To}-{Image} {Model} {Evolution}},
	shorttitle = {Image-{Perfect} {Imperfections}},
	url = {http://arxiv.org/abs/2408.17285},
	doi = {10.48550/arXiv.2408.17285},
	abstract = {Text-to-image models, such as Stable Diffusion (SD), undergo iterative updates to improve image quality and address concerns such as safety. Improvements in image quality are straightforward to assess. However, how model updates resolve existing concerns and whether they raise new questions remain unexplored. This study takes an initial step in investigating the evolution of text-to-image models from the perspectives of safety, bias, and authenticity. Our findings, centered on Stable Diffusion, indicate that model updates paint a mixed picture. While updates progressively reduce the generation of unsafe images, the bias issue, particularly in gender, intensifies. We also find that negative stereotypes either persist within the same Non-White race group or shift towards other Non-White race groups through SD updates, yet with minimal association of these traits with the White race group. Additionally, our evaluation reveals a new concern stemming from SD updates: State-of-the-art fake image detectors, initially trained for earlier SD versions, struggle to identify fake images generated by updated versions. We show that fine-tuning these detectors on fake images generated by updated versions achieves at least 96.6{\textbackslash}\% accuracy across various SD versions, addressing this issue. Our insights highlight the importance of continued efforts to mitigate biases and vulnerabilities in evolving text-to-image models.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Wu, Yixin and Shen, Yun and Backes, Michael and Zhang, Yang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.17285 [cs]
remark: 文本生成图像模型的安全性和偏见问题研究。
TLDR: It is found that negative stereotypes either persist within the same Non-White race group or shift towards other Non-White race groups through SD updates, yet with minimal association of these traits with the White race group.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{ding_understanding_2024,
	title = {Understanding {Implosion} in {Text}-to-{Image} {Generative} {Models}},
	url = {http://arxiv.org/abs/2409.12314},
	doi = {10.1145/3658644.3690205},
	abstract = {Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce "model implosion," where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of "supervised graph alignment" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.},
	urldate = {2025-05-14},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Ding, Wenxin and Li, Cathy Y. and Shan, Shawn and Zhao, Ben Y. and Zheng, Haitao},
	month = dec,
	year = {2024},
	note = {arXiv:2409.12314 [cs]
TLDR: This work establishes the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models, and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric.
remark: 分析文本生成图像模型的中毒攻击机制。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {1211--1225},
}

@misc{dong_jailbreaking_2024,
	title = {Jailbreaking {Text}-to-{Image} {Models} with {LLM}-{Based} {Agents}},
	url = {http://arxiv.org/abs/2408.00523},
	doi = {10.48550/arXiv.2408.00523},
	abstract = {Recent advancements have significantly improved automated task-solving capabilities using autonomous agents powered by large language models (LLMs). However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving their potential for addressing generative AI safety tasks largely unexplored. In this paper, we propose Atlas, an advanced LLM-based multi-agent framework targeting generative AI models, specifically focusing on jailbreak attacks against text-to-image (T2I) models with built-in safety filters. Atlas consists of two agents, namely the mutation agent and the selection agent, each comprising four key modules: a vision-language model (VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses its VLM brain to determine whether a prompt triggers the T2I model's safety filter. It then collaborates iteratively with the LLM brain of the selection agent to generate new candidate jailbreak prompts with the highest potential to bypass the filter. In addition to multi-agent communication, we leverage in-context learning (ICL) memory mechanisms and the chain-of-thought (COT) approach to learn from past successes and failures, thereby enhancing Atlas's performance. Our evaluation demonstrates that Atlas successfully jailbreaks several state-of-the-art T2I models equipped with multi-modal safety filters in a black-box setting. Additionally, Atlas outperforms existing methods in both query efficiency and the quality of generated images. This work convincingly demonstrates the successful application of LLM-based agents in studying the safety vulnerabilities of popular text-to-image generation models. We urge the community to consider advanced techniques like ours in response to the rapidly evolving text-to-image generation field.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Dong, Yingkai and Li, Zheng and Meng, Xiangtao and Yu, Ning and Guo, Shanqing},
	month = sep,
	year = {2024},
	note = {arXiv:2408.00523 [cs]
TLDR: This work convincingly demonstrates the successful application of LLM-based agents in studying the safety vulnerabilities of popular text-to-image generation models, and urges the community to consider advanced techniques like Atlas in response to the rapidly evolving text-to-image generation field.
remark: LLM代理框架Atlas破解T2I模型安全过滤。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{chen_unveiling_2024,
	title = {Unveiling the {Vulnerability} of {Private} {Fine}-{Tuning} in {Split}-{Based} {Frameworks} for {Large} {Language} {Models}: {A} {Bidirectionally} {Enhanced} {Attack}},
	shorttitle = {Unveiling the {Vulnerability} of {Private} {Fine}-{Tuning} in {Split}-{Based} {Frameworks} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2409.00960},
	doi = {10.1145/3658644.3690295},
	abstract = {Recent advancements in pre-trained large language models (LLMs) have significantly influenced various domains. Adapting these models for specific tasks often involves fine-tuning (FT) with private, domain-specific data. However, privacy concerns keep this data undisclosed, and the computational demands for deploying LLMs pose challenges for resource-limited data holders. This has sparked interest in split learning (SL), a Model-as-a-Service (MaaS) paradigm that divides LLMs into smaller segments for distributed training and deployment, transmitting only intermediate activations instead of raw data. SL has garnered substantial interest in both industry and academia as it aims to balance user data privacy, model ownership, and resource challenges in the private fine-tuning of LLMs. Despite its privacy claims, this paper reveals significant vulnerabilities arising from the combination of SL and LLM-FT: the Not-too-far property of fine-tuning and the auto-regressive nature of LLMs. Exploiting these vulnerabilities, we propose Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL. BiSR utilizes pre-trained weights as prior knowledge, combining a learning-based attack with a bidirectional optimization-based approach for highly effective data reconstruction. Additionally, it incorporates a Noise-adaptive Mixture of Experts (NaMoE) model to enhance reconstruction performance under perturbation. We conducted systematic experiments on various mainstream LLMs and different setups, empirically demonstrating BiSR's state-of-the-art performance. Furthermore, we thoroughly examined three representative defense mechanisms, showcasing our method's capability to reconstruct private data even in the presence of these defenses.},
	language = {en},
	urldate = {2025-05-14},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Chen, Guanzhong and Qin, Zhenghan and Yang, Mingxin and Zhou, Yajie and Fan, Tao and Du, Tianyu and Xu, Zenglin},
	month = dec,
	year = {2024},
	note = {arXiv:2409.00960 [cs]
TLDR: Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL, is proposed, showcasing the method's capability to reconstruct private data even in the presence of these defenses.
remark: 揭示LLMs分割学习私密微调的脆弱性。},
	keywords = {Computer Science - Cryptography and Security},
	pages = {2904--2918},
}

@misc{yang_alleviating_2025,
	title = {Alleviating the {Fear} of {Losing} {Alignment} in {LLM} {Fine}-tuning},
	url = {http://arxiv.org/abs/2504.09757},
	doi = {10.48550/arXiv.2504.09757},
	abstract = {Large language models (LLMs) have demonstrated revolutionary capabilities in understanding complex contexts and performing a wide range of tasks. However, LLMs can also answer questions that are unethical or harmful, raising concerns about their applications. To regulate LLMs' responses to such questions, a training strategy called {\textbackslash}textit\{alignment\} can help. Yet, alignment can be unexpectedly compromised when fine-tuning an LLM for downstream tasks. This paper focuses on recovering the alignment lost during fine-tuning. We observe that there are two distinct directions inherent in an aligned LLM: the {\textbackslash}textit\{aligned direction\} and the {\textbackslash}textit\{harmful direction\}. An LLM is inclined to answer questions in the aligned direction while refusing queries in the harmful direction. Therefore, we propose to recover the harmful direction of the fine-tuned model that has been compromised. Specifically, we restore a small subset of the fine-tuned model's weight parameters from the original aligned model using gradient descent. We also introduce a rollback mechanism to avoid aggressive recovery and maintain downstream task performance. Our evaluation on 125 fine-tuned LLMs demonstrates that our method can reduce their harmful rate (percentage of answering harmful questions) from 33.25{\textbackslash}\% to 1.74{\textbackslash}\%, without sacrificing task performance much. In contrast, the existing methods either only reduce the harmful rate to a limited extent or significantly impact the normal functionality. Our code is available at https://github.com/kangyangWHU/LLMAlignment},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Yang, Kang and Tao, Guanhong and Chen, Xun and Xu, Jun},
	month = apr,
	year = {2025},
	note = {arXiv:2504.09757 [cs]
remark: 微调恢复LLM对齐性以减少有害回答。},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{dong_philosophers_2024,
	title = {The {Philosopher}'s {Stone}: {Trojaning} {Plugins} of {Large} {Language} {Models}},
	shorttitle = {The {Philosopher}'s {Stone}},
	url = {http://arxiv.org/abs/2312.00374},
	doi = {10.48550/arXiv.2312.00374},
	abstract = {Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers,an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses a superior LLM to align na{\textbackslash}"ively poisoned data based on our insight that it can better inject poisoning knowledge during training. In contrast, FUSION leverages a novel over-poisoning procedure to transform a benign adapter into a malicious one by magnifying the attention between trigger and target in model weights. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can use malware to control the system (e.g., a LLM-driven robot) or to launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the existing baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we designed and evaluated three potential defenses. However, none proved entirely effective in safeguarding against our attacks, highlighting the need for more robust defenses supporting a secure LLM supply chain.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Dong, Tian and Xue, Minhui and Chen, Guoxing and Holland, Rayne and Meng, Yan and Li, Shaofeng and Liu, Zhen and Zhu, Haojin},
	month = sep,
	year = {2024},
	note = {arXiv:2312.00374 [cs]
TLDR: These novel attacks represent the first study of supply chain threats for LLMs through the lens of Trojan plugins, and demonstrate that an infected adapter can induce, on specific triggers, an LLM to output content defined by an adversary and to even maliciously use tools.
remark: 低秩适配器可被利用控制LLM。},
	keywords = {Computer Science - Cryptography and Security},
}

@article{shen_bait_nodate,
	title = {{BAIT}: {Large} {Language} {Model} {Backdoor} {Scanning} by {Inverting} {Attack} {Target}},
	abstract = {Recent literature has shown that LLMs are vulnerable to backdoor attacks, where malicious attackers inject a secret token sequence (i.e., trigger) into training prompts and enforce their responses to include a speciﬁc target sequence. Unlike discriminative NLP models, which have a ﬁnite output space (e.g., those in sentiment analysis), LLMs are generative models, and their output space grows exponentially with the length of response, thereby posing signiﬁcant challenges to existing backdoor detection techniques, such as trigger inversion. In this paper, we conduct a theoretical analysis of the LLM backdoor learning process under speciﬁc assumptions, revealing that the autoregressive training paradigm in causal language models inherently induces strong causal relationships among tokens in backdoor targets. We hence develop a novel LLM backdoor scanning technique, BAIT (Large Language Model Backdoor ScAnning by Inverting Attack Target). Instead of inverting backdoor triggers like in existing scanning techniques for non-LLMs, BAIT determines if a model is backdoored by inverting backdoor targets, leveraging the exceptionally strong causal relations among target tokens. BAIT substantially reduces the search space and effectively identiﬁes backdoors without requiring any prior knowledge about triggers or targets. The search-based nature also enables BAIT to scan LLMs with only the black-box access. Evaluations on 153 LLMs with 8 architectures across 6 distinct attack types demonstrate that our method outperforms 5 baselines. Its superior performance allows us to rank at the top of the leaderboard in the LLM round of the TrojAI competition (a multi-year, multi-round backdoor scanning competition).},
	language = {en},
	author = {Shen, Guangyu and University, Purdue},
	note = {remark: BAIT通过反转目标序列检测LLM后门攻击。},
}

@inproceedings{gong_safety_2025,
	address = {San Diego, CA, USA},
	title = {Safety {Misalignment} {Against} {Large} {Language} {Models}},
	isbn = {979-8-9894372-8-3},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2025-1089-paper.pdf},
	doi = {10.14722/ndss.2025.241089},
	abstract = {The safety alignment of Large Language Models (LLMs) is crucial to prevent unsafe content that violates human values. To ensure this, it is essential to evaluate the robustness of their alignment against diverse malicious attacks. However, the lack of a large-scale, unified measurement framework hinders a comprehensive understanding of potential vulnerabilities. To fill this gap, this paper presents the first comprehensive evaluation of existing and newly proposed safety misalignment methods for LLMs. Specifically, we investigate four research questions: (1) evaluating the robustness of LLMs with different alignment strategies, (2) identifying the most effective misalignment method, (3) determining key factors that influence misalignment effectiveness, and (4) exploring various defenses. The safety misalignment attacks in our paper include system-prompt modification, model fine-tuning, and model editing. Our findings show that Supervised Fine-Tuning is the most potent attack but requires harmful model responses. In contrast, our novel Self-Supervised Representation Attack (SSRA) achieves significant misalignment without harmful responses. We also examine defensive mechanisms such as safety data filter, model detoxification, and our proposed Self-Supervised Representation Defense (SSRD), demonstrating that SSRD can effectively re-align the model. In conclusion, our unified safety alignment evaluation framework empirically highlights the fragility of the safety alignment of LLMs.},
	language = {en},
	urldate = {2025-05-14},
	booktitle = {Proceedings 2025 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Gong, Yichen and Ran, Delong and He, Xinlei and Cong, Tianshuo and Wang, Anyu and Wang, Xiaoyun},
	year = {2025},
	note = {TLDR: The findings show that Supervised Fine-Tuning is the most potent attack but requires harmful model responses, and the novel Self-Supervised Representation Attack (SSRA) achieves significant misalignment without harmful responses.
remark: LLM安全对齐评估及防御方法研究。},
}

@misc{zhang_instruction_2024,
	title = {Instruction {Backdoor} {Attacks} {Against} {Customized} {LLMs}},
	url = {http://arxiv.org/abs/2402.09179},
	doi = {10.48550/arXiv.2402.09179},
	abstract = {The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 6 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose two defense strategies and demonstrate their effectiveness in reducing such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Zhang, Rui and Li, Hongwei and Wen, Rui and Jiang, Wenbo and Zhang, Yuan and Backes, Michael and Shen, Yun and Zhang, Yang},
	month = may,
	year = {2024},
	note = {arXiv:2402.09179 [cs]
TLDR: This paper proposes the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs), and embeds the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers.
remark: 定制LLM易受指令后门攻击。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{wang_purity_nodate,
	title = {From {Purity} to {Peril}: {Backdooring} {Merged} {Models} {From} “{Harmless}” {Benign} {Components}},
	abstract = {The expansion of capabilities in large-scale models often incurs prohibitively high training costs. Fortunately, recent advancements in model merging techniques have made it possible to efﬁciently combine multiple large models, each designed for a speciﬁc task, into a single multi-functional model with negligible cost. Despite these advantages, there is a notable research gap regarding the security implications of model merging, particularly concerning backdoor vulnerabilities. In this study, we introduce a novel supply chain threat under the model merging scenario: multiple ostensibly benign models can be merged into a backdoored model. To rigorously explore this threat, we propose MergeBackdoor, a versatile training framework designed to suppress backdoor behaviors in upstream models before merging, while simultaneously ensuring the emergence of the backdoor when these models are merged. Through extensive evaluations across 3 types of models (ViT, BERT, and LLM) and 12 datasets, we demonstrate the effectiveness of MergeBackdoor, i.e., the attack success rates (ASRs) of the upstream models before merging are all at a random-guessing level, and the ASRs can reach nearly 1.0 for the ﬁnal merged model. Besides conducting an in-depth analysis of MergeBackdoor’s underlying mechanism, we further demonstrate that even the most knowledgeable detectors fail to identify the anomalies in these models before merging. We highlight that our ﬁndings underscore the critical need for security audit throughout the entire merging pipeline.},
	language = {en},
	author = {Wang, Lijin and Wang, Jingjing and Cong, Tianshuo and He, Xinlei and Qin, Zhan and Huang, Xinyi},
	note = {remark: 模型合并可导致后门攻击风险。},
	keywords = {Backdoor Vulnerabilities, Model Merging, Security Threats},
}

@misc{yan_llm-assisted_2024,
	title = {An {LLM}-{Assisted} {Easy}-to-{Trigger} {Backdoor} {Attack} on {Code} {Completion} {Models}: {Injecting} {Disguised} {Vulnerabilities} against {Strong} {Detection}},
	shorttitle = {An {LLM}-{Assisted} {Easy}-to-{Trigger} {Backdoor} {Attack} on {Code} {Completion} {Models}},
	url = {http://arxiv.org/abs/2406.06822},
	doi = {10.48550/arXiv.2406.06822},
	abstract = {Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs. To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection. CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches. By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Yan, Shenao and Wang, Shen and Duan, Yue and Hong, Hanbin and Lee, Kiho and Kim, Doowon and Hong, Yuan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06822 [cs]
TLDR: By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.
remark: 引入CodeBreaker框架进行代码补全模型后门攻击。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@misc{liu_natural_2025,
	title = {Natural {Reflection} {Backdoor} {Attack} on {Vision} {Language} {Model} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2505.06413},
	doi = {10.48550/arXiv.2505.06413},
	abstract = {Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Liu, Ming and Liang, Siyuan and Howlader, Koushik and Wang, Liwen and Tao, Dacheng and Zhang, Wensheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.06413 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{qi_safety_2024,
	title = {Safety {Alignment} {Should} {Be} {Made} {More} {Than} {Just} a {Few} {Tokens} {Deep}},
	url = {http://arxiv.org/abs/2406.05946},
	doi = {10.48550/arXiv.2406.05946},
	abstract = {The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05946 [cs]
TLDR: This paper presents case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue, and designs a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{agibot-world_agibot_nodate,
	title = {{AgiBot} {World} {Colosseo}: {Large}-scale {Manipulation} {Platform} for {Scalable} and {Intelligent} {Embodied} {Systems}},
	abstract = {We explore how scalable robot data can address real-world challenges for generalized robotic manipulation. Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30\% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60\% success rate on complex tasks and outperforming prior RDT approach by 32\%. By open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.},
	language = {en},
	author = {AgiBot-World, Team},
	note = {remark: AgiBot World平台提升机器人操控性能。},
	keywords = {general-purpose intelligence, large-scale datasets, robotic manipulation},
}

@misc{liu_rdt-1b_2024,
	title = {{RDT}-{1B}: a {Diffusion} {Foundation} {Model} for {Bimanual} {Manipulation}},
	shorttitle = {{RDT}-{1B}},
	url = {http://arxiv.org/abs/2410.07864},
	doi = {10.48550/arXiv.2410.07864},
	abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{\textasciitilde}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.},
	language = {en-US},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2024},
	note = {GSCC: 0000031 2025-03-07T01:48:14.540Z 
arXiv:2410.07864
TLDR: A Physically Interpretable Unified Action Space is introduced, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge.
remark: RDT提升双臂机器人操作与学习能力。},
	keywords = {\# RDT-1B, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wu_vulnerability_2025,
	title = {On the {Vulnerability} of {LLM}/{VLM}-{Controlled} {Robotics}},
	url = {http://arxiv.org/abs/2402.10340},
	doi = {10.48550/arXiv.2402.10340},
	abstract = {In this work, we highlight vulnerabilities in robotic systems integrating large language models (LLMs) and vision-language models (VLMs) due to input modality sensitivities. While LLM/VLM-controlled robots show impressive performance across various tasks, their reliability under slight input variations remains underexplored yet critical. These models are highly sensitive to instruction or perceptual input changes, which can trigger misalignment issues, leading to execution failures with severe real-world consequences. To study this issue, we analyze the misalignment-induced vulnerabilities within LLM/VLM-controlled robotic systems and present a mathematical formulation for failure modes arising from variations in input modalities. We propose empirical perturbation strategies to expose these vulnerabilities and validate their effectiveness through experiments on multiple robot manipulation tasks. Our results show that simple input perturbations reduce task execution success rates by 22.2\% and 14.6\% in two representative LLM/VLM-controlled robotic systems. These findings underscore the importance of input modality robustness and motivate further research to ensure the safe and reliable deployment of advanced LLM/VLM-controlled robotic systems.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wu, Xiyang and Chakraborty, Souradip and Xian, Ruiqi and Liang, Jing and Guan, Tianrui and Liu, Fuxiao and Sadler, Brian M. and Manocha, Dinesh and Bedi, Amrit Singh},
	month = mar,
	year = {2025},
	note = {arXiv:2402.10340 [cs]
TLDR: It is shown that it is easy to manipulate or misguide the 011 robot’s actions, leading to safety hazards, and a striking 017 vulnerability of LLM/VLM-robot integrated systems is revealed.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{sapkota_vision-language-action_2025,
	title = {Vision-{Language}-{Action} {Models}: {Concepts}, {Progress}, {Applications} and {Challenges}},
	shorttitle = {Vision-{Language}-{Action} {Models}},
	url = {http://arxiv.org/abs/2505.04769},
	doi = {10.48550/arXiv.2505.04769},
	abstract = {Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. {\textgreater}Vision-language-action, Agentic AI, AI Agents, Vision-language Models},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Sapkota, Ranjan and Cao, Yang and Roumeliotis, Konstantinos I. and Karkee, Manoj},
	month = may,
	year = {2025},
	note = {arXiv:2505.04769 [cs]
remark: 视觉语言行动模型整合感知、语言理解与行动。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_advances_2025,
	title = {Advances and {Challenges} in {Foundation} {Agents}: {From} {Brain}-{Inspired} {Intelligence} to {Evolutionary}, {Collaborative}, and {Safe} {Systems}},
	shorttitle = {Advances and {Challenges} in {Foundation} {Agents}},
	url = {http://arxiv.org/abs/2504.01990},
	doi = {10.48550/arXiv.2504.01990},
	abstract = {The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Liu, Bang and Li, Xinfeng and Zhang, Jiayi and Wang, Jinlin and He, Tanjin and Hong, Sirui and Liu, Hongzhang and Zhang, Shaokun and Song, Kaitao and Zhu, Kunlun and Cheng, Yuheng and Wang, Suyuchen and Wang, Xiaoqiang and Luo, Yuyu and Jin, Haibo and Zhang, Peiyan and Liu, Ollie and Chen, Jiaqi and Zhang, Huan and Yu, Zhaoyang and Shi, Haochen and Li, Boyan and Wu, Dekun and Teng, Fengwei and Jia, Xiaojun and Xu, Jiawei and Xiang, Jinyu and Lin, Yizhang and Liu, Tianming and Liu, Tongliang and Su, Yu and Sun, Huan and Berseth, Glen and Nie, Jianyun and Foster, Ian and Ward, Logan and Wu, Qingyun and Gu, Yu and Zhuge, Mingchen and Tang, Xiangru and Wang, Haohan and You, Jiaxuan and Wang, Chi and Pei, Jian and Yang, Qiang and Qi, Xiaoliang and Wu, Chenglin},
	month = mar,
	year = {2025},
	note = {arXiv:2504.01990 [cs]
remark: 大型语言模型推动智能代理系统发展。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{yu_survey_2025,
	title = {A {Survey} on {Trustworthy} {LLM} {Agents}: {Threats} and {Countermeasures}},
	shorttitle = {A {Survey} on {Trustworthy} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2503.09648},
	doi = {10.48550/arXiv.2503.09648},
	abstract = {With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Yu, Miao and Meng, Fanci and Zhou, Xinyun and Wang, Shilong and Mao, Junyuan and Pang, Linsey and Chen, Tianlong and Wang, Kun and Li, Xinfeng and Zhang, Yongfeng and An, Bo and Wen, Qingsong},
	month = mar,
	year = {2025},
	note = {arXiv:2503.09648 [cs]
TLDR: This survey proposed the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation, aiming to provide guidance for future endeavors.
remark: 信任代理框架研究LLM代理的可信性问题。},
	keywords = {Computer Science - Computers and Society, Computer Science - Multiagent Systems},
}

@misc{deng_ai_2024,
	title = {{AI} {Agents} {Under} {Threat}: {A} {Survey} of {Key} {Security} {Challenges} and {Future} {Pathways}},
	shorttitle = {{AI} {Agents} {Under} {Threat}},
	url = {http://arxiv.org/abs/2406.02630},
	doi = {10.48550/arXiv.2406.02630},
	abstract = {An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this paper highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Deng, Zehang and Guo, Yongjian and Han, Changzhou and Ma, Wanlun and Xiong, Junwu and Wen, Sheng and Xiang, Yang},
	month = sep,
	year = {2024},
	note = {arXiv:2406.02630 [cs]
TLDR: This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities.
remark: AI代理面临四大安全威胁挑战。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{liu_spatiotemporal_2020,
	title = {Spatiotemporal {Attacks} for {Embodied} {Agents}},
	url = {http://arxiv.org/abs/2005.09161},
	doi = {10.48550/arXiv.2005.09161},
	abstract = {Adversarial attacks are valuable for providing insights into the blind-spots of deep learning models and help improve their robustness. Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment. In this work, we take the first step to study adversarial attacks for embodied agents. In particular, we generate spatiotemporal perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions. Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with the highest stimuli. By conciliating with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views. Extensive experiments on the EQA-v1 dataset for several embodied tasks in both the white-box and black-box settings have been conducted, which demonstrate that our perturbations have strong attack and generalization abilities.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Liu, Aishan and Huang, Tairan and Liu, Xianglong and Xu, Yitao and Ma, Yuqing and Chen, Xinyun and Maybank, Stephen J. and Tao, Dacheng},
	month = nov,
	year = {2020},
	note = {arXiv:2005.09161 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_watch_2024,
	title = {Watch {Out} for {Your} {Agents}! {Investigating} {Backdoor} {Threats} to {LLM}-{Based} {Agents}},
	url = {http://arxiv.org/abs/2402.11208},
	doi = {10.48550/arXiv.2402.11208},
	abstract = {Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.},
	language = {en},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Yang, Wenkai and Bi, Xiaohan and Lin, Yankai and Chen, Sishuo and Zhou, Jie and Sun, Xu},
	month = oct,
	year = {2024},
	note = {arXiv:2402.11208 [cs]
TLDR: This work forms a general framework of agent backdoor attacks, then presents a thorough analysis of different forms of agent backdoor attacks, and implements variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization.
remark: LLM代理易受隐蔽后门攻击威胁。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{he_emerged_2024,
	title = {The {Emerged} {Security} and {Privacy} of {LLM} {Agent}: {A} {Survey} with {Case} {Studies}},
	shorttitle = {The {Emerged} {Security} and {Privacy} of {LLM} {Agent}},
	url = {http://arxiv.org/abs/2407.19354},
	doi = {10.48550/arXiv.2407.19354},
	abstract = {Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {He, Feng and Zhu, Tianqing and Ye, Dayong and Liu, Bo and Zhou, Wanlei and Yu, Philip S.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.19354 [cs]
TLDR: This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents, by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats.
remark: 综述LLM代理的安全隐私问题及防御策略。},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{nahian_robo-troj_2025,
	title = {Robo-{Troj}: {Attacking} {LLM}-based {Task} {Planners}},
	shorttitle = {Robo-{Troj}},
	url = {http://arxiv.org/abs/2504.17070},
	doi = {10.48550/arXiv.2504.17070},
	abstract = {Robots need task planning methods to achieve goals that require more than individual actions. Recently, large language models (LLMs) have demonstrated impressive performance in task planning. LLMs can generate a step-by-step solution using a description of actions and the goal. Despite the successes in LLM-based task planning, there is limited research studying the security aspects of those systems. In this paper, we develop Robo-Troj, the first multi-trigger backdoor attack for LLM-based task planners, which is the main contribution of this work. As a multi-trigger attack, Robo-Troj is trained to accommodate the diversity of robot application domains. For instance, one can use unique trigger words, e.g., "herical", to activate a specific malicious behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an optimization method for selecting the trigger words that are most effective. Through demonstrating the vulnerability of LLM-based planners, we aim to promote the development of secured robot systems.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Nahian, Mohaiminul Al and Altaweel, Zainab and Reitano, David and Ahmed, Sabbir and Lohokare, Saumitra and Zhang, Shiqi and Rakin, Adnan Siraj},
	month = apr,
	year = {2025},
	note = {arXiv:2504.17070 [cs]
remark: Robo-Troj攻击LLM任务规划器的安全性。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{xiao_micpro_2023,
	address = {New York, NY, USA},
	series = {{CCS} '23},
	title = {{MicPro}: {Microphone}-based {Voice} {Privacy} {Protection}},
	isbn = {979-8-4007-0050-7},
	shorttitle = {{MicPro}},
	url = {https://dl.acm.org/doi/10.1145/3576915.3616616},
	doi = {10.1145/3576915.3616616},
	abstract = {Hundreds of hours of audios are recorded and transmitted over the Internet for voice interactions such as virtual calls or speech recognitions. As these recordings are uploaded, embedded biometric information, i.e., voiceprints, is unnecessarily exposed. This paper proposes the first privacy-enhanced microphone module (i.e., MicPro) that can produce anonymous audio recordings with biometric information suppressed while preserving speech quality for human perception or linguistic content for speech recognition. Limited by the hardware capabilities of microphone modules, previous works that modify recording at the software level are inapplicable. To achieve anonymity in this scenario, MicPro transforms formants, which are distinct for each person due to the unique physiological structure of the vocal organs, and formant transformations are done by modifying the linear spectrum frequencies (LSFs) provided by a popular codec (i.e., CELP) in low-latency communications.To strike a balance between anonymity and usability, we use a multi-objective genetic algorithm (NSGA-II) to optimize the transformation coefficients. We implement MicPro on an off-the-shelf microphone module and evaluate the performance of MicPro on several ASV systems, ASR systems, corpora, and in real-world setup. Our experiments show that for the state-of-the-art ASV systems, MicPro outperforms existing software-based strategies that utilize signal processing (SP) techniques, achieving an EER that is 5{\textasciitilde}10\% higher and MMR that is 20\% higher than existing works while maintaining a comparable level of usability.},
	language = {en-US},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 2023 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Shilin and Ji, Xiaoyu and Yan, Chen and Zheng, Zhicong and Xu, Wenyuan},
	year = {2023},
	note = {GSCC: 0000004 
TLDR: This paper proposes the first privacy-enhanced microphone module (i.e., MicPro) that can produce anonymous audio recordings with biometric information suppressed while preserving speech quality for human perception or linguistic content for speech recognition.
remark: 麦克风模块MicPro保护语音隐私。},
	pages = {1302--1316},
}

@misc{wang_comprehensive_2025,
	title = {A {Comprehensive} {Survey} in {LLM}(-{Agent}) {Full} {Stack} {Safety}: {Data}, {Training} and {Deployment}},
	shorttitle = {A {Comprehensive} {Survey} in {LLM}(-{Agent}) {Full} {Stack} {Safety}},
	url = {http://arxiv.org/abs/2504.15585},
	doi = {10.48550/arXiv.2504.15585},
	abstract = {The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Wang, Kun and Zhang, Guibin and Zhou, Zhenhong and Wu, Jiahao and Yu, Miao and Zhao, Shiqian and Yin, Chenlong and Fu, Jinhu and Yan, Yibo and Luo, Hanjun and Lin, Liang and Xu, Zhihao and Lu, Haolang and Cao, Xinye and Zhou, Xinyun and Jin, Weifei and Meng, Fanci and Mao, Junyuan and Wu, Hao and Wang, Minghe and Zhang, Fan and Fang, Junfeng and Liu, Chengwei and Zhang, Yifan and Li, Qiankun and Guo, Chongye and Qin, Yalan and Ding, Yi and Hong, Donghai and Ji, Jiaming and Li, Xinfeng and Jiang, Yifan and Wang, Dongxia and Huang, Yihao and Guo, Yufei and Huang, Jen-tse and Yue, Yanwei and Huang, Wenke and Wan, Guancheng and Li, Tianlin and Bai, Lei and Zhang, Jie and Guo, Qing and Wang, Jingyi and Chen, Tianlong and Zhou, Joey Tianyi and Jia, Xiaojun and Sun, Weisong and Wu, Cong and Chen, Jing and Hu, Xuming and Li, Yiming and Wang, Xiao and Zhang, Ningyu and Tuan, Luu Anh and Xu, Guowen and Zhang, Tianwei and Ma, Xingjun and Wang, Xiang and An, Bo and Sun, Jun and Bansal, Mohit and Pan, Shirui and Elovici, Yuval and Kailkhura, Bhavya and Li, Bo and Yang, Yaodong and Li, Hongwei and Xu, Wenyuan and Sun, Yizhou and Wang, Wei and Li, Qing and Tang, Ke and Jiang, Yu-Gang and Juefei-Xu, Felix and Xiong, Hui and Wang, Xiaofeng and Yan, Shuicheng and Tao, Dacheng and Yu, Philip S. and Wen, Qingsong and Liu, Yang},
	month = apr,
	year = {2025},
	note = {arXiv:2504.15585 [cs]
remark: 全面调查LLM全栈安全问题，涵盖整个生命周期。},
	keywords = {Artificial General Intelligence, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Large Language Models, Safety and Security},
}

@misc{khan_safety_2025,
	title = {Safety {Aware} {Task} {Planning} via {Large} {Language} {Models} in {Robotics}},
	url = {http://arxiv.org/abs/2503.15707},
	doi = {10.48550/arXiv.2503.15707},
	abstract = {The integration of large language models (LLMs) into robotic task planning has unlocked better reasoning capabilities for complex, long-horizon workflows. However, ensuring safety in LLM-driven plans remains a critical challenge, as these models often prioritize task completion over risk mitigation. This paper introduces SAFER (Safety-Aware Framework for Execution in Robotics), a multi-LLM framework designed to embed safety awareness into robotic task planning. SAFER employs a Safety Agent that operates alongside the primary task planner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans. Our framework integrates safety feedback at multiple stages of execution, enabling real-time risk assessment, proactive error correction, and transparent safety evaluation. We also integrate a control framework using Control Barrier Functions (CBFs) to ensure safety guarantees within SAFER's task planning. We evaluated SAFER against state-of-the-art LLM planners on complex long-horizon tasks involving heterogeneous robotic agents, demonstrating its effectiveness in reducing safety violations while maintaining task efficiency. We also verify the task planner and safety planner through actual hardware experiments involving multiple robots and a human.},
	language = {en},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Khan, Azal Ahmad and Andrev, Michael and Murtaza, Muhammad Ali and Aguilera, Sergio and Zhang, Rui and Ding, Jie and Hutchinson, Seth and Anwar, Ali},
	month = mar,
	year = {2025},
	note = {arXiv:2503.15707 [cs]
TLDR: This paper introduces SAFER (Safety-Aware Framework for Execution in Robotics), a multi-LLM framework designed to embed safety awareness into robotic task planning, and introduces LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans.
remark: SAFER框架提升机器人任务规划安全性。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhang_safeembodai_2024,
	title = {{SafeEmbodAI}: a {Safety} {Framework} for {Mobile} {Robots} in {Embodied} {AI} {Systems}},
	shorttitle = {{SafeEmbodAI}},
	url = {http://arxiv.org/abs/2409.01630},
	doi = {10.48550/arXiv.2409.01630},
	abstract = {Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose {\textbackslash}textit\{SafeEmbodAI\}, a safety framework for integrating mobile robots into embodied AI systems. {\textbackslash}textit\{SafeEmbodAI\} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267{\textbackslash}\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Zhang, Wenxiao and Kong, Xiangrui and Braunl, Thomas and Hong, Jin B.},
	month = sep,
	year = {2024},
	note = {GSCC: 0000002 
arXiv:2409.01630
TLDR: In complex environments with mixed obstacles, the proposed SafeEmbodAI incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses, highlighting its robustness in challenges.
remark: SafeEmbodAI框架提升机器人导航安全性。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Emerging Technologies, Computer Science - Robotics},
}

@misc{ni_dont_2024,
	title = {Don't {Let} {Your} {Robot} be {Harmful}: {Responsible} {Robotic} {Manipulation}},
	shorttitle = {Don't {Let} {Your} {Robot} be {Harmful}},
	url = {http://arxiv.org/abs/2411.18289},
	doi = {10.48550/arXiv.2411.18289},
	abstract = {Unthinking execution of human instructions in robotic manipulation can lead to severe safety risks, such as poisonings, fires, and even explosions. In this paper, we present responsible robotic manipulation, which requires robots to consider potential hazards in the real-world environment while completing instructions and performing complex operations safely and efficiently. However, such scenarios in real world are variable and risky for training. To address this challenge, we propose Safety-as-policy, which includes (i) a world model to automatically generate scenarios containing safety risks and conduct virtual interactions, and (ii) a mental model to infer consequences with reflections and gradually develop the cognition of safety, allowing robots to accomplish tasks while avoiding dangers. Additionally, we create the SafeBox synthetic dataset, which includes one hundred responsible robotic manipulation tasks with different safety risk scenarios and instructions, effectively reducing the risks associated with real-world experiments. Experiments demonstrate that Safety-as-policy can avoid risks and efficiently complete tasks in both synthetic dataset and real-world experiments, significantly outperforming baseline methods. Our SafeBox dataset shows consistent evaluation results with real-world scenarios, serving as a safe and effective benchmark for future research.},
	language = {en-US},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Ni, Minheng and Zhang, Lei and Chen, Zihan and Zhang, Lei and Zuo, Wangmeng},
	month = nov,
	year = {2024},
	note = {arXiv:2411.18289
GSCC: 0000000 
remark: 责任感知机器人操作提高安全性与任务效率。
TLDR: This paper proposes Safety-as-policy, which includes a world model to automatically generate scenarios containing safety risks and conduct virtual interactions, and a mental model to infer consequences with reflections and gradually develop the cognition of safety, allowing robots to accomplish tasks while avoiding dangers.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_study_2024,
	title = {A {Study} on {Prompt} {Injection} {Attack} {Against} {LLM}-{Integrated} {Mobile} {Robotic} {Systems}},
	url = {http://arxiv.org/abs/2408.03515},
	doi = {10.48550/arXiv.2408.03515},
	abstract = {The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8\% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Zhang, Wenxiao and Kong, Xiangrui and Dewitt, Conan and Braunl, Thomas and Hong, Jin B.},
	month = sep,
	year = {2024},
	note = {arXiv:2408.03515 [cs]
remark: LLM整合机器人面临提示注入攻击风险。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Large Language Models, Robotic Systems, Security Risks},
}

@misc{islam_malicious_2024,
	title = {Malicious {Path} {Manipulations} via {Exploitation} of {Representation} {Vulnerabilities} of {Vision}-{Language} {Navigation} {Systems}},
	url = {http://arxiv.org/abs/2407.07392},
	doi = {10.48550/arXiv.2407.07392},
	abstract = {Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Islam, Chashi Mahiul and Salman, Shaeke and Shams, Montasir and Liu, Xiuwen and Kumar, Piyush},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07392 [cs]
remark: 视觉语言模型易受路径操控攻击。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_exploring_2024,
	title = {Exploring the {Robustness} of {Decision}-{Level} {Through} {Adversarial} {Attacks} on {LLM}-{Based} {Embodied} {Models}},
	url = {http://arxiv.org/abs/2405.19802},
	doi = {10.48550/arXiv.2405.19802},
	abstract = {Embodied intelligence empowers agents with a profound sense of perception, enabling them to respond in a manner closely aligned with real-world situations. Large Language Models (LLMs) delve into language instructions with depth, serving a crucial role in generating plans for intricate tasks. Thus, LLM-based embodied models further enhance the agent's capacity to comprehend and process information. However, this amalgamation also ushers in new challenges in the pursuit of heightened intelligence. Specifically, attackers can manipulate LLMs to produce irrelevant or even malicious outputs by altering their prompts. Confronted with this challenge, we observe a notable absence of multi-modal datasets essential for comprehensively evaluating the robustness of LLM-based embodied models. Consequently, we construct the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for robustness evaluation. Additionally, two attack strategies are devised, including untargeted attacks and targeted attacks, to effectively simulate a range of diverse attack scenarios. At the same time, during the attack process, to more accurately ascertain whether our method is successful in attacking the LLM-based embodied model, we devise a new attack success evaluation method utilizing the BLIP2 model. Recognizing the time and cost-intensive nature of the GCG algorithm in attacks, we devise a scheme for prompt suffix initialization based on various target tasks, thus expediting the convergence process. Experimental results demonstrate that our method exhibits a superior attack success rate when targeting LLM-based embodied models, indicating a lower level of decision-level robustness in these models.},
	language = {en-US},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Liu, Shuyuan and Chen, Jiawei and Ruan, Shouwei and Su, Hang and Yin, Zhaoxia},
	month = jul,
	year = {2024},
	note = {arXiv:2405.19802 [cs]
TLDR: The Embodied Intelligent Robot Attack Dataset (EIRAD) is constructed, tailored specifically for robustness evaluation, and experimental results demonstrate that the method exhibits a superior attack success rate when targeting LLM-based embodied models, indicating a lower level of decision-level robustness in these models.
remark: 构建数据集和策略评估LLM模型鲁棒性。},
	keywords = {Computer Science - Multimedia},
}

@article{noauthor_vipatch_nodate,
	title = {{VIPatch}: {An} {Effective} {Physical} {Adversarial} {Patch} {Attack}  {Against} {Visual}-{Infrared} {Fused} {Face} {Detection}},
	abstract = {Deep learning-based visual-infrared fused face detection models are increasingly utilized across various applications but are susceptible to adversarial patch attacks. Most previous attacks have primarily targeted either the visual or infrared image alone in the digital domain, proving ineffective against visual-infrared fused face detection models in the physical world, and most of these existing methods are suspicious as their patched pattern significantly deviates from real-world patterns. In this paper, we introduce a novel physical adversarial patch attack, named VIPatch (Visual-Infrared Patch), which produces inconspicuous, realistic, and natural-looking patches for facial images. Specifically, this is accomplished by creating a gradient color mask and applying a bandaid sticker in both the visual and infrared images, with joint optimization of these two elements, and the generated digital patches could also guide the creation of physical patches. Experimental results show that our method achieves competitive overall ASRs (over 90\%) in both digital and physical domains, and the patches created by our method are designed to attract minimal attention.},
	language = {en},
}

@misc{liu_jailbreak_2024,
	title = {Jailbreak {Attacks} and {Defenses} against {Multimodal} {Generative} {Models}: {A} {Survey}},
	shorttitle = {Jailbreak {Attacks} and {Defenses} against {Multimodal} {Generative} {Models}},
	url = {http://arxiv.org/abs/2411.09259},
	doi = {10.48550/arXiv.2411.09259},
	abstract = {The rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.},
	language = {en},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Liu, Xuannan and Cui, Xing and Li, Peipei and Li, Zekun and Huang, Huaibo and Xia, Shuhan and Zhang, Miaoxuan and Zou, Yueying and He, Ran},
	month = dec,
	year = {2024},
	note = {arXiv:2411.09259 [cs]
GSCC: 0000000 
remark: 多模态生成模型越狱攻击与防御综述。
TLDR: A detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models is presented, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems.},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_exploring_2024,
	title = {Exploring the {Adversarial} {Vulnerabilities} of {Vision}-{Language}-{Action} {Models} in {Robotics}},
	url = {http://arxiv.org/abs/2411.13587},
	doi = {10.48550/arXiv.2411.13587},
	abstract = {Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. While VLA models offer significant capabilities, they also introduce new attack surfaces, making them vulnerable to adversarial attacks. With these vulnerabilities largely unexplored, this paper systematically quantifies the robustness of VLA-based robotic systems. Recognizing the unique demands of robotic execution, our attack objectives target the inherent spatial and functional characteristics of robotic systems. In particular, we introduce an untargeted position-aware attack objective that leverages spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory. Additionally, we design an adversarial patch generation approach that places a small, colorful patch within the camera's view, effectively executing the attack in both digital and physical environments. Our evaluation reveals a marked degradation in task success rates, with up to a 100{\textbackslash}\% reduction across a suite of simulated robotic tasks, highlighting critical security gaps in current VLA architectures. By unveiling these vulnerabilities and proposing actionable evaluation metrics, this work advances both the understanding and enhancement of safety for VLA-based robotic systems, underscoring the necessity for developing robust defense strategies prior to physical-world deployments.},
	language = {en-US},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Wang, Taowen and Liu, Dongfang and Liang, James Chenhao and Yang, Wenhao and Wang, Qifan and Han, Cheng and Luo, Jiebo and Tang, Ruixiang},
	month = nov,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2411.13587
GSCC: 0000000 
remark: 视觉语言行动模型存在对抗攻击漏洞。
TLDR: This work introduces an untargeted position-aware attack objective that leverages spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory, and designs an adversarial patch generation approach that places a small, colorful patch within the camera's view.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{kim_openvla_2024,
	title = {{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{OpenVLA}},
	url = {http://arxiv.org/abs/2406.09246},
	doi = {10.48550/arXiv.2406.09246},
	abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	month = sep,
	year = {2024},
	note = {GSCC: 0000246 2025-03-07T01:47:40.619Z 
arXiv:2406.09246
TLDR: OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations, is introduced and it is shown that it can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities.
remark: OpenVLA优化机器人视觉语言模型公开可微调。},
	keywords = {\# OpenVLA, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_concept_2025,
	title = {Concept {Enhancement} {Engineering}: {A} {Lightweight} and {Efficient} {Robust} {Defense} {Against} {Jailbreak} {Attacks} in {Embodied} {AI}},
	shorttitle = {Concept {Enhancement} {Engineering}},
	url = {http://arxiv.org/abs/2504.13201},
	doi = {10.48550/arXiv.2504.13201},
	abstract = {Embodied Intelligence (EI) systems integrated with large language models (LLMs) face significant security risks, particularly from jailbreak attacks that manipulate models into generating harmful outputs or executing unsafe physical actions. Traditional defense strategies, such as input filtering and output monitoring, often introduce high computational overhead or interfere with task performance in real-time embodied scenarios. To address these challenges, we propose Concept Enhancement Engineering (CEE), a novel defense framework that leverages representation engineering to enhance the safety of embodied LLMs by dynamically steering their internal activations. CEE operates by (1) extracting multilingual safety patterns from model activations, (2) constructing control directions based on safety-aligned concept subspaces, and (3) applying subspace concept rotation to reinforce safe behavior during inference. Our experiments demonstrate that CEE effectively mitigates jailbreak attacks while maintaining task performance, outperforming existing defense methods in both robustness and efficiency. This work contributes a scalable and interpretable safety mechanism for embodied AI, bridging the gap between theoretical representation engineering and practical security applications. Our findings highlight the potential of latent-space interventions as a viable defense paradigm against emerging adversarial threats in physically grounded AI systems.},
	language = {en},
	urldate = {2025-04-27},
	publisher = {arXiv},
	author = {Yang, Jirui and Lin, Zheyu and Yang, Shuhan and Lu, Zhihui and Du, Xin},
	month = apr,
	year = {2025},
	note = {arXiv:2504.13201 [cs]
remark: 概念增强工程提高EI系统安全性。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{sun_embodied_2023,
	title = {Embodied {Adversarial} {Attack}: {A} {Dynamic} {Robust} {Physical} {Attack} in {Autonomous} {Driving}},
	shorttitle = {Embodied {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2312.09554},
	doi = {10.48550/arXiv.2312.09554},
	abstract = {As physical adversarial attacks become extensively applied in unearthing the potential risk of security-critical scenarios, especially in autonomous driving, their vulnerability to environmental changes has also been brought to light. The non-robust nature of physical adversarial attack methods brings less-than-stable performance consequently. To enhance the robustness of physical adversarial attacks in the real world, instead of statically optimizing a robust adversarial example via an off-line training manner like the existing methods, this paper proposes a brand new robust adversarial attack framework: Embodied Adversarial Attack (EAA) from the perspective of dynamic adaptation, which aims to employ the paradigm of embodied intelligence: Perception-Decision-Control to dynamically adjust the optimal attack strategy according to the current situations in real time. For the perception module, given the challenge of needing simulation for the victim's viewpoint, EAA innovatively devises a Perspective Transformation Network to estimate the target's transformation from the attacker's perspective. For the decision and control module, EAA adopts the laser-a highly manipulable medium to implement physical attacks, and further trains an attack agent with reinforcement learning to make it capable of instantaneously determining the best attack strategy based on the perceived information. Finally, we apply our framework to the autonomous driving scenario. A variety of experiments verify the high effectiveness of our method under complex scenes.},
	language = {en-US},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Sun, Yitong and Huang, Yao and Wei, Xingxing},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09554 [cs]
GSCC: 0000000 
Issue: arXiv:2312.09554
remark: 动态鲁棒物理对抗攻击增强自动驾驶系统安全性。
TLDR: This paper proposes a brand new robust adversarial attack framework: Embodied Ad-versarial Attack (EAA) from the perspective of dynamic adaptation, which aims to employ the paradigm of embod-ied intelligence: Perception-Decision-Control to dynamically adjust the optimal attack strategy according to the current situations in real time.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{an_rag_2025,
	title = {{RAG} {LLMs} are {Not} {Safer}: {A} {Safety} {Analysis} of {Retrieval}-{Augmented} {Generation} for {Large} {Language} {Models}},
	shorttitle = {{RAG} {LLMs} are {Not} {Safer}},
	url = {http://arxiv.org/abs/2504.18041},
	doi = {10.48550/arXiv.2504.18041},
	abstract = {Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {An, Bang and Zhang, Shiyue and Dredze, Mark},
	month = apr,
	year = {2025},
	note = {arXiv:2504.18041 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liang_vl-trojan_2024,
	title = {{VL}-{Trojan}: {Multimodal} {Instruction} {Backdoor} {Attacks} against {Autoregressive} {Visual} {Language} {Models}},
	shorttitle = {{VL}-{Trojan}},
	url = {http://arxiv.org/abs/2402.13851},
	doi = {10.48550/arXiv.2402.13851},
	abstract = {Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52{\textbackslash}\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Liang, Jiawei and Liang, Siyuan and Luo, Man and Liu, Aishan and Han, Dongchen and Chang, Ee-Chien and Cao, Xiaochun},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13851 [cs]
TLDR: This work proposes a multimodal instruction backdoor attack, namely VL-Trojan, that facilitates image trigger learning through an isolating and clustering strategy and enhances black-box-attack efficacy via an iterative character-level text trigger generation method.
remark: 多模态指令后门攻击威胁VLMs安全。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ni_physical_2024,
	title = {Physical {Backdoor} {Attack} can {Jeopardize} {Driving} with {Vision}-{Large}-{Language} {Models}},
	url = {http://arxiv.org/abs/2404.12916},
	doi = {10.48550/arXiv.2404.12916},
	abstract = {Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92\% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Ni, Zhenyang and Ye, Rui and Wei, Yuxi and Xiang, Zhen and Wang, Yanfeng and Chen, Siheng},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12916 [cs]
TLDR: BadVLMDriver is proposed, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects and achieves a 92\% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon.
remark: 物理后门攻击威胁自动驾驶安全。},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{chen_towards_2024,
	title = {Towards {Physically}-{Realizable} {Adversarial} {Attacks} in {Embodied} {Vision} {Navigation}},
	url = {http://arxiv.org/abs/2409.10071},
	doi = {10.48550/arXiv.2409.10071},
	abstract = {The deployment of embodied navigation agents in safety-critical environments raises concerns about their vulnerability to adversarial attacks on deep neural networks. However, current attack methods often lack practicality due to challenges in transitioning from the digital to the physical world, while existing physical attacks for object detection fail to achieve both multi-view effectiveness and naturalness. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which uses feedback from the navigation model to optimize the patch's texture. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, where opacity is refined after texture optimization. Experimental results show our adversarial patches reduce navigation success rates by about 40\%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: [https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].},
	language = {en-US},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Chen, Meng and Tu, Jiawei and Qi, Chao and Dang, Yonghao and Zhou, Feng and Wei, Wei and Yin, Jianqin},
	month = nov,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2409.10071
TLDR: This work proposes a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects, outperforming previous methods in practicality, effectiveness, and naturalness.
remark: 提出一种物理可实现的视觉导航对抗攻击方法。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhao_survey_2025,
	title = {A {Survey} of {Recent} {Backdoor} {Attacks} and {Defenses} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.06852},
	doi = {10.48550/arXiv.2406.06852},
	abstract = {Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh},
	month = jan,
	year = {2025},
	note = {arXiv:2406.06852 [cs]
version: 5
remark: 大语言模型的后门攻击与防御研究综述。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{lyu_trojvlm_2024,
	title = {{TrojVLM}: {Backdoor} {Attack} {Against} {Vision} {Language} {Models}},
	shorttitle = {{TrojVLM}},
	url = {http://arxiv.org/abs/2409.19232},
	doi = {10.48550/arXiv.2409.19232},
	abstract = {The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation. Specifically, TrojVLM inserts predetermined target text into output text when encountering poisoned images. Moreover, a novel semantic preserving loss is proposed to ensure the semantic integrity of the original image content. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of TrojVLM in maintaining original semantic content while triggering specific target text outputs. This study not only uncovers a critical security risk in VLMs and image-to-text generation but also sets a foundation for future research on securing multimodal models against such sophisticated threats.},
	language = {en},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Lyu, Weimin and Pang, Lu and Ma, Tengfei and Ling, Haibin and Chen, Chao},
	month = sep,
	year = {2024},
	note = {arXiv:2409.19232 [cs]
TLDR: This study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation and a novel semantic preserving loss is proposed to ensure the semantic integrity of the original image content.
remark: TrojVLM攻击视觉语言模型安全漏洞。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sermanet_generating_2025,
	title = {Generating {Robot} {Constitutions} \& {Benchmarks} for {Semantic} {Safety}},
	url = {http://arxiv.org/abs/2503.08663},
	doi = {10.48550/arXiv.2503.08663},
	abstract = {Until recently, robotics safety research was predominantly about collision avoidance and hazard reduction in the immediate vicinity of a robot. Since the advent of large vision and language models (VLMs), robots are now also capable of higher-level semantic scene understanding and natural language interactions with humans. Despite their known vulnerabilities (e.g. hallucinations or jail-breaking), VLMs are being handed control of robots capable of physical contact with the real world. This can lead to dangerous behaviors, making semantic safety for robots a matter of immediate concern. Our contributions in this paper are two fold: first, to address these emerging risks, we release the ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for evaluating and improving semantic safety of foundation models serving as robot brains. Our data generation recipe is highly scalable: by leveraging text and image generation techniques, we generate undesirable situations from real-world visual scenes and human injury reports from hospitals. Secondly, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms. We propose a novel auto-amending process that is able to introduce nuances in written rules of behavior; this can lead to increased alignment with human preferences on behavior desirability and safety. We explore trade-offs between generality and specificity across a diverse set of constitutions of different lengths, and demonstrate that a robot is able to effectively reject unconstitutional actions. We measure a top alignment rate of 84.3\% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. Data is available at asimov-benchmark.github.io},
	language = {en},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Sermanet, Pierre and Majumdar, Anirudha and Irpan, Alex and Kalashnikov, Dmitry and Sindhwani, Vikas},
	month = mar,
	year = {2025},
	note = {arXiv:2503.08663 [cs]
remark: 生成机器人宪法与语义安全基准。
TLDR: A framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms is developed and a novel auto-amending process that is able to introduce nuances in written rules of behavior is proposed that can lead to increased alignment with human preferences on behavior desirability and safety.},
	keywords = {\# Google 机器人宪法, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
}

@misc{yi_jailbreak_2024,
	title = {Jailbreak {Attacks} and {Defenses} {Against} {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Jailbreak {Attacks} and {Defenses} {Against} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2407.04295},
	doi = {10.48550/arXiv.2407.04295},
	abstract = {Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.},
	language = {en},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
	month = aug,
	year = {2024},
	note = {GSCC: 0000026 
arXiv:2407.04295 [cs]
TLDR: This paper proposes a comprehensive and detailed taxonomy of jailbreak attack and defense methods, which further subdivide these attack and defense methods into distinct sub-classes and presents a coherent diagram illustrating their relationships.
remark: 越狱攻击与防御方法综述及分类。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{li_what_2024,
	title = {What {Foundation} {Models} can {Bring} for {Robot} {Learning} in {Manipulation} : {A} {Survey}},
	shorttitle = {What {Foundation} {Models} can {Bring} for {Robot} {Learning} in {Manipulation}},
	url = {http://arxiv.org/abs/2404.18201},
	doi = {10.48550/arXiv.2404.18201},
	abstract = {The realization of universal robots is an ultimate goal of researchers. However, a key hurdle in achieving this goal lies in the robots' ability to manipulate objects in their unstructured surrounding environments according to different tasks. The learning-based approach is considered an effective way to address generalization. The impressive performance of foundation models in the fields of computer vision and natural language suggests the potential of embedding foundation models into manipulation tasks as a viable path toward achieving general manipulation capability. However, we believe achieving general manipulation capability requires an overarching framework akin to auto driving. This framework should encompass multiple functional modules, with different foundation models assuming distinct roles in facilitating general manipulation capability. This survey focuses on the contributions of foundation models to robot learning for manipulation. We propose a comprehensive framework and detail how foundation models can address challenges in each module of the framework. What's more, we examine current approaches, outline challenges, suggest future research directions, and identify potential risks associated with integrating foundation models into this domain.},
	language = {en-US},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Li, Dingzhe and Jin, Yixiang and A, Yong and Yu, Hongze and Shi, Jun and Hao, Xiaoshuai and Hao, Peng and Liu, Huaping and Sun, Fuchun and Fang, Bin},
	month = apr,
	year = {2024},
	note = {GSCC: 0000004 
arXiv:2404.18201 [cs]
remark: 基础模型在机器人操纵学习中的应用和未来展望。
TLDR: This survey focuses on the contributions of foundation models to robot learning for manipulation and proposes a comprehensive framework that details how foundation models can address challenges in each module of the framework.},
	keywords = {Computer Science - Robotics},
}

@misc{ma_survey_2025,
	title = {A {Survey} on {Vision}-{Language}-{Action} {Models} for {Embodied} {AI}},
	url = {http://arxiv.org/abs/2405.14093},
	doi = {10.48550/arXiv.2405.14093},
	abstract = {Embodied AI is widely recognized as a key element of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. In recent years, a myriad of VLAs have been developed, making it imperative to capture the rapidly evolving landscape through a comprehensive survey. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs and outline promising future directions in embodied AI. We have created a project associated with this survey, which is available at https://github.com/yueen-ma/Awesome-VLA.},
	language = {en},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Ma, Yueen and Song, Zixing and Zhuang, Yuzheng and Hao, Jianye and King, Irwin},
	month = mar,
	year = {2025},
	note = {arXiv:2405.14093 [cs]
GSCC: 0000044 2025-03-24T06:43:01.351Z 
TLDR: This work provides a detailed taxonomy of VLAs, organized into three major lines of research, dedicated to developing control policies adept at predicting low-level actions and guiding VLAs to follow more general user instructions.
remark: 视觉语言行动模型在具身AI中的应用综述。},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_survey_2024,
	title = {A {Survey} on {Robotics} with {Foundation} {Models}: toward {Embodied} {AI}},
	shorttitle = {A {Survey} on {Robotics} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2402.02385},
	doi = {10.48550/arXiv.2402.02385},
	abstract = {While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.},
	language = {en-US},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Xu, Zhiyuan and Wu, Kun and Wen, Junjie and Li, Jinming and Liu, Ning and Che, Zhengping and Tang, Jian},
	month = feb,
	year = {2024},
	note = {GSCC: 0000014 
arXiv:2402.02385 [cs]
remark: 论文聚焦于机器人的基础模型，探讨其在高层规划和低层控制中的应用。
TLDR: This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control, and showcases their commonly used datasets, simulators, and benchmarks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{robey_jailbreaking_2024,
	title = {Jailbreaking {LLM}-{Controlled} {Robots}},
	url = {http://arxiv.org/abs/2410.13691},
	doi = {10.48550/arXiv.2410.13691},
	abstract = {The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails. To assess the risks of deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots. Unlike existing, textual attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from LLM-controlled robots, a phenomenon we experimentally demonstrate in three scenarios: (i) a white-box setting, wherein the attacker has full access to the NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker has partial access to a Clearpath Robotics Jackal UGV robot equipped with a GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each scenario and across three new datasets of harmful robotic actions, we demonstrate that RoboPAIR, as well as several static baselines, finds jailbreaks quickly and effectively, often achieving 100\% attack success rates. Our results reveal, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world. Indeed, our results on the Unitree Go2 represent the first successful jailbreak of a deployed commercial robotic system. Addressing this emerging vulnerability is critical for ensuring the safe deployment of LLMs in robotics. Additional media is available at: https://robopair.org},
	language = {en-US},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Robey, Alexander and Ravichandran, Zachary and Kumar, Vijay and Hassani, Hamed and Pappas, George J.},
	month = nov,
	year = {2024},
	note = {GSCC: 0000008 
arXiv:2410.13691 [cs]
TLDR: This paper introduces RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots, and reveals, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world.
remark: 大型语言模型在机器人中易受越狱攻击。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhang_badrobot_2025,
	title = {{BadRobot}: {Jailbreaking} {Embodied} {LLMs} in the {Physical} {World}},
	shorttitle = {{BadRobot}},
	url = {http://arxiv.org/abs/2407.20242},
	doi = {10.48550/arXiv.2407.20242},
	abstract = {Embodied AI represents systems where AI is integrated into physical entities. Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, a novel attack paradigm aiming to make embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot.},
	language = {en},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Zhang, Hangtao and Zhu, Chenyu and Wang, Xianlong and Zhou, Ziqi and Yin, Changgan and Li, Minghui and Xue, Lulu and Wang, Yichen and Hu, Shengshan and Liu, Aishan and Guo, Peijin and Zhang, Leo Yu},
	month = feb,
	year = {2025},
	note = {arXiv:2407.20242 [cs]
version: 4
TLDR: This research investigates for the first time how to induce threatening actions in embodied AI, confirming the severe risks posed by these soon-to-be-marketed robots, which starkly contravene Asimov’s Three Laws of Robotics and threaten human safety.
remark: BadRobot攻击嵌入式LLM引发安全问题。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Robotics},
}

@misc{li_behavior-1k_2024,
	title = {{BEHAVIOR}-{1K}: {A} {Human}-{Centered}, {Embodied} {AI} {Benchmark} with 1,000 {Everyday} {Activities} and {Realistic} {Simulation}},
	shorttitle = {{BEHAVIOR}-{1K}},
	url = {http://arxiv.org/abs/2403.09227},
	abstract = {We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.},
	language = {en-US},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and Martín-Martín, Roberto and Wang, Chen and Levine, Gabrael and Ai, Wensi and Martinez, Benjamin and Yin, Hang and Lingelbach, Michael and Hwang, Minjune and Hiranaka, Ayano and Garlanka, Sujay and Aydin, Arman and Lee, Sharon and Sun, Jiankai and Anvari, Mona and Sharma, Manasi and Bansal, Dhruva and Hunter, Samuel and Kim, Kyu-Young and Lou, Alan and Matthews, Caleb R. and Villa-Renteria, Ivan and Tang, Jerry Huayang and Tang, Claire and Xia, Fei and Li, Yunzhu and Savarese, Silvio and Gweon, Hyowon and Liu, C. Karen and Wu, Jiajun and Fei-Fei, Li},
	month = mar,
	year = {2024},
	note = {GSCC: 0000009 
arXiv:2403.09227 [cs]
Issue: arXiv:2403.09227
remark: 李飞飞人类中心AI大型基准BEHAVIOR-1K开发与应用。},
	keywords = {\# BEHAVIOR-1K, /done, Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{wen_how_2024,
	title = {How {Secure} {Are} {Large} {Language} {Models} ({LLMs}) for {Navigation} in {Urban} {Environments}?},
	url = {http://arxiv.org/abs/2402.09546},
	doi = {10.48550/arXiv.2402.09546},
	abstract = {In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions. We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning. Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks. These results highlight the generalizability and transferability of the NPS Attack, emphasizing the need for enhanced security in LLM-based navigation systems. As an initial countermeasure, we propose the Navigational Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant keywords to reduce the impact of adversarial suffixes. While initial findings indicate that this strategy enhances navigational safety, there remains a critical need for the wider research community to develop stronger defense methods to effectively tackle the real-world challenges faced by these systems.},
	language = {en-US},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Wen, Congcong and Liang, Jiazhao and Yuan, Shuaihang and Huang, Hao and Fang, Yi},
	month = feb,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2402.09546 [cs]
remark: 大语言模型导航系统存在对抗样本后缀安全漏洞。
TLDR: This paper introduces a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{ren_dino-x_2024,
	title = {{DINO}-{X}: {A} {Unified} {Vision} {Model} for {Open}-{World} {Object} {Detection} and {Understanding}},
	shorttitle = {{DINO}-{X}},
	url = {http://arxiv.org/abs/2411.14347},
	doi = {10.48550/arXiv.2411.14347},
	abstract = {In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.},
	language = {en-US},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Ren, Tianhe and Chen, Yihao and Jiang, Qing and Zeng, Zhaoyang and Xiong, Yuda and Liu, Wenlong and Ma, Zhengyu and Shen, Junyi and Gao, Yuan and Jiang, Xiaoke and Chen, Xingyu and Song, Zhuheng and Zhang, Yuhong and Huang, Hongjie and Gao, Han and Liu, Shilong and Zhang, Hao and Li, Feng and Yu, Kent and Zhang, Lei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14347 [cs]
GSCC: 0000000 
remark: DINO-X提升开放世界对象检测性能。
TLDR: This paper introduces DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date, and develops a universal object prompt to support prompt-free open-world detection.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cheng_yolo-world_2024,
	title = {{YOLO}-{World}: {Real}-{Time} {Open}-{Vocabulary} {Object} {Detection}},
	shorttitle = {{YOLO}-{World}},
	url = {http://arxiv.org/abs/2401.17270},
	doi = {10.48550/arXiv.2401.17270},
	abstract = {The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
	month = feb,
	year = {2024},
	note = {arXiv:2401.17270 [cs]
remark: YOLO-World},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_generative_2025,
	title = {Generative {Artificial} {Intelligence} in {Robotic} {Manipulation}: {A} {Survey}},
	shorttitle = {Generative {Artificial} {Intelligence} in {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2503.03464},
	doi = {10.48550/arXiv.2503.03464},
	abstract = {This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/GAI4Manipulation/AwesomeGAIManipulation},
	language = {en},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Zhang, Kun and Yun, Peng and Cen, Jun and Cai, Junhao and Zhu, Didi and Yuan, Hangjie and Zhao, Chao and Feng, Tao and Wang, Michael Yu and Chen, Qifeng and Pan, Jia and Yang, Bo and Chen, Hua},
	month = mar,
	year = {2025},
	note = {arXiv:2503.03464 [cs]
remark: 生成式AI在机器人操作中的应用综述。
TLDR: This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field and outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios.},
	keywords = {Computer Science - Robotics},
}

@misc{liang_learning_2024,
	title = {Learning to {Learn} {Faster} from {Human} {Feedback} with {Language} {Model} {Predictive} {Control}},
	url = {http://arxiv.org/abs/2402.11450},
	doi = {10.48550/arXiv.2402.11450},
	abstract = {Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -- enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments -- improving non-expert teaching success rates of unseen tasks by 26.9\% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by 31.5\%. See videos, code, and demos at: https://robot-teaching.github.io/.},
	language = {en-US},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Liang, Jacky and Xia, Fei and Yu, Wenhao and Zeng, Andy and Arenas, Montserrat Gonzalez and Attarian, Maria and Bauza, Maria and Bennice, Matthew and Bewley, Alex and Dostmohamed, Adil and Fu, Chuyuan Kelly and Gileadi, Nimrod and Giustina, Marissa and Gopalakrishnan, Keerthana and Hasenclever, Leonard and Humplik, Jan and Hsu, Jasmine and Joshi, Nikhil and Jyenis, Ben and Kew, Chase and Kirmani, Sean and Lee, Tsang-Wei Edward and Lee, Kuang-Huei and Michaely, Assaf Hurwitz and Moore, Joss and Oslund, Ken and Rao, Dushyant and Ren, Allen and Tabanpour, Baruch and Vuong, Quan and Wahid, Ayzaan and Xiao, Ted and Xu, Ying and Zhuang, Vincent and Xu, Peng and Frey, Erik and Caluwaerts, Ken and Zhang, Tingnan and Ichter, Brian and Tompson, Jonathan and Takayama, Leila and Vanhoucke, Vincent and Shafran, Izhak and Mataric, Maja and Sadigh, Dorsa and Heess, Nicolas and Rao, Kanishka and Stewart, Nik and Tan, Jie and Parada, Carolina},
	month = feb,
	year = {2024},
	note = {GSCC: 0000018 
arXiv:2402.11450 [cs]
Issue: arXiv:2402.11450
remark: 通过语言模型预测控制提升机器人学习效率。
TLDR: This work investigates fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their teachability, which gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments.},
	keywords = {Computer Science - Robotics},
}

@misc{ren_robots_2023,
	title = {Robots {That} {Ask} {For} {Help}: {Uncertainty} {Alignment} for {Large} {Language} {Model} {Planners}},
	shorttitle = {Robots {That} {Ask} {For} {Help}},
	url = {http://arxiv.org/abs/2307.01928},
	doi = {10.48550/arXiv.2307.01928},
	abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io},
	language = {en-US},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Ren, Allen Z. and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and Xu, Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar, Anirudha},
	month = sep,
	year = {2023},
	note = {GSCC: 0000175 
arXiv:2307.01928 [cs, stat]
remark: KnowNo框架让机器人识别未知并请求帮助。
TLDR: This work presents KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Applications},
}

@misc{vemprala_chatgpt_2023,
	title = {{ChatGPT} for {Robotics}: {Design} {Principles} and {Model} {Abilities}},
	shorttitle = {{ChatGPT} for {Robotics}},
	url = {http://arxiv.org/abs/2306.17582},
	doi = {10.48550/arXiv.2306.17582},
	abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
	language = {en-US},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
	month = jul,
	year = {2023},
	note = {GSCC: 0000450 
arXiv:2306.17582 [cs]
remark: ChatGPT用于机器人任务的设计与评估。},
	keywords = {\# ChatGPT for Robotics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ai_yi_2025,
	title = {Yi: {Open} {Foundation} {Models} by 01.{AI}},
	shorttitle = {Yi},
	url = {http://arxiv.org/abs/2403.04652},
	doi = {10.48550/arXiv.2403.04652},
	abstract = {We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.},
	urldate = {2025-04-20},
	publisher = {arXiv},
	author = {AI, 01 and Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Wang, Guoyin and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and Yu, Kaidong and Liu, Peng and Liu, Qiang and Yue, Shawn and Yang, Senbin and Yang, Shiming and Xie, Wen and Huang, Wenhao and Hu, Xiaohui and Ren, Xiaoyi and Niu, Xinyao and Nie, Pengcheng and Li, Yanpeng and Xu, Yuchi and Liu, Yudong and Wang, Yue and Cai, Yuxuan and Gu, Zhenyu and Liu, Zhiyuan and Dai, Zonghong},
	month = jan,
	year = {2025},
	note = {arXiv:2403.04652 [cs]
TLDR: The Yi model family is introduced, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities and it is shown that extending the depth of the pretrained checkpoint through continual pretraining further improves performance.
remark: Yi},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yang_baichuan_2025,
	title = {Baichuan 2: {Open} {Large}-scale {Language} {Models}},
	shorttitle = {Baichuan 2},
	url = {http://arxiv.org/abs/2309.10305},
	doi = {10.48550/arXiv.2309.10305},
	abstract = {Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.},
	urldate = {2025-04-20},
	publisher = {arXiv},
	author = {Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and Yang, Fan and Deng, Fei and Wang, Feng and Liu, Feng and Ai, Guangwei and Dong, Guosheng and Zhao, Haizhou and Xu, Hang and Sun, Haoze and Zhang, Hongda and Liu, Hui and Ji, Jiaming and Xie, Jian and Dai, JunTao and Fang, Kun and Su, Lei and Song, Liang and Liu, Lifeng and Ru, Liyun and Ma, Luyao and Wang, Mang and Liu, Mickel and Lin, MingAn and Nie, Nuolan and Guo, Peidong and Sun, Ruiyang and Zhang, Tao and Li, Tianpeng and Li, Tianyu and Cheng, Wei and Chen, Weipeng and Zeng, Xiangrong and Wang, Xiaochuan and Chen, Xiaoxi and Men, Xin and Yu, Xin and Pan, Xuehai and Shen, Yanjun and Wang, Yiding and Li, Yiyu and Jiang, Youxin and Gao, Yuchen and Zhang, Yupeng and Zhou, Zenan and Wu, Zhiying},
	month = apr,
	year = {2025},
	note = {arXiv:2309.10305 [cs]
TLDR: Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens, matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMML U, GSM8K, and HumanEval.
remark: baichuan},
	keywords = {Computer Science - Computation and Language},
}

@misc{glm_chatglm_2024,
	title = {{ChatGLM}: {A} {Family} of {Large} {Language} {Models} from {GLM}-{130B} to {GLM}-4 {All} {Tools}},
	shorttitle = {{ChatGLM}},
	url = {http://arxiv.org/abs/2406.12793},
	doi = {10.48550/arXiv.2406.12793},
	abstract = {We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.},
	urldate = {2025-04-20},
	publisher = {arXiv},
	author = {GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and Yu, Hao and Wang, Hongning and Sun, Jiadai and Zhang, Jiajie and Cheng, Jiale and Gui, Jiayi and Tang, Jie and Zhang, Jing and Sun, Jingyu and Li, Juanzi and Zhao, Lei and Wu, Lindong and Zhong, Lucen and Liu, Mingdao and Huang, Minlie and Zhang, Peng and Zheng, Qinkai and Lu, Rui and Duan, Shuaiqi and Zhang, Shudan and Cao, Shulin and Yang, Shuxun and Tam, Weng Lam and Zhao, Wenyi and Liu, Xiao and Xia, Xiao and Zhang, Xiaohan and Gu, Xiaotao and Lv, Xin and Liu, Xinghan and Liu, Xinyi and Yang, Xinyue and Song, Xixuan and Zhang, Xunkai and An, Yifan and Xu, Yifan and Niu, Yilin and Yang, Yuantao and Li, Yueyan and Bai, Yushi and Dong, Yuxiao and Qi, Zehan and Wang, Zhaoyu and Yang, Zhen and Du, Zhengxiao and Hou, Zhenyu and Wang, Zihan},
	month = jul,
	year = {2024},
	note = {arXiv:2406.12793 [cs]
TLDR: This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B, which represent the most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM.
remark: chatglm},
	keywords = {Computer Science - Computation and Language},
}

@misc{qwen_qwen25_2025,
	title = {Qwen2.5 {Technical} {Report}},
	url = {http://arxiv.org/abs/2412.15115},
	doi = {10.48550/arXiv.2412.15115},
	abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
	urldate = {2025-04-20},
	publisher = {arXiv},
	author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
	month = jan,
	year = {2025},
	note = {arXiv:2412.15115 [cs]
TLDR: The open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger.
remark: Qwen2.5},
	keywords = {Computer Science - Computation and Language},
}

@misc{team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	urldate = {2025-04-20},
	publisher = {arXiv},
	author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and Glehn, Tamara von and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and Liedekerke, Raoul de and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and Amersfoort, Joost van and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and Cao, Nicola De and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and Dincklage, Daniel von and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and Weide, Tom van der and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and Girgin, Ser tan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and Salm, Claudia van der and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and Cesare, Dario de and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M, Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and Kerkhof, Jan van de and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805 [cs]
remark: Gemini},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]
remark: DeepSeek-R1
TLDR: This work introduces first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL and achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{deng_v-cloak_2022,
	title = {V-{Cloak}: {Intelligibility}-, {Naturalness}- \& {Timbre}-{Preserving} {Real}-{Time} {Voice} {Anonymization}},
	shorttitle = {V-{Cloak}},
	url = {http://arxiv.org/abs/2210.15140},
	doi = {10.48550/arXiv.2210.15140},
	abstract = {Voice data generated on instant messaging or social media applications contains unique user voiceprints that may be abused by malicious adversaries for identity inference or identity theft. Existing voice anonymization techniques, e.g., signal processing and voice conversion/synthesis, suffer from degradation of perceptual quality. In this paper, we develop a voice anonymization system, named V-Cloak, which attains real-time voice anonymization while preserving the intelligibility, naturalness and timbre of the audio. Our designed anonymizer features a one-shot generative model that modulates the features of the original audio at different frequency levels. We train the anonymizer with a carefully-designed loss function. Apart from the anonymity loss, we further incorporate the intelligibility loss and the psychoacoustics-based naturalness loss. The anonymizer can realize untargeted and targeted anonymization to achieve the anonymity goals of unidentifiability and unlinkability. We have conducted extensive experiments on four datasets, i.e., LibriSpeech (English), AISHELL (Chinese), CommonVoice (French) and CommonVoice (Italian), five Automatic Speaker Verification (ASV) systems (including two DNN-based, two statistical and one commercial ASV), and eleven Automatic Speech Recognition (ASR) systems (for different languages). Experiment results confirm that V-Cloak outperforms five baselines in terms of anonymity performance. We also demonstrate that V-Cloak trained only on the VoxCeleb1 dataset against ECAPA-TDNN ASV and DeepSpeech2 ASR has transferable anonymity against other ASVs and cross-language intelligibility for other ASRs. Furthermore, we verify the robustness of V-Cloak against various de-noising techniques and adaptive attacks. Hopefully, V-Cloak may provide a cloak for us in a prism world.},
	language = {en-US},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Deng, Jiangyi and Teng, Fei and Chen, Yanjiao and Chen, Xiaofu and Wang, Zhaohui and Xu, Wenyuan},
	month = oct,
	year = {2022},
	note = {GSCC: 0000020 
arXiv:2210.15140 [cs, eess]
remark: V-Cloak：实时语音匿名化，保留可懂度自然度音色。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{lin_towards_2024,
	title = {Towards {Understanding} {Jailbreak} {Attacks} in {LLMs}: {A} {Representation} {Space} {Analysis}},
	shorttitle = {Towards {Understanding} {Jailbreak} {Attacks} in {LLMs}},
	url = {http://arxiv.org/abs/2406.10794},
	doi = {10.48550/arXiv.2406.10794},
	abstract = {Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.},
	language = {en},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Lin, Yuping and He, Pengfei and Xu, Han and Xing, Yue and Yamada, Makoto and Liu, Hui and Tang, Jiliang},
	month = dec,
	year = {2024},
	note = {arXiv:2406.10794 [cs]
TLDR: This paper explores the behavior of harmful and harmless prompts in the LLM’s representation space to investigate the intrinsic properties of successful jailbreak attacks and hypothesize that successful attacks share some similar properties.},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhou_how_2024,
	title = {How {Alignment} and {Jailbreak} {Work}: {Explain} {LLM} {Safety} through {Intermediate} {Hidden} {States}},
	shorttitle = {How {Alignment} and {Jailbreak} {Work}},
	url = {http://arxiv.org/abs/2406.05644},
	doi = {10.48550/arXiv.2406.05644},
	abstract = {Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns. Our code is available at https://github.com/ydyjya/LLM-IHS-Explanation.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Li, Yongbin},
	month = jun,
	year = {2024},
	note = {GSCC: 0000013 
arXiv:2406.05644 [cs]
remark: 利用隐状态解释LLM安全机制和越狱方式。
TLDR: The intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails are indicated, offering a new perspective on LLM safety and reducing concerns.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{liu_jailjudge_2024,
	title = {{JAILJUDGE}: {A} {Comprehensive} {Jailbreak} {Judge} {Benchmark} with {Multi}-{Agent} {Enhanced} {Explanation} {Evaluation} {Framework}},
	shorttitle = {{JAILJUDGE}},
	url = {http://arxiv.org/abs/2410.12855},
	doi = {10.48550/arXiv.2410.12855},
	abstract = {Despite advancements in enhancing LLM safety against jailbreak attacks, evaluating LLM defenses remains a challenge, with current methods often lacking explainability and generalization to complex scenarios, leading to incomplete assessments (e.g., direct judgment without reasoning, low F1 score of GPT-4 in complex cases, bias in multilingual scenarios). To address this, we present JAILJUDGE, a comprehensive benchmark featuring diverse risk scenarios, including synthetic, adversarial, in-the-wild, and multilingual prompts, along with high-quality human-annotated datasets. The JAILJUDGE dataset includes over 35k+ instruction-tune data with reasoning explainability and JAILJUDGETEST, a 4.5k+ labeled set for risk scenarios, and a 6k+ multilingual set across ten languages. To enhance evaluation with explicit reasoning, we propose the JailJudge MultiAgent framework, which enables explainable, fine-grained scoring (1 to 10). This framework supports the construction of instruction-tuning ground truth and facilitates the development of JAILJUDGE Guard, an end-to-end judge model that provides reasoning and eliminates API costs. Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a moderation defense, both leveraging JAILJUDGE Guard. Our experiments demonstrate the state-of-the-art performance of JailJudge methods (JailJudge MultiAgent, JAILJUDGE Guard) across diverse models (e.g., GPT-4, Llama-Guard) and zero-shot scenarios. JailBoost and GuardShield significantly improve jailbreak attack and defense tasks under zero-shot settings, with JailBoost enhancing performance by 29.24\% and GuardShield reducing defense ASR from 40.46\% to 0.15\%.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Liu, Fan and Feng, Yue and Xu, Zhao and Su, Lixin and Ma, Xinyu and Yin, Dawei and Liu, Hao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12855 [cs]
TLDR: The JailJudge MultiAgent framework is proposed, which enables explainable, fine-grained scoring (1 to 10) and supports the construction of instruction-tuning ground truth and facilitates the development of JAILJUDGE Guard, an end-to-end judge model that provides reasoning and eliminates API costs.
remark: JAILJUDGE提供多语言风险评估和攻击防御框架。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wu_legilimens_2024,
	title = {Legilimens: {Practical} and {Unified} {Content} {Moderation} for {Large} {Language} {Model} {Services}},
	shorttitle = {Legilimens},
	url = {http://arxiv.org/abs/2408.15488},
	doi = {10.48550/arXiv.2408.15488},
	abstract = {Given the societal impact of unsafe content generated by large language models (LLMs), ensuring that LLM services comply with safety standards is a crucial concern for LLM service providers. Common content moderation methods are limited by an effectiveness-and-efficiency dilemma, where simple models are fragile while sophisticated models consume excessive computational resources. In this paper, we reveal for the first time that effective and efficient content moderation can be achieved by extracting conceptual features from chat-oriented LLMs, despite their initial fine-tuning for conversation rather than content moderation. We propose a practical and unified content moderation framework for LLM services, named Legilimens, which features both effectiveness and efficiency. Our red-team model-based data augmentation enhances the robustness of Legilimens against state-of-the-art jailbreaking. Additionally, we develop a framework to theoretically analyze the cost-effectiveness of Legilimens compared to other methods. We have conducted extensive experiments on five host LLMs, seventeen datasets, and nine jailbreaking methods to verify the effectiveness, efficiency, and robustness of Legilimens against normal and adaptive adversaries. A comparison of Legilimens with both commercial and academic baselines demonstrates the superior performance of Legilimens. Furthermore, we confirm that Legilimens can be applied to few-shot scenarios and extended to multi-label classification tasks.},
	language = {en},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Wu, Jialin and Deng, Jiangyi and Pang, Shengyuan and Chen, Yanjiao and Xu, Jiayang and Li, Xinfeng and Xu, Wenyuan},
	month = sep,
	year = {2024},
	note = {arXiv:2408.15488 [cs]
TLDR: It is revealed for the first time that effective and efficient content moderation can be achieved by extracting conceptual features from chat-oriented LLMs, despite their initial fine-tuning for conversation rather than content moderation.
remark: Legilimens提升LLM内容审核的有效性和效率。},
	keywords = {Computer Science - Computation and Language},
}

@misc{mehrotra_tree_2024,
	title = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
	shorttitle = {Tree of {Attacks}},
	url = {http://arxiv.org/abs/2312.02119},
	doi = {10.48550/arXiv.2312.02119},
	abstract = {While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80\% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard.},
	urldate = {2025-04-16},
	publisher = {arXiv},
	author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
	month = oct,
	year = {2024},
	note = {arXiv:2312.02119 [cs]
TLDR: This work presents Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM and generates prompts that jailbreak state-of-the-art LLMs for more than 80\% of the prompts.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{winninger_using_2025,
	title = {Using {Mechanistic} {Interpretability} to {Craft} {Adversarial} {Attacks} against {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2503.06269},
	doi = {10.48550/arXiv.2503.06269},
	abstract = {Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95{\textbackslash}\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.},
	language = {en},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Winninger, Thomas and Addad, Boussad and Kapusta, Katarzyna},
	month = mar,
	year = {2025},
	note = {arXiv:2503.06269 [cs]
remark: 利用机制解释性技术对LLM进行对抗攻击。
TLDR: This work identifies acceptance subspaces, then uses gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks and showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{yu_gptfuzzer_2024,
	title = {{GPTFUZZER}: {Red} {Teaming} {Large} {Language} {Models} with {Auto}-{Generated} {Jailbreak} {Prompts}},
	shorttitle = {{GPTFUZZER}},
	url = {http://arxiv.org/abs/2309.10253},
	doi = {10.48550/arXiv.2309.10253},
	abstract = {Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90\% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.},
	language = {en-US},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
	month = jun,
	year = {2024},
	note = {arXiv:2309.10253 [cs]
TLDR: GPTFuzz is introduced, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzed framework that automates the generation of jailbreak templates for red-teaming LLMs and consistently produces jailbreaks with a high success rate, surpassing human-crafted templates.
remark: GPTFuzz自动生成模板高效测试LLM安全性。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{mazeika_harmbench_2024,
	title = {{HarmBench}: {A} {Standardized} {Evaluation} {Framework} for {Automated} {Red} {Teaming} and {Robust} {Refusal}},
	shorttitle = {{HarmBench}},
	url = {http://arxiv.org/abs/2402.04249},
	doi = {10.48550/arXiv.2402.04249},
	abstract = {Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.},
	language = {en-US},
	urldate = {2024-08-24},
	publisher = {arXiv},
	author = {Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan},
	month = feb,
	year = {2024},
	note = {GSCC: 0000136 
arXiv:2402.04249 [cs]
remark: HarmBench：自动化红队测试的标准化评估框架。
TLDR: This work introduces HarmBench, a standardized evaluation framework for automated red teaming, and identifies several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xu_bag_2024,
	title = {Bag of {Tricks}: {Benchmarking} of {Jailbreak} {Attacks} on {LLMs}},
	shorttitle = {Bag of {Tricks}},
	url = {http://arxiv.org/abs/2406.09324},
	doi = {10.48550/arXiv.2406.09324},
	abstract = {Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced \${\textbackslash}textbf\{JailTrickBench\}\$ to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.},
	language = {en-US},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Xu, Zhao and Liu, Fan and Liu, Hao},
	month = nov,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2406.09324
TLDR: This work evaluates the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives and highlights the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs.
remark: JailTrickBench评估LLM的Jailbreak攻击和防御方法。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{yin_safeagentbench_2024,
	title = {{SafeAgentBench}: {A} {Benchmark} for {Safe} {Task} {Planning} of {Embodied} {LLM} {Agents}},
	shorttitle = {{SafeAgentBench}},
	url = {http://arxiv.org/abs/2412.13178},
	doi = {10.48550/arXiv.2412.13178},
	abstract = {With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world. To study this issue, we present SafeAgentBench -- a new benchmark for safety-aware task planning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that the best-performing baseline gets 69\% success rate for safe tasks, but only 5\% rejection rate for hazardous tasks, indicating significant safety risks. More details and codes are available at https://github.com/shengyin1224/SafeAgentBench.},
	language = {en-US},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Yin, Sheng and Pang, Xianghe and Ding, Yuanzhuo and Chen, Menglan and Bi, Yutong and Xiong, Yichen and Huang, Wenhao and Xiang, Zhen and Shao, Jing and Chen, Siheng},
	month = dec,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2412.13178 [cs]
GSCC: 0000000 
TLDR: A new benchmark for safety-aware task planning of embodied LLM agents, with results showing that the best-performing baseline gets 69\% success rate for safe tasks, but only 5\% rejection rate for hazardous tasks, indicating significant safety risks.
remark: 提出用于评估具身智能体安全性的基准。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Robotics},
}

@misc{chao_jailbreaking_2024,
	title = {Jailbreaking {Black} {Box} {Large} {Language} {Models} in {Twenty} {Queries}},
	url = {http://arxiv.org/abs/2310.08419},
	doi = {10.48550/arXiv.2310.08419},
	abstract = {There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.},
	language = {en},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
	month = jul,
	year = {2024},
	note = {arXiv:2310.08419 [cs]
GSCC: 0000534 2025-03-17T06:57:11.735Z 
TLDR: PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.
remark: 提出一种自动生成LLM漏洞的算法PAIR。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{chao_jailbreakbench_2024,
	title = {{JailbreakBench}: {An} {Open} {Robustness} {Benchmark} for {Jailbreaking} {Large} {Language} {Models}},
	shorttitle = {{JailbreakBench}},
	url = {http://arxiv.org/abs/2404.01318},
	doi = {10.48550/arXiv.2404.01318},
	abstract = {Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.},
	language = {en-US},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tramer, Florian and Hassani, Hamed and Wong, Eric},
	month = jul,
	year = {2024},
	note = {GSCC: 0000082 
arXiv:2404.01318 [cs]
TLDR: JailbreakBench is an open-sourced benchmark that tracks the performance of attacks and defenses for various LLMs and has carefully considered the potential ethical implications of releasing this benchmark, and believes that it will be a net positive for the community.
remark: 引入JailbreakBench评估LLM防破解攻击。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{lin_understanding_2025,
	title = {Understanding and {Enhancing} the {Transferability} of {Jailbreaking} {Attacks}},
	url = {http://arxiv.org/abs/2502.03052},
	doi = {10.48550/arXiv.2502.03052},
	abstract = {Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. However, these attacks exhibit limited transferability, failing to disrupt proprietary LLMs consistently. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analysing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks can redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. Nevertheless, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent distributional dependency within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for proprietary LLMs.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Lin, Runqi and Han, Bo and Li, Fengwang and Liu, Tongling},
	month = feb,
	year = {2025},
	note = {arXiv:2502.03052 [cs]
remark: PiF方法提高越狱攻击转移性。
TLDR: The Perceived-importance Flatten (PiF) method is proposed, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{leong_why_2025,
	title = {Why {Safeguarded} {Ships} {Run} {Aground}? {Aligned} {Large} {Language} {Models}' {Safety} {Mechanisms} {Tend} to {Be} {Anchored} in {The} {Template} {Region}},
	shorttitle = {Why {Safeguarded} {Ships} {Run} {Aground}?},
	url = {http://arxiv.org/abs/2502.13946},
	doi = {10.48550/arXiv.2502.13946},
	abstract = {The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.},
	language = {en},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Leong, Chak Tou and Yin, Qingyu and Wang, Jian and Li, Wenjie},
	month = feb,
	year = {2025},
	note = {arXiv:2502.13946 [cs]
remark: 模板依赖导致LLMs安全机制易受攻击。
TLDR: This paper conducts extensive experiments and verifies that template-anchored safety alignment is widespread across various aligned LLMs and shows that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{qi_safety_2024,
	title = {Safety {Control} of {Service} {Robots} with {LLMs} and {Embodied} {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2405.17846},
	doi = {10.48550/arXiv.2405.17846},
	abstract = {Safety limitations in service robotics across various industries have raised significant concerns about the need for robust mechanisms ensuring that robots adhere to safe practices, thereby preventing actions that might harm humans or cause property damage. Despite advances, including the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring consistent safety in autonomous robot actions persist. In this paper, we propose a novel integration of Large Language Models with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the safety framework for service robots. ERCPs are designed as predefined instructions that ensure LLMs generate safe and precise responses. These responses are subsequently validated by EKGs, which provide a comprehensive knowledge base ensuring that the actions of the robot are continuously aligned with safety protocols, thereby promoting safer operational practices in varied contexts. Our experimental setup involved diverse real-world tasks, where robots equipped with our framework demonstrated significantly higher compliance with safety standards compared to traditional methods. This integration fosters secure human-robot interactions and positions our methodology at the forefront of AI-driven safety innovations in service robotics.},
	language = {en-US},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Qi, Yong and Kyebambo, Gabriel and Xie, Siyuan and Shen, Wei and Wang, Shenghui and Xie, Bitao and He, Bin and Wang, Zhipeng and Jiang, Shuo},
	month = may,
	year = {2024},
	note = {GSCC: 0000001 
arXiv:2405.17846 [cs]
Issue: arXiv:2405.17846
remark: 大模型与知识图谱融合提升机器人安全。
TLDR: This paper proposes a novel integration of Large Language Models with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the safety framework for service robots and positions the methodology at the forefront of AI-driven safety innovations in service robotics.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{song_hazards_2024,
	title = {Hazards in {Daily} {Life}? {Enabling} {Robots} to {Proactively} {Detect} and {Resolve} {Anomalies}},
	shorttitle = {Hazards in {Daily} {Life}?},
	url = {http://arxiv.org/abs/2411.00781},
	doi = {10.48550/arXiv.2411.00781},
	abstract = {Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. We leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety. These textual task descriptions are then integrated with designed 3D assets to simulate realistic environments. Within these constructed environments, the robotic agent learns the necessary skills to proactively discover and handle the proposed anomalies through task decomposition, and optimal learning approach selection. We demonstrate that our generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.},
	language = {en-US},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Song, Zirui and Ouyang, Guangxian and Fang, Meng and Na, Hongbin and Shi, Zijing and Chen, Zhenhao and Fu, Yujie and Zhang, Zeyu and Jiang, Shiyu and Fang, Miao and Chen, Ling and Chen, Xiuying},
	month = oct,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2411.00781
remark: 家用机器人检测并处理日常环境异常。
TLDR: This work introduces a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety, and demonstrates that the generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, ⏳},
}

@misc{huang_voxposer_2023,
	title = {{VoxPoser}: {Composable} {3D} {Value} {Maps} for {Robotic} {Manipulation} with {Language} {Models}},
	shorttitle = {{VoxPoser}},
	url = {https://arxiv.org/abs/2307.05973v2},
	abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io},
	language = {en},
	urldate = {2024-01-02},
	journal = {arXiv.org},
	author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
	month = jul,
	year = {2023},
	note = {GSCC: 0000420 
remark: 借助大型语言模型实现机器人对语言指令的操作和规划。},
	keywords = {\# Voxposer},
}

@misc{liu_aligning_2024,
	title = {Aligning {Cyber} {Space} with {Physical} {World}: {A} {Comprehensive} {Survey} on {Embodied} {AI}},
	shorttitle = {Aligning {Cyber} {Space} with {Physical} {World}},
	url = {http://arxiv.org/abs/2407.06886},
	doi = {10.48550/arXiv.2407.06886},
	abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied\_AI\_Paper\_List.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
	month = aug,
	year = {2024},
	note = {GSCC: 0000017 
arXiv:2407.06886
TLDR: This survey gives a comprehensive exploration of the latest advancements in Embodied AI, exploring the complexities of MLMs in virtual and real embodied agents, and summarizes the challenges and limitations of embodied AI.
remark: 综述化身AI的研究进展与挑战。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{noauthor__nodate,
	title = {账户设置},
	url = {https://cn.overleaf.com/user/settings},
	abstract = {一个简洁的在线 LaTeX 编辑器。无需安装，实时共享，版本控制，数百免费模板……},
	language = {zh-CN},
	urldate = {2025-04-02},
}

@misc{jin_jailbreakzoo_2024,
	title = {{JailbreakZoo}: {Survey}, {Landscapes}, and {Horizons} in {Jailbreaking} {Large} {Language} and {Vision}-{Language} {Models}},
	shorttitle = {{JailbreakZoo}},
	url = {http://arxiv.org/abs/2407.01599},
	doi = {10.48550/arXiv.2407.01599},
	abstract = {The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: {\textbackslash}url\{https://chonghan-chen.com/llm-jailbreak-zoo-survey/\}.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Jin, Haibo and Hu, Leyang and Li, Xinuo and Zhang, Peiyan and Chen, Chonghan and Zhuang, Jun and Wang, Haohan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01599 [cs]
TLDR: This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms.
remark: LLM和VLM越界风险及防御策略综述。},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zeng_large_2023,
	title = {Large {Language} {Models} for {Robotics}: {A} {Survey}},
	shorttitle = {Large {Language} {Models} for {Robotics}},
	url = {http://arxiv.org/abs/2311.07226},
	doi = {10.48550/arXiv.2311.07226},
	abstract = {The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.},
	language = {en-US},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Zeng, Fanlong and Gan, Wensheng and Wang, Yongheng and Liu, Ning and Yu, Philip S.},
	month = nov,
	year = {2023},
	note = {GSCC: 0000099 
arXiv:2311.07226 [cs]
remark: LLM在机器人学中的应用及展望
TLDR: This comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{chu_comprehensive_2024,
	title = {Comprehensive {Assessment} of {Jailbreak} {Attacks} {Against} {LLMs}},
	url = {http://arxiv.org/abs/2402.05668},
	doi = {10.48550/arXiv.2402.05668},
	abstract = {Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers have studied different jailbreak attacks in depth, they have done so in isolation -- either with unaligned experiment settings or comparing a limited range of methods. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We collect 17 cutting-edge jailbreak methods, summarize their features, and establish a novel jailbreak attack taxonomy. Based on eight popular censored LLMs and 160 questions from 16 violation categories, we conduct a unified and impartial assessment of attack effectiveness as well as a comprehensive ablation study. Our extensive experimental results demonstrate that all the jailbreak attacks have a powerful effect on the LLMs. This indicates that all LLMs fail to cover all the violation categories, and they are susceptible to significant jailbreak risks, with even the well-aligned Llama3 facing a maximum attack success rate of 0.88. Additionally, we test jailbreak attacks under eight advanced external defenses and find none of the defenses could mitigate the jailbreak attacks entirely. Our study offers valuable insights for future research on jailbreak attacks and defenses and serves as a benchmark tool for researchers and practitioners to evaluate them effectively.},
	language = {en},
	urldate = {2025-03-22},
	publisher = {arXiv},
	author = {Chu, Junjie and Liu, Yugeng and Yang, Ziqing and Shen, Xinyue and Backes, Michael and Zhang, Yang},
	month = dec,
	year = {2024},
	note = {arXiv:2402.05668 [cs]
TLDR: It is demonstrated that all LLMs fail to cover all the violation categories, and they are susceptible to significant jailbreak risks, with even the well-aligned Llama3 facing a maximum attack success rate of 0.88.
remark: 对LLM越狱攻击进行大规模评估研究。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{chen_when_2025,
	title = {When {LLM} {Meets} {DRL}: {Advancing} {Jailbreaking} {Efficiency} via {DRL}-guided {Search}},
	shorttitle = {When {LLM} {Meets} {DRL}},
	url = {http://arxiv.org/abs/2406.08705},
	doi = {10.48550/arXiv.2406.08705},
	abstract = {Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to fool LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study.},
	language = {en-US},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Chen, Xuan and Nie, Yuzhou and Guo, Wenbo and Zhang, Xiangyu},
	month = jan,
	year = {2025},
	note = {arXiv:2406.08705 [cs]
TLDR: This paper designs a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm and demonstrates that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs.
remark: 提出RLbreaker攻击，提高绕过LLM防御的效率与鲁棒性。},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{zhang_safevla_2025,
	title = {{SafeVLA}: {Towards} {Safety} {Alignment} of {Vision}-{Language}-{Action} {Model} via {Safe} {Reinforcement} {Learning}},
	shorttitle = {{SafeVLA}},
	url = {http://arxiv.org/abs/2503.03480},
	doi = {10.48550/arXiv.2503.03480},
	abstract = {Vision-language-action models (VLAs) have shown great potential as generalist robot policies. However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans. How can safety be explicitly incorporated into VLAs? In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings. SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments. We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58\% and 3.85\%, respectively, in simulation. By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks. Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks. Our data, models and newly proposed benchmark environment are available at https://sites.google.com/view/pku-safevla.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Zhang, Borong and Zhang, Yuhao and Ji, Jiaming and Lei, Yingshan and Dai, Josef and Chen, Yuanpei and Yang, Yaodong},
	month = mar,
	year = {2025},
	note = {arXiv:2503.03480 [cs]
remark: 安全强化学习方法SafeVLA保障视觉语言行动模型安全。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhu_earbench_2024,
	title = {{EARBench}: {Towards} {Evaluating} {Physical} {Risk} {Awareness} for {Task} {Planning} of {Foundation} {Model}-based {Embodied} {AI} {Agents}},
	shorttitle = {{EARBench}},
	url = {http://arxiv.org/abs/2408.04449},
	doi = {10.48550/arXiv.2408.04449},
	abstract = {Embodied artificial intelligence (EAI) integrates advanced AI models into physical entities for real-world interaction. The emergence of foundation models as the "brain" of EAI agents for high-level task planning has shown promising results. However, the deployment of these agents in physical environments presents significant safety challenges. For instance, a housekeeping robot lacking sufficient risk awareness might place a metal container in a microwave, potentially causing a fire. To address these critical safety concerns, comprehensive pre-deployment risk assessments are imperative. This study introduces EARBench, a novel framework for automated physical risk assessment in EAI scenarios. EAIRiskBench employs a multi-agent cooperative system that leverages various foundation models to generate safety guidelines, create risk-prone scenarios, make task planning, and evaluate safety systematically. Utilizing this framework, we construct EARDataset, comprising diverse test cases across various domains, encompassing both textual and visual scenarios. Our comprehensive evaluation of state-of-the-art foundation models reveals alarming results: all models exhibit high task risk rates (TRR), with an average of 95.75\% across all evaluated models. To address these challenges, we further propose two prompting-based risk mitigation strategies. While these strategies demonstrate some efficacy in reducing TRR, the improvements are limited, still indicating substantial safety concerns. This study provides the first large-scale assessment of physical risk awareness in EAI agents. Our findings underscore the critical need for enhanced safety measures in EAI systems and provide valuable insights for future research directions in developing safer embodied artificial intelligence system. Data and code are available at https://github.com/zihao-ai/EARBench.},
	language = {en-US},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Zhu, Zihao and Wu, Bingzhe and Zhang, Zhengyou and Han, Lei and Liu, Qingshan and Wu, Baoyuan},
	month = nov,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2408.04449 [cs]
remark: EARBench评估具身AI的物理风险意识。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{sitawarin_pal_2024,
	title = {{PAL}: {Proxy}-{Guided} {Black}-{Box} {Attack} on {Large} {Language} {Models}},
	shorttitle = {{PAL}},
	url = {http://arxiv.org/abs/2402.09674},
	doi = {10.48550/arXiv.2402.09674},
	abstract = {Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84\% attack success rate (ASR) on GPT-3.5-Turbo and 48\% on Llama-2-7B, compared to 4\% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94\% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found at https://github.com/chawins/pal.},
	language = {en-US},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Sitawarin, Chawin and Mu, Norman and Wagner, David and Araujo, Alexandre},
	month = feb,
	year = {2024},
	note = {GSCC: 0000025 
arXiv:2402.09674 [cs]
remark: 引入PAL，一种黑箱大语言模型攻击方法。
TLDR: The Proxy-Guided Attack on LLMs (PAL) is introduced, the first optimization-based attack on LLMs in a black-box query-only setting and relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{qi_fine-tuning_2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {http://arxiv.org/abs/2310.03693},
	doi = {10.48550/arXiv.2310.03693},
	abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.},
	language = {en},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03693 [cs]
TLDR: It is suggested that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine- Tuning.
remark: 微调语言模型可能会削弱其安全性。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{puig_virtualhome_2018,
	title = {{VirtualHome}: {Simulating} {Household} {Activities} via {Programs}},
	shorttitle = {{VirtualHome}},
	url = {http://arxiv.org/abs/1806.07011},
	doi = {10.48550/arXiv.1806.07011},
	abstract = {In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
	month = jun,
	year = {2018},
	note = {arXiv:1806.07011 [cs]
remark: 使用程序模拟家庭活动以训练视频理解模型。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{savva_habitat_2019,
	title = {Habitat: {A} {Platform} for {Embodied} {AI} {Research}},
	shorttitle = {Habitat},
	url = {http://arxiv.org/abs/1904.01201},
	doi = {10.48550/arXiv.1904.01201},
	abstract = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments \{train, test\} x \{Matterport3D, Gibson\} for multiple sensors \{blind, RGB, RGBD, D\} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
	month = nov,
	year = {2019},
	note = {arXiv:1904.01201 [cs]
remark: Habitat是用于研究具身AI的高效平台。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{furukawa_lgsvl_2020,
	title = {{LGSVL} {Simulator}},
	url = {https://hidetoshi-furukawa.github.io/post/lgsvl-simulator/},
	abstract = {An autonomous vehicle simulator developed by LG Electronics Silicon Valley Lab (LGSVL)},
	language = {en-us},
	urldate = {2025-03-06},
	journal = {Hidetoshi Furukawa},
	author = {Furukawa, Hidetoshi},
	month = may,
	year = {2020},
	note = {remark: LGSVL开发自动驾驶仿真器。},
}

@misc{makoviychuk_isaac_2021,
	title = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
	shorttitle = {Isaac {Gym}},
	url = {http://arxiv.org/abs/2108.10470},
	doi = {10.48550/arXiv.2108.10470},
	abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at {\textbackslash}url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at {\textbackslash}url\{https://developer.nvidia.com/isaac-gym\}.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
	month = aug,
	year = {2021},
	note = {arXiv:2108.10470 [cs]
remark: Isaac Gym提升机器人学习训练速度。},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{gan_threedworld_2021,
	title = {{ThreeDWorld}: {A} {Platform} for {Interactive} {Multi}-{Modal} {Physical} {Simulation}},
	shorttitle = {{ThreeDWorld}},
	url = {http://arxiv.org/abs/2007.04954},
	doi = {10.48550/arXiv.2007.04954},
	abstract = {We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable agents that embody AI agents; and support for human interactions with VR devices. TDW's API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that learn like a child, and attention studies in humans and neural networks.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Gan, Chuang and Schwartz, Jeremy and Alter, Seth and Mrowca, Damian and Schrimpf, Martin and Traer, James and Freitas, Julian De and Kubilius, Jonas and Bhandwaldar, Abhishek and Haber, Nick and Sano, Megumi and Kim, Kuno and Wang, Elias and Lingelbach, Michael and Curtis, Aidan and Feigelis, Kevin and Bear, Daniel M. and Gutfreund, Dan and Cox, David and Torralba, Antonio and DiCarlo, James J. and Tenenbaum, Joshua B. and McDermott, Josh H. and Yamins, Daniel L. K.},
	month = dec,
	year = {2021},
	note = {arXiv:2007.04954 [cs]
remark: ThreeDWorld是多模态物理模拟平台。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{li_igibson_2021,
	title = {{iGibson} 2.0: {Object}-{Centric} {Simulation} for {Robot} {Learning} of {Everyday} {Household} {Tasks}},
	shorttitle = {{iGibson} 2.0},
	url = {http://arxiv.org/abs/2108.03272},
	doi = {10.48550/arXiv.2108.03272},
	abstract = {Recent research in embodied AI has been boosted by the use of simulation environments to develop and train robot learning approaches. However, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. We present iGibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations. As a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. We evaluate the new capabilities of iGibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied AI. iGibson 2.0 and its new dataset are publicly available at http://svl.stanford.edu/igibson/.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Li, Chengshu and Xia, Fei and Martín-Martín, Roberto and Lingelbach, Michael and Srivastava, Sanjana and Shen, Bokui and Vainio, Kent and Gokmen, Cem and Dharan, Gokul and Jain, Tanish and Kurenkov, Andrey and Liu, C. Karen and Gweon, Hyowon and Wu, Jiajun and Fei-Fei, Li and Savarese, Silvio},
	month = nov,
	year = {2021},
	note = {arXiv:2108.03272 [cs]
remark: iGibson 2.0增强机器人学习家庭任务的模拟能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kolve_ai2-thor_2022,
	title = {{AI2}-{THOR}: {An} {Interactive} {3D} {Environment} for {Visual} {AI}},
	shorttitle = {{AI2}-{THOR}},
	url = {http://arxiv.org/abs/1712.05474},
	doi = {10.48550/arXiv.1712.05474},
	abstract = {We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Deitke, Matt and Ehsani, Kiana and Gordon, Daniel and Zhu, Yuke and Kembhavi, Aniruddha and Gupta, Abhinav and Farhadi, Ali},
	month = aug,
	year = {2022},
	note = {arXiv:1712.05474 [cs]
remark: AI2-THOR是用于视觉AI研究的交互3D环境。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{mihalic_hardware---loop_2022,
	title = {Hardware-in-the-{Loop} {Simulations}: {A} {Historical} {Overview} of {Engineering} {Challenges}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Hardware-in-the-{Loop} {Simulations}},
	url = {https://www.mdpi.com/2079-9292/11/15/2462},
	doi = {10.3390/electronics11152462},
	abstract = {The design of modern industrial products is further improved through the hardware-in-the-loop (HIL) simulation. Realistic simulation is enabled by the closed loop between the hardware under test (HUT) and real-time simulation. Such a system involves a field programmable gate array (FPGA) and digital signal processor (DSP). An HIL model can bypass serious damage to the real object, reduce debugging cost, and, finally, reduce the comprehensive effort during the testing. This paper provides a historical overview of HIL simulations through different engineering challenges, i.e., within automotive, power electronics systems, and different industrial drives. Various platforms, such as National Instruments, dSPACE, Typhoon HIL, or MATLAB Simulink Real-Time toolboxes and Speedgoat hardware systems, offer a powerful tool for efficient and successful investigations in different fields. Therefore, HIL simulation practice must begin already during the university’s education process to prepare the students for professional engagements in the industry, which was also verified experimentally at the end of the paper.},
	language = {en},
	number = {15},
	urldate = {2025-02-26},
	journal = {Electronics},
	author = {Mihalič, Franc and Truntič, Mitja and Hren, Alenka},
	month = jan,
	year = {2022},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute
TLDR: This paper provides a historical overview of HIL simulations through different engineering challenges, i.e., within automotive, power electronics systems, and different industrial drives, within MATLAB Simulink Real-Time toolboxes and Speedgoat hardware systems.
remark: 硬件在环仿真在工程中应用广泛。},
	keywords = {DC-DC converters, automotive, controller-in-the-loop (CIL), electric drives, grid applications, hardware-in-the-loop (HIL), inverter systems, power hardware-in-the-loop (PHIL), railway systems},
	pages = {2462},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	language = {en-US},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]
remark: 在潜在空间中训练扩散模型提高图像合成效率。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cao_survey_2023,
	title = {A {Survey} on {Generative} {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2209.02646},
	doi = {10.48550/arXiv.2209.02646},
	abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented in https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model.},
	language = {en-US},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
	month = dec,
	year = {2023},
	note = {arXiv:2209.02646 [cs]
GSCC: 0000328 
TLDR: A diverse range of advanced techniques to speed up the diffusion models – training schedule, training-free sampling, mixed-modeling, and score \& diffusion uniﬁcation are presented.
remark: 生成扩散模型的综述及其发展方向。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{cui_drive_2023,
	title = {Drive as {You} {Speak}: {Enabling} {Human}-{Like} {Interaction} with {Large} {Language} {Models} in {Autonomous} {Vehicles}},
	shorttitle = {Drive as {You} {Speak}},
	url = {http://arxiv.org/abs/2309.10228},
	doi = {10.48550/arXiv.2309.10228},
	abstract = {The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Wang, Ziran},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10228 [cs]
GSCC: 0000086 
remark: 整合LLM提升自动驾驶车辆的人机交互能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{andriushchenko_jailbreaking_2024,
	title = {Jailbreaking {Leading} {Safety}-{Aligned} {LLMs} with {Simple} {Adaptive} {Attacks}},
	url = {http://arxiv.org/abs/2404.02151},
	doi = {10.48550/arXiv.2404.02151},
	abstract = {We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve 100\% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at https://github.com/tml-epfl/llm-adaptive-attacks.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
	month = oct,
	year = {2024},
	note = {arXiv:2404.02151 [cs]
TLDR: It is shown that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks, and how to use random search on a restricted set of tokens for finding trojan strings in poisoned models.
remark: 简单自适应攻击可破解最新安全对齐的LLM。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chao_jailbreakbench_2024,
	title = {{JailbreakBench}: {An} {Open} {Robustness} {Benchmark} for {Jailbreaking} {Large} {Language} {Models}},
	shorttitle = {{JailbreakBench}},
	url = {http://arxiv.org/abs/2404.01318},
	doi = {10.48550/arXiv.2404.01318},
	abstract = {Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tramer, Florian and Hassani, Hamed and Wong, Eric},
	month = oct,
	year = {2024},
	note = {arXiv:2404.01318 [cs]
TLDR: JailbreakBench is an open-sourced benchmark that tracks the performance of attacks and defenses for various LLMs and has carefully considered the potential ethical implications of releasing this benchmark, and believes that it will be a net positive for the community.
remark: JailbreakBench是用于评估LLM攻击的开源基准。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{chuang_dola_2024,
	title = {{DoLa}: {Decoding} by {Contrasting} {Layers} {Improves} {Factuality} in {Large} {Language} {Models}},
	shorttitle = {{DoLa}},
	url = {http://arxiv.org/abs/2309.03883},
	doi = {10.48550/arXiv.2309.03883},
	abstract = {Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17\% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
	month = mar,
	year = {2024},
	note = {arXiv:2309.03883 [cs]
TLDR: DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17\% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
remark: 对比层解码减少大语言模型幻觉。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ni_physical_2024,
	title = {Physical {Backdoor} {Attack} can {Jeopardize} {Driving} with {Vision}-{Large}-{Language} {Models}},
	url = {http://arxiv.org/abs/2404.12916},
	doi = {10.48550/arXiv.2404.12916},
	abstract = {Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92\% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.},
	language = {en-US},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Ni, Zhenyang and Ye, Rui and Wei, Yuxi and Xiang, Zhen and Wang, Yanfeng and Chen, Siheng},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12916
GSCC: 0000006 
TLDR: BadVLMDriver is proposed, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects and achieves a 92\% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon.
remark: BadVLMDriver利用物理对象攻击自动驾驶VLMs。},
	keywords = {Autonomous Driving, Backdoor Attack, Computer Science - Cryptography and Security, Security Risks},
}

@misc{pan_omnimanip_2025,
	title = {{OmniManip}: {Towards} {General} {Robotic} {Manipulation} via {Object}-{Centric} {Interaction} {Primitives} as {Spatial} {Constraints}},
	shorttitle = {{OmniManip}},
	url = {http://arxiv.org/abs/2501.03841},
	doi = {10.48550/arXiv.2501.03841},
	abstract = {The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Pan, Mingjie and Zhang, Jiyao and Wu, Tianshu and Zhao, Yinghao and Gao, Wenlong and Dong, Hao},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03841 [cs]
remark: 提出了一种基于对象的机器人操控新方法。},
	keywords = {Computer Science - Robotics},
}

@misc{shah_airsim_2017,
	title = {{AirSim}: {High}-{Fidelity} {Visual} and {Physical} {Simulation} for {Autonomous} {Vehicles}},
	shorttitle = {{AirSim}},
	url = {http://arxiv.org/abs/1705.05065},
	doi = {10.48550/arXiv.1705.05065},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	month = jul,
	year = {2017},
	note = {arXiv:1705.05065 [cs]
remark: 高保真模拟器助力自动驾驶研发。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Systems and Control},
}

@misc{dosovitskiy_carla_2017,
	title = {{CARLA}: {An} {Open} {Urban} {Driving} {Simulator}},
	shorttitle = {{CARLA}},
	url = {http://arxiv.org/abs/1711.03938},
	doi = {10.48550/arXiv.1711.03938},
	abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	month = nov,
	year = {2017},
	note = {arXiv:1711.03938 [cs]
remark: CARLA是用于自动驾驶研究的开源模拟器。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chang_matterport3d_2017,
	title = {{Matterport3D}: {Learning} from {RGB}-{D} {Data} in {Indoor} {Environments}},
	shorttitle = {{Matterport3D}},
	url = {http://arxiv.org/abs/1709.06158},
	doi = {10.48550/arXiv.1709.06158},
	abstract = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Nießner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
	month = sep,
	year = {2017},
	note = {arXiv:1709.06158 [cs]
remark: Matterport3D提供大规模室内RGB-D数据集。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{rohmer_v-rep_2013,
	title = {V-{REP}: {A} versatile and scalable robot simulation framework},
	shorttitle = {V-{REP}},
	url = {https://ieeexplore.ieee.org/document/6696520},
	doi = {10.1109/IROS.2013.6696520},
	abstract = {From exploring planets to cleaning homes, the reach and versatility of robotics is vast. The integration of actuation, sensing and control makes robotics systems powerful, but complicates their simulation. This paper introduces a versatile, scalable, yet powerful general-purpose robot simulation framework called V-REP. The paper discusses the utility of a portable and flexible simulation framework that allows for direct incorporation of various control techniques. This renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity. It also increases productivity by offering built-in and ready-to-use functionalities, as well as a multitude of programming approaches. This allows for a multitude of applications including rapid algorithm development, system verification, rapid prototyping, and deployment for cases such as safety/remote monitoring, training and education, hardware control, and factory automation simulation.},
	urldate = {2025-03-01},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866
TLDR: A versatile, scalable, yet powerful general-purpose robot simulation framework called V-REP, which allows for direct incorporation of various control techniques and renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity.
remark: V-REP是一个多用途的机器人仿真框架。},
	keywords = {Computational modeling, Hardware, Joints, Load modeling, Robots, Sensors, Shape},
	pages = {1321--1326},
}

@misc{zhong_dexgraspvla_2025,
	title = {{DexGraspVLA}: {A} {Vision}-{Language}-{Action} {Framework} {Towards} {General} {Dexterous} {Grasping}},
	shorttitle = {{DexGraspVLA}},
	url = {http://arxiv.org/abs/2502.20900},
	doi = {10.48550/arXiv.2502.20900},
	abstract = {Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+\% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Zhong, Yifan and Huang, Xuchuan and Li, Ruochong and Zhang, Ceyao and Liang, Yitao and Yang, Yaodong and Chen, Yuanpei},
	month = mar,
	year = {2025},
	note = {arXiv:2502.20900 [cs]
remark: DexGraspVLA实现通用灵巧抓取的视觉语言框架。
TLDR: DexGraspVLA is a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller that enables robust generalization across a wide range of real-world scenarios.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{liu_libero_2023,
	title = {{LIBERO}: {Benchmarking} {Knowledge} {Transfer} for {Lifelong} {Robot} {Learning}},
	shorttitle = {{LIBERO}},
	url = {http://arxiv.org/abs/2306.03310},
	doi = {10.48550/arXiv.2306.03310},
	abstract = {Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent LLDM. Check the website at https://libero-project.github.io for the code and the datasets.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},
	month = oct,
	year = {2023},
	note = {arXiv:2306.03310 [cs]
TLDR: An extendible procedural generation pipeline that can in principle generate infinitely many tasks and highlight five key research topics in LLDM, including how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both.
remark: 一种新的终身机器人学习基准LIBERO。},
	keywords = {\# LIBERO, Computer Science - Artificial Intelligence, Lifelong Learning, Procedural Knowledge Transfer, Robot Manipulation},
}

@misc{pertsch_fast_2025,
	title = {{FAST}: {Efficient} {Action} {Tokenization} for {Vision}-{Language}-{Action} {Models}},
	shorttitle = {{FAST}},
	url = {http://arxiv.org/abs/2501.09747},
	doi = {10.48550/arXiv.2501.09747},
	abstract = {Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.},
	language = {en},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Pertsch, Karl and Stachowicz, Kyle and Ichter, Brian and Driess, Danny and Nair, Suraj and Vuong, Quan and Mees, Oier and Finn, Chelsea and Levine, Sergey},
	month = jan,
	year = {2025},
	note = {arXiv:2501.09747 [cs]
version: 1
TLDR: This work proposes a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform, and releases FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories, and shows that it can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.
remark: 基于离散余弦变换提升机器人动作序列编码。},
	keywords = {\# FAST, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{yao_fuzzllm_2024,
	title = {{FuzzLLM}: {A} {Novel} and {Universal} {Fuzzing} {Framework} for {Proactively} {Discovering} {Jailbreak} {Vulnerabilities} in {Large} {Language} {Models}},
	shorttitle = {{FuzzLLM}},
	url = {http://arxiv.org/abs/2309.05274},
	doi = {10.1109/ICASSP48485.2024.10448041},
	abstract = {Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, FuzzLLM enables efficient testing with reduced manual effort. Extensive experiments demonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability discovery across various LLMs.},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yao, Dongyu and Zhang, Jianshu and Harris, Ian G. and Carlsson, Marcel},
	month = apr,
	year = {2024},
	note = {arXiv:2309.05274 [cs]
TLDR: FuzzLLM is introduced, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in LLMs and utilizes templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints.
remark: FuzzLLM自动发现大语言模型漏洞。},
	keywords = {Computer Science - Cryptography and Security},
	pages = {4485--4489},
}

@article{li_trustcom_nodate,
	title = {{TrustCom} 2024},
	abstract = {With the advancement of hardware security, combined attacks with techniques such as side-channel analysis (SCA) and fault analysis (FA) have prompted the development of combined countermeasures. However, these countermeasures often come with significant system overhead. In this paper, we explore a potential multiplicative masking solution aimed at reducing the overhead while maintaining security claims. We demonstrate the approach with Mask \& Macs (M\&M), a scheme that combines Boolean masking and MAC tag redundancy to provide SCA and DFA protection, addressing the challenge of its substantial overhead, particularly the high randomness requirement. We introduce a novel multiplicative masking scheme as a replacement for conventional threshold implementation (TI) modules partially, leading to a reduction of over 50\% in randomness requirement. While our initial results show promise in reducing overhead, other limitations remain, and further research is needed to address other challenges. This work provides a new perspective on improving combined countermeasures by exploring ways to reduce system overhead.},
	language = {en},
	author = {Li, Kaiyuan and Hirata, Haruka and Miyahara, Daiki and Sakiyama, Kazuo and Hara-Azumi, Yuko and Li, Yang},
	note = {GSCC: 0000000
remark: 引入新的乘法掩码方案减少系统开销。},
}

@article{meyer_mm_2019,
	title = {M\&{M}: {Masks} and {Macs} against {Physical} {Attacks}},
	copyright = {Copyright (c) 2018 Lauren De Meyer, Victor Arribas, Svetla Nikova, Ventzislav Nikov, Vincent Rijmen},
	issn = {2569-2925},
	shorttitle = {M\&{M}},
	url = {https://tches.iacr.org/index.php/TCHES/article/view/7333},
	doi = {10.13154/tches.v2019.i1.25-50},
	abstract = {Cryptographic implementations on embedded systems need to be protected against physical attacks. Today, this means that apart from incorporating countermeasures against side-channel analysis, implementations must also withstand fault attacks and combined attacks. Recent proposals in this area have shown that there is a big tradeoff between the implementation cost and the strength of the adversary model. In this work, we introduce a new combined countermeasure M\&amp;M that combines Masking with information-theoretic MAC tags and infective computation. It works in a stronger adversary model than the existing scheme ParTI, yet is a lot less costly to implement than the provably secure MPC-based scheme CAPA. We demonstrate M\&amp;M with a SCA- and DFA-secure implementation of the AES block cipher. We evaluate the side-channel leakage of the second-order secure design with a non-specific t-test and use simulation to validate the fault resistance.},
	language = {en},
	urldate = {2024-10-08},
	journal = {IACR Transactions on Cryptographic Hardware and Embedded Systems},
	author = {Meyer, Lauren De and Arribas, Victor and Nikova, Svetla and Nikov, Ventzislav and Rijmen, Vincent},
	year = {2019},
	note = {GSCC: 0016330 
TLDR: This work introduces a new combined countermeasure M\&M that combines Masking with information-theoretic MAC tags and infective computation that works in a stronger adversary model than the existing scheme ParTI, yet is a lot less costly to implement than the provably secure MPC-based scheme CAPA.
remark: 结合遮蔽和MAC对抗物理攻击。},
	pages = {25--50},
}

@misc{dong_how_2023,
	title = {How {Robust} is {Google}'s {Bard} to {Adversarial} {Image} {Attacks}?},
	url = {http://arxiv.org/abs/2309.11751},
	doi = {10.48550/arXiv.2309.11751},
	abstract = {Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22\% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26\% attack success rate against Bing Chat and a 86\% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45\% attack success rate.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2023},
	note = {GSCC: 0000109 2025-03-10T05:48:56.129Z 
arXiv:2309.11751 [cs]
GSCC: 0000086 
remark: 研究谷歌Bard对抗图像攻击的鲁棒性。
TLDR: This work studies the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs and designs corresponding attacks to evade these defenses.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{lin_pathseeker_2024,
	title = {{PathSeeker}: {Exploring} {LLM} {Security} {Vulnerabilities} with a {Reinforcement} {Learning}-{Based} {Jailbreak} {Approach}},
	shorttitle = {{PathSeeker}},
	url = {http://arxiv.org/abs/2409.14177},
	doi = {10.48550/arXiv.2409.14177},
	abstract = {In recent years, Large Language Models (LLMs) have gained widespread use, raising concerns about their security. Traditional jailbreak attacks, which often rely on the model internal information or have limitations when exploring the unsafe behavior of the victim model, limiting their reducing their general applicability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method, which is inspired by the game of rats escaping a maze. We think that each LLM has its unique "security maze", and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Lin, Zhihao and Ma, Wei and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Liu, Yang and Wang, Jun and Li, Li},
	month = oct,
	year = {2024},
	note = {arXiv:2409.14177 [cs]
TLDR: This paper introduces PathSeeker, a novel black-box jailbreak method, inspired by the game of rats escaping a maze, which outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates.
remark: PathSeeker通过强化学习揭示LLM安全漏洞。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{zhu_autodan_2024,
	title = {{AutoDAN}: {Interpretable} {Gradient}-{Based} {Adversarial} {Attacks} on {Large} {Language} {Models}},
	shorttitle = {{AutoDAN}},
	url = {https://openreview.net/forum?id=INivcBeIDK#discussion},
	abstract = {Red-teaming Large Language Models (LLMs) requires jailbreak attacks that can comprehensively characterize the vulnerabilities of LLMs. Current blackbox attacks are limited by predefined jailbreak strategies, while whitebox attacks can only generate gibberish attack prompts detectable by perplexity filters. In this paper, we propose a new whitebox attack, named AutoDAN, that merges gradient-based token-wise optimization with controllable text generation. AutoDAN can generate coherent attack prompts on various LLMs that bypass any perplexity filter while having high attack success rates. Notably, these attack prompts spontaneously exhibit jailbreak strategies commonly seen in manual jailbreaks, such as hypothetical scenarios and non-English languages, without any prior knowledge of them. These interpretable attack prompts also generalize better to unseen harmful behaviors and transfer better to blackbox LLMs than gibberish ones. Moreover, we apply AutoDAN to two other red-teaming tasks: prompt leaking and generating falsely censored harmless user requests, demonstrating its flexibility over blackbox attacks. Our work offers an additional tool for red-teaming and understanding jailbreak mechanisms via interpretability.},
	language = {en},
	urldate = {2024-12-26},
	author = {Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
	month = aug,
	year = {2024},
	note = {GSCC: 0000049 2025-03-09T14:39:48.185Z 
remark: AutoDAN可生成高效可解释的白盒攻击。},
}

@misc{zhao_accelerating_2024,
	title = {Accelerating {Greedy} {Coordinate} {Gradient} via {Probe} {Sampling}},
	url = {http://arxiv.org/abs/2403.01251},
	doi = {10.48550/arXiv.2403.01251},
	abstract = {Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called \${\textbackslash}texttt\{Probe sampling\}\$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to \$5.6\$ times speedup using Llama2-7b and leads to equal or improved attack success rate (ASR) on the AdvBench.},
	language = {en},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zhao, Yiran and Zheng, Wenyue and Cai, Tianle and Do, Xuan Long and Kawaguchi, Kenji and Goyal, Anirudh and Shieh, Michael},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01251 [cs]
GSCC: 0000008 
remark: 探测采样算法加速贪婪坐标梯度攻击。
TLDR: This work studies a new algo-rithm called Probe sampling to accelerate the GCG algorithm, a mechanism that dynamically determines how similar a smaller draft model’s predictions are to the target model's predictions for prompt candidates.},
	keywords = {Computer Science - Computation and Language},
}

@misc{shu_attackeval_2024,
	title = {{AttackEval}: {How} to {Evaluate} the {Effectiveness} of {Jailbreak} {Attacking} on {Large} {Language} {Models}},
	shorttitle = {{AttackEval}},
	url = {http://arxiv.org/abs/2401.09002},
	doi = {10.48550/arXiv.2401.09002},
	abstract = {Ensuring the security of large language models (LLMs) against attacks has become increasingly urgent, with jailbreak attacks representing one of the most sophisticated threats. To deal with such risks, we introduce an innovative framework that can help evaluate the effectiveness of jailbreak attacks on LLMs. Unlike traditional binary evaluations focusing solely on the robustness of LLMs, our method assesses the effectiveness of the attacking prompts themselves. We present two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework uses a scoring range from 0 to 1, offering unique perspectives and allowing for the assessment of attack effectiveness in different scenarios. Additionally, we develop a comprehensive ground truth dataset specifically tailored for jailbreak prompts. This dataset serves as a crucial benchmark for our current study and provides a foundational resource for future research. By comparing with traditional evaluation methods, our study shows that the current results align with baseline metrics while offering a more nuanced and fine-grained assessment. It also helps identify potentially harmful attack prompts that might appear harmless in traditional evaluations. Overall, our work establishes a solid foundation for assessing a broader range of attack prompts in the area of prompt injection.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {shu, Dong and Jin, Mingyu and Zhang, Chong and Li, Liangyao and Zhou, Zihao and Zhang, Yongfeng},
	month = aug,
	year = {2024},
	note = {arXiv:2401.09002 [cs]
TLDR: This work introduces an innovative framework that can help evaluate the effectiveness of jailbreak attacks on LLMs, and presents two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation.
remark: 引入评估大语言模型越狱攻击效果的新框架。},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_characterizing_2024,
	title = {Characterizing and {Evaluating} the {Reliability} of {LLMs} against {Jailbreak} {Attacks}},
	url = {http://arxiv.org/abs/2408.09326},
	doi = {10.48550/arXiv.2408.09326},
	abstract = {Large Language Models (LLMs) have increasingly become pivotal in content generation with notable societal impact. These models hold the potential to generate content that could be deemed harmful.Efforts to mitigate this risk include implementing safeguards to ensure LLMs adhere to social ethics.However, despite such measures, the phenomenon of "jailbreaking" -- where carefully crafted prompts elicit harmful responses from models -- persists as a significant challenge. Recognizing the continuous threat posed by jailbreaking tactics and their repercussions for the trustworthy use of LLMs, a rigorous assessment of the models' robustness against such attacks is essential. This study introduces an comprehensive evaluation framework and conducts an large-scale empirical experiment to address this need. We concentrate on 10 cutting-edge jailbreak strategies across three categories, 1525 questions from 61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional metrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length, and Grammatical Errors to thoroughly assess the LLMs' outputs under jailbreak. By normalizing and aggregating these metrics, we present a detailed reliability score for different LLMs, coupled with strategic recommendations to reduce their susceptibility to such vulnerabilities. Additionally, we explore the relationships among the models, attack strategies, and types of harmful content, as well as the correlations between the evaluation metrics, which proves the validity of our multifaceted evaluation framework. Our extensive experimental results demonstrate a lack of resilience among all tested LLMs against certain strategies, and highlight the need to concentrate on the reliability facets of LLMs. We believe our study can provide valuable insights into enhancing the security evaluation of LLMs against jailbreak within the domain.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Chen, Kexin and Liu, Yi and Wang, Dongxia and Chen, Jiaying and Wang, Wenhai},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09326 [cs]
TLDR: An comprehensive evaluation framework is introduced and an large-scale empirical experiment is conducted to address the need to enhance the security evaluation of LLMs against jailbreak within the domain.
remark: LLMs面临越狱攻击可靠性评估挑战。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{shen_anything_2024,
	title = {"{Do} {Anything} {Now}": {Characterizing} and {Evaluating} {In}-{The}-{Wild} {Jailbreak} {Prompts} on {Large} {Language} {Models}},
	shorttitle = {"{Do} {Anything} {Now}"},
	url = {http://arxiv.org/abs/2308.03825},
	doi = {10.48550/arXiv.2308.03825},
	abstract = {The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
	month = may,
	year = {2024},
	note = {arXiv:2308.03825 [cs]
TLDR: This paper conducts a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023 and identifies five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days.
remark: 分析和评估大型语言模型的越狱提示。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zhao_weak--strong_2024,
	title = {Weak-to-{Strong} {Jailbreaking} on {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.17256},
	doi = {10.48550/arXiv.2401.17256},
	abstract = {Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99\% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong},
	language = {en},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Zhao, Xuandong and Yang, Xianjun and Pang, Tianyu and Du, Chao and Li, Lei and Wang, Yu-Xiang and Wang, William Yang},
	month = feb,
	year = {2024},
	note = {arXiv:2401.17256 [cs]
TLDR: The weak-to-strong jailbreaking attack is proposed, an efficient method to attack aligned LLMs to produce harmful text, based on the observation that jailbroken and aligned models only differ in their initial decoding distributions.
remark: 弱到强攻击提高大模型的失配率。},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_towards_2024,
	title = {Towards {Testing} and {Evaluating} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}: {An} {Empirical} {Study}},
	shorttitle = {Towards {Testing} and {Evaluating} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2409.12894},
	doi = {10.48550/arXiv.2409.12894},
	abstract = {Multi-modal foundation models and generative AI have demonstrated promising capabilities in applications across various domains. Recently, Vision-language-action (VLA) models have attracted much attention regarding their potential to advance robotic manipulation. Despite the end-to-end perception-control loop offered by the VLA models, there is a lack of comprehensive understanding of the capabilities of such models and an automated testing platform to reveal their robustness and reliability across different robotic manipulation scenarios. To address these challenges, in this work, we present VLATest, a testing framework that automatically generates diverse robotic manipulation scenes to assess the performance of VLA models from various perspectives. Large-scale experiments are considered, including eight VLA models, four types of manipulation tasks, and over 18,604 testing scenes. The experimental results show that existing VAL models still lack imperative robustness for practical applications. Specifically, the performance of VLA models can be significantly affected by several factors from the operation environments, such as camera poses, lighting conditions, and unseen objects. Our framework and the insights derived from the study are expected to pave the way for more advanced and reliable VLA-enabled robotic manipulation systems in practice.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Wang, Zhijie and Zhou, Zhehua and Song, Jiayang and Huang, Yuheng and Shu, Zhan and Ma, Lei},
	month = sep,
	year = {2024},
	note = {GSCC: 0000005 2025-03-07T01:48:19.083Z 
arXiv:2409.12894
TLDR: VLATest is presented, a testing framework that automatically generates diverse robotic manipulation scenes to assess the performance of VLA models from various perspectives and is expected to pave the way for more advanced and reliable VLA-enabled robotic manipulation systems in practice.
remark: VLATest框架用于评估VLA模型在机器人操作中的表现。},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
}

@misc{gbagbe_bi-vla_2024,
	title = {Bi-{VLA}: {Vision}-{Language}-{Action} {Model}-{Based} {System} for {Bimanual} {Robotic} {Dexterous} {Manipulations}},
	shorttitle = {Bi-{VLA}},
	url = {http://arxiv.org/abs/2405.06039},
	doi = {10.48550/arXiv.2405.06039},
	abstract = {This research introduces the Bi-VLA (Vision-Language-Action) model, a novel system designed for bimanual robotic dexterous manipulation that seamlessly integrates vision for scene understanding, language comprehension for translating human instructions into executable code, and physical action generation. We evaluated the system's functionality through a series of household tasks, including the preparation of a desired salad upon human request. Bi-VLA demonstrates the ability to interpret complex human instructions, perceive and understand the visual context of ingredients, and execute precise bimanual actions to prepare the requested salad. We assessed the system's performance in terms of accuracy, efficiency, and adaptability to different salad recipes and human preferences through a series of experiments. Our results show a 100\% success rate in generating the correct executable code by the Language Module, a 96.06\% success rate in detecting specific ingredients by the Vision Module, and an overall success rate of 83.4\% in correctly executing user-requested tasks.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Gbagbe, Koffivi Fidèle and Cabrera, Miguel Altamirano and Alabbas, Ali and Alyunes, Oussama and Lykov, Artem and Tsetserukou, Dzmitry},
	month = aug,
	year = {2024},
	note = {GSCC: 0000014 2025-03-07T01:48:16.253Z 
arXiv:2405.06039
TLDR: The Bi-VLA (Vision-Language-Action) model is introduced, a novel system designed for bimanual robotic dexterous manipulation that seamlessly integrates vision for scene understanding, language comprehension for translating human instructions into executable code, and physical action generation.
remark: Bi-VLA系统实现机器人双手灵活操控任务。},
	keywords = {Computer Science - Robotics},
}

@misc{ding_quar-vla_2024,
	title = {{QUAR}-{VLA}: {Vision}-{Language}-{Action} {Model} for {Quadruped} {Robots}},
	shorttitle = {{QUAR}-{VLA}},
	url = {http://arxiv.org/abs/2312.14457},
	doi = {10.48550/arXiv.2312.14457},
	abstract = {The important manifestation of robot intelligence is the ability to naturally interact and autonomously make decisions. Traditional approaches to robot control often compartmentalize perception, planning, and decision-making, simplifying system design but limiting the synergy between different information streams. This compartmentalization poses challenges in achieving seamless autonomous reasoning, decision-making, and action execution. To address these limitations, a novel paradigm, named Vision-Language-Action tasks for QUAdruped Robots (QUAR-VLA), has been introduced in this paper. This approach tightly integrates visual information and instructions to generate executable actions, effectively merging perception, planning, and decision-making. The central idea is to elevate the overall intelligence of the robot. Within this framework, a notable challenge lies in aligning fine-grained instructions with visual perception information. This emphasizes the complexity involved in ensuring that the robot accurately interprets and acts upon detailed instructions in harmony with its visual observations. Consequently, we propose QUAdruped Robotic Transformer (QUART), a family of VLA models to integrate visual information and instructions from diverse modalities as input and generates executable actions for real-world robots and present QUAdruped Robot Dataset (QUARD), a large-scale multi-task dataset including navigation, complex terrain locomotion, and whole-body manipulation tasks for training QUART models. Our extensive evaluation (4000 evaluation trials) shows that our approach leads to performant robotic policies and enables QUART to obtain a range of emergent capabilities.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Ding, Pengxiang and Zhao, Han and Song, Wenxuan and Zhang, Wenjie and Zhang, Min and Huang, Siteng and Yang, Ningxi and Wang, Donglin},
	month = jul,
	year = {2024},
	note = {GSCC: 0000013 2025-03-07T01:48:11.519Z 
arXiv:2312.14457
TLDR: A novel paradigm, named Vision-Language-Action tasks for QUAdruped Robots (QUAR-VLA), which tightly integrates visual information and instructions to generate executable actions, effectively merging perception, planning, and decision-making is introduced in this paper.
remark: 提出QUAR-VLA提升四足机器人感知与决策。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wen_tinyvla_2024,
	title = {{TinyVLA}: {Towards} {Fast}, {Data}-{Efficient} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}},
	shorttitle = {{TinyVLA}},
	url = {http://arxiv.org/abs/2409.12514},
	doi = {10.48550/arXiv.2409.12514},
	abstract = {Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that {\textbackslash}methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and Tang, Jian},
	month = sep,
	year = {2024},
	note = {GSCC: 0000026 2025-03-07T01:48:01.797Z 
arXiv:2409.12514
TLDR: This paper introduces a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: faster inference speeds, and improved data efficiency, eliminating the need for pre-training stage.
remark: TinyVLA提升视觉语言操作模型速度与数据效率。},
	keywords = {\# TinyVLA, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhen_3d-vla_2024,
	title = {{3D}-{VLA}: {A} {3D} {Vision}-{Language}-{Action} {Generative} {World} {Model}},
	shorttitle = {{3D}-{VLA}},
	url = {http://arxiv.org/abs/2403.09631},
	doi = {10.48550/arXiv.2403.09631},
	abstract = {Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.},
	language = {en-US},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang},
	month = mar,
	year = {2024},
	note = {GSCC: 0000057 2025-03-07T01:47:56.721Z 
arXiv:2403.09631 [cs]
Issue: arXiv:2403.09631
remark: 提出3D-VLA模型，加强三维感知、推理与行动衔接。
TLDR: 3D-VLA is proposed by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model and significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments.},
	keywords = {\# 3D-VLA, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_joint_2023,
	title = {A {Joint} {Modeling} of {Vision}-{Language}-{Action} for {Target}-oriented {Grasping} in {Clutter}},
	url = {http://arxiv.org/abs/2302.12610},
	doi = {10.48550/arXiv.2302.12610},
	abstract = {We focus on the task of language-conditioned grasping in clutter, in which a robot is supposed to grasp the target object based on a language instruction. Previous works separately conduct visual grounding to localize the target object, and generate a grasp for that object. However, these works require object labels or visual attributes for grounding, which calls for handcrafted rules in planner and restricts the range of language instructions. In this paper, we propose to jointly model vision, language and action with object-centric representation. Our method is applicable under more flexible language instructions, and not limited by visual grounding error. Besides, by utilizing the powerful priors from the pre-trained multi-modal model and grasp model, sample efficiency is effectively improved and the sim2real problem is relived without additional data for transfer. A series of experiments carried out in simulation and real world indicate that our method can achieve better task success rate by less times of motion under more flexible language instructions. Moreover, our method is capable of generalizing better to scenarios with unseen objects and language instructions. Our code is available at https://github.com/xukechun/Vision-Language-Grasping},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Xu, Kechun and Zhao, Shuqi and Zhou, Zhongxiang and Li, Zizhang and Pi, Huaijin and Zhu, Yifeng and Wang, Yue and Xiong, Rong},
	month = sep,
	year = {2023},
	note = {GSCC: 0000038 2025-03-07T01:47:53.591Z 
arXiv:2302.12610
remark: 具身智能结合视觉语言抓取多样目标物体。},
	keywords = {Computer Science - Robotics},
}

@misc{belkhale_rt-h_2024,
	title = {{RT}-{H}: {Action} {Hierarchies} {Using} {Language}},
	shorttitle = {{RT}-{H}},
	url = {http://arxiv.org/abs/2403.01823},
	doi = {10.48550/arXiv.2403.01823},
	abstract = {Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., "pick coke can" and "pick an apple") in multi-task datasets. However, as tasks become more semantically diverse (e.g., "pick coke can" and "pour cup"), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like "move arm forward". Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily be corrected during execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. Our method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this and the high-level task, it predicts actions, using visual context at all stages. We show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at https://rt-hierarchy.github.io.},
	language = {en-US},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Belkhale, Suneel and Ding, Tianli and Xiao, Ted and Sermanet, Pierre and Vuong, Quon and Tompson, Jonathan and Chebotar, Yevgen and Dwibedi, Debidatta and Sadigh, Dorsa},
	month = mar,
	year = {2024},
	note = {GSCC: 0000053 2025-03-07T01:47:49.911Z 
arXiv:2403.01823 [cs]
Issue: arXiv:2403.01823
remark: RT-H通过语言分层提升机器人任务执行能力。
TLDR: The method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this and the high-level task, it predicts actions, using visual context at all stages, and it shows that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets.},
	keywords = {\# RT-H, Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{huang_embodied_2023,
	title = {An {Embodied} {Generalist} {Agent} in {3D} {World}},
	url = {http://arxiv.org/abs/2311.12871},
	doi = {10.48550/arXiv.2311.12871},
	abstract = {Leveraging massive knowledge and learning schemes from large language models (LLMs), recent machine learning models show notable successes in building generalist agents that exhibit the capability of general-purpose task solving in diverse domains, including natural language processing, computer vision, and robotics. However, a significant challenge remains as these models exhibit limited ability in understanding and interacting with the 3D world. We argue this limitation significantly hinders the current models from performing real-world tasks and further achieving general intelligence. To this end, we introduce an embodied multi-modal and multi-task generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our proposed agent, referred to as LEO, is trained with shared LLM-based model architectures, objectives, and weights in two stages: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning. To facilitate the training, we meticulously curate and generate an extensive dataset comprising object-level and scene-level multi-modal tasks with exceeding scale and complexity, necessitating a deep understanding of and interaction with the 3D world. Through rigorous experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, embodied navigation, and robotic manipulation. Our ablation results further provide valuable insights for the development of future embodied generalist agents.},
	language = {en-US},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
	month = nov,
	year = {2023},
	note = {GSCC: 0000107 2025-03-07T01:47:45.763Z 
arXiv:2311.12871 [cs]
Issue: arXiv:2311.12871
remark: 提出了一种能在3D世界中操作的通用智能体LEO
TLDR: This work introduces LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world and demonstrates its remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation.},
	keywords = {\# LEO, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{octo_model_team_octo_2024,
	title = {Octo: {An} {Open}-{Source} {Generalist} {Robot} {Policy}},
	shorttitle = {Octo},
	url = {http://arxiv.org/abs/2405.12213},
	doi = {10.48550/arXiv.2405.12213},
	abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Octo Model Team and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
	month = may,
	year = {2024},
	note = {GSCC: 0000215 2025-03-07T01:47:38.371Z 
arXiv:2405.12213 [cs]
remark: 介绍Octo通用机器人策略模型及其微调能力。
TLDR: This work introduces Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date, and demonstrates that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces.},
	keywords = {\# Octo, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chi_diffusion_2024,
	title = {Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion}},
	shorttitle = {Diffusion {Policy}},
	url = {http://arxiv.org/abs/2303.04137},
	doi = {10.48550/arXiv.2303.04137},
	abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
	language = {en-US},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
	month = mar,
	year = {2024},
	note = {GSCC: 0000595 2025-03-07T01:47:33.724Z 
arXiv:2303.04137 [cs]
remark: 扩散策略通过动作扩散生成机器人行为。
TLDR: This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process, and finds that it consistently outperforms existing state-of-the-art robot learning methods.},
	keywords = {\# Diffusion Policy, Computer Science - Robotics},
}

@misc{brohan_rt-2_2023,
	title = {{RT}-2: {Vision}-{Language}-{Action} {Models} {Transfer} {Web} {Knowledge} to {Robotic} {Control}},
	shorttitle = {{RT}-2},
	url = {http://arxiv.org/abs/2307.15818},
	doi = {10.48550/arXiv.2307.15818},
	abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
	language = {en-US},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	month = jul,
	year = {2023},
	note = {GSCC: 0000848 2025-03-07T01:47:30.718Z 
arXiv:2307.15818
TLDR: This work proposes a simple, general recipe to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web.
remark: 视觉语言行动模型提升机器人控制能力。},
	keywords = {\# RT-2, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{brohan_rt-1_2023,
	title = {{RT}-1: {Robotics} {Transformer} for {Real}-{World} {Control} at {Scale}},
	shorttitle = {{RT}-1},
	url = {http://arxiv.org/abs/2212.06817},
	doi = {10.48550/arXiv.2212.06817},
	abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
	language = {en-US},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	month = aug,
	year = {2023},
	note = {GSCC: 0000979 2025-03-07T01:47:25.790Z 
arXiv:2212.06817 [cs]
remark: 推出适用于机器人的转换器模型。
TLDR: This paper presents a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties and verify the conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.},
	keywords = {\# RT-1, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{noauthor_survey_nodate,
	title = {A {Survey} of {Autonomous} {Driving}: {Common} {Practices} and {Emerging} {Technologies} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/9046805},
	urldate = {2025-03-06},
}

@misc{noauthor_x-plane_nodate,
	title = {X-{Plane} {\textbar} {The} world’s most advanced flight simulator.},
	url = {https://www.x-plane.com/},
	urldate = {2025-03-06},
}

@misc{noauthor__nodate-1,
	title = {首页 {\textbar} 精准仿真引领者},
	url = {http://www.pd-automotive.com/pc/#/},
	urldate = {2025-03-06},
}

@misc{noauthor_tencenttad_sim_2025,
	title = {Tencent/{TAD}\_Sim},
	url = {https://github.com/Tencent/TAD_Sim},
	abstract = {腾讯自动驾驶仿真系统 TAD Sim (Tencent Autonomous Driving Simulation) 单机版是腾讯自动驾驶以建立更加安全和高效的自动驾驶测试工具为目标, 为自动驾驶系统研发和验证而量身定做的跨平台分布式系统.},
	urldate = {2025-03-06},
	publisher = {Tencent},
	month = mar,
	year = {2025},
	note = {original-date: 2024-08-01T06:43:06Z},
}

@misc{noauthor_virtual_nodate,
	title = {Virtual {Test} {Drive} {\textbar} {Hexagon}},
	url = {https://hexagon.com/products/virtual-test-drive},
	urldate = {2025-03-06},
}

@misc{noauthor_panosim-_nodate,
	title = {{PanoSim}-领先的自动驾驶虚拟仿真软件，工业级驾驶模拟器，满足智能网联智驾研发需求},
	url = {https://www.panosim.com/},
	urldate = {2025-03-06},
}

@misc{noauthor_apollo_nodate,
	title = {Apollo开发者社区},
	url = {https://apollo.baidu.com/},
	urldate = {2025-03-06},
}

@misc{noauthor_isaac_nodate,
	title = {Isaac {Sim} - {Robotics} {Simulation} and {Synthetic} {Data} {Generation} {\textbar} {NVIDIA} {Developer}},
	url = {https://developer.nvidia.com/isaac/sim},
	urldate = {2025-03-01},
}

@misc{noauthor_bullet_2022,
	title = {Bullet {Real}-{Time} {Physics} {Simulation} {\textbar} {Home} of {Bullet} and {PyBullet}: physics simulation for games, visual effects, robotics and reinforcement learning.},
	shorttitle = {Bullet {Real}-{Time} {Physics} {Simulation} {\textbar} {Home} of {Bullet} and {PyBullet}},
	url = {https://pybullet.org/wordpress/},
	language = {en-US},
	urldate = {2025-03-01},
	month = mar,
	year = {2022},
}

@misc{noauthor_---_nodate,
	title = {中国信通院-科研能力-权威发布-蓝皮书},
	url = {https://www.caict.ac.cn/kxyj/qwfb/bps/202408/t20240827_491546.htm},
	urldate = {2025-02-27},
}

@misc{noauthor__nodate-2,
	title = {机器人技术 市场规模 {\textbar} {Mordor} {Intelligence}},
	url = {https://www.mordorintelligence.com/zh-CN/industry-reports/robotics-market},
	abstract = {预计2024年机器人市场规模将达到458.5亿美元，并以15.91\%的复合年增长率增长，到2029年将达到959.3亿美元。},
	language = {zh},
	urldate = {2025-02-27},
}

@misc{noauthor_establishing_2024,
	title = {Establishing {Standards} for {Embodied} {AI} – {Communications} of the {ACM}},
	url = {https://cacm.acm.org/blogcacm/establishing-standards-for-embodied-ai/},
	language = {en-US},
	urldate = {2025-02-27},
	month = jul,
	year = {2024},
}

@misc{dai_automated_2024,
	title = {Automated {Creation} of {Digital} {Cousins} for {Robust} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2410.07408},
	doi = {10.48550/arXiv.2410.07408},
	abstract = {Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in digital twins, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.},
	language = {en-US},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Dai, Tianyuan and Wong, Josiah and Jiang, Yunfan and Wang, Chen and Gokmen, Cem and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07408 [cs]
TLDR: It is found that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer.
remark: 提出数字表亲用于增强策略学习。},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_vlabench_2024,
	title = {{VLABench}: {A} {Large}-{Scale} {Benchmark} for {Language}-{Conditioned} {Robotics} {Manipulation} with {Long}-{Horizon} {Reasoning} {Tasks}},
	shorttitle = {{VLABench}},
	url = {http://arxiv.org/abs/2412.18194},
	doi = {10.48550/arXiv.2412.18194},
	abstract = {General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh{\textbackslash}\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks.},
	language = {en-US},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Zhang, Shiduo and Xu, Zhe and Liu, Peiju and Yu, Xiaopeng and Li, Yuan and Gao, Qinghui and Fei, Zhaoye and Yin, Zhangyue and Wu, Zuxuan and Jiang, Yu-Gang and Qiu, Xipeng},
	month = dec,
	year = {2024},
	note = {arXiv:2412.18194 [cs]
TLDR: VLABench is an open-source benchmark for evaluating universal LCM task learning and assesses multiple competencies including understanding of mesh{\textbackslash}\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc.
remark: 提出VLABench评估视觉语言智能体的长远任务能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yang_embodiedbench_2025,
	title = {{EmbodiedBench}: {Comprehensive} {Benchmarking} {Multi}-modal {Large} {Language} {Models} for {Vision}-{Driven} {Embodied} {Agents}},
	shorttitle = {{EmbodiedBench}},
	url = {http://arxiv.org/abs/2502.09560},
	doi = {10.48550/arXiv.2502.09560},
	abstract = {Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9\% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.},
	language = {en-US},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Yang, Rui and Chen, Hanyang and Zhang, Junyu and Zhao, Mark and Qian, Cheng and Wang, Kangrui and Wang, Qineng and Koripella, Teja Venkat and Movahedi, Marziyeh and Li, Manling and Ji, Heng and Zhang, Huan and Zhang, Tong},
	month = feb,
	year = {2025},
	note = {arXiv:2502.09560 [cs]
remark: 引入EmbodiedBench评估视觉驱动的多模态智能体表现。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wei_lmsanitator_2024,
	title = {{LMSanitator}: {Defending} {Prompt}-{Tuning} {Against} {Task}-{Agnostic} {Backdoors}},
	shorttitle = {{LMSanitator}},
	url = {http://arxiv.org/abs/2308.13904},
	doi = {10.14722/ndss.2024.23238},
	abstract = {Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8\% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1\% in most scenarios.},
	language = {en},
	urldate = {2024-03-05},
	booktitle = {Proceedings 2024 {Network} and {Distributed} {System} {Security} {Symposium}},
	author = {Wei, Chengkun and Meng, Wenlong and Zhang, Zhikun and Chen, Min and Zhao, Minghu and Fang, Wenjing and Wang, Lei and Zhang, Zihui and Chen, Wenzhi},
	year = {2024},
	note = {arXiv:2308.13904 [cs]
GSCC: 0000009 
remark: LMSanitator检测并移除提示调优中的任务无关后门。
TLDR: LMSanitator is proposed, a novel approach for detecting and removing task-agnostic backdoors on Transformer models that aims to invert the predefined attack vectors of the pretrained models' output when the input is embedded with triggers, which achieves much better convergence performance and backdoor detection accuracy.},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{lu_poex_2025,
	title = {{POEX}: {Understanding} and {Mitigating} {Policy} {Executable} {Jailbreak} {Attacks} against {Embodied} {AI}},
	shorttitle = {{POEX}},
	url = {http://arxiv.org/abs/2412.16633},
	doi = {10.48550/arXiv.2412.16633},
	abstract = {Embodied AI systems are rapidly evolving due to the integration of LLMs as planning modules, which transform complex instructions into executable policies. However, LLMs are vulnerable to jailbreak attacks, which can generate malicious content. This paper investigates the feasibility and rationale behind applying traditional LLM jailbreak attacks to EAI systems. We aim to answer three questions: (1) Do traditional LLM jailbreak attacks apply to EAI systems? (2) What challenges arise if they do not? and (3) How can we defend against EAI jailbreak attacks? To this end, we first measure existing LLM-based EAI systems using a newly constructed dataset, i.e., the Harmful-RLbench. Our study confirms that traditional LLM jailbreak attacks are not directly applicable to EAI systems and identifies two unique challenges. First, the harmful text does not necessarily constitute harmful policies. Second, even if harmful policies can be generated, they are not necessarily executable by the EAI systems, which limits the potential risk. To facilitate a more comprehensive security analysis, we refine and introduce POEX, a novel red teaming framework that optimizes adversarial suffixes to induce harmful yet executable policies against EAI systems. The design of POEX employs adversarial constraints, policy evaluators, and suffix optimization to ensure successful policy execution while evading safety detection inside an EAI system. Experiments on the real-world robotic arm and simulator using Harmful-RLbench demonstrate the efficacy, highlighting severe safety vulnerabilities and high transferability across models. Finally, we propose prompt-based and model-based defenses, achieving an 85\% success rate in mitigating attacks and enhancing safety awareness in EAI systems. Our findings underscore the urgent need for robust security measures to ensure the safe deployment of EAI in critical applications.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Lu, Xuancun and Huang, Zhengxian and Li, Xinfeng and ji, Xiaoyu and Xu, Wenyuan},
	month = feb,
	year = {2025},
	note = {arXiv:2412.16633 [cs]
TLDR: This paper demystified how traditional LLM jailbreak attacks behave in the Embodied AI context, and proposed Policy Executable (POEX) jailbreak attacks, where harmful instructions and optimized suffixes are injected into LLM-based planning modules, leading embodied AI to perform harmful actions in both simulated and physical environments.
remark: 研究具身智能的策略执行越狱攻击及防御方法。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Robotics},
}

@misc{liu_compromising_2024,
	title = {Compromising {Embodied} {Agents} with {Contextual} {Backdoor} {Attacks}},
	url = {http://arxiv.org/abs/2408.02882},
	doi = {10.48550/arXiv.2408.02882},
	abstract = {Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called {\textbackslash}method\{\}. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning. To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers. We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents. To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning. Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems.},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Liu, Aishan and Zhou, Yuguang and Liu, Xianglong and Zhang, Tianyuan and Liang, Siyuan and Wang, Jiakai and Pu, Yanjun and Li, Tianlin and Zhang, Junqi and Zhou, Wenbo and Guo, Qing and Tao, Dacheng},
	month = aug,
	year = {2024},
	note = {GSCC: 0000005 
arXiv:2408.02882 [cs]
remark: 介绍了一种针对具身智能体的上下文后门攻击方法。
TLDR: A dual-modality activation strategy is implemented that controls both the generation and execution of program defects through textual and visual triggers that compromise key aspects of confidentiality, integrity, and availability in embodied agents.},
	keywords = {\# Contextual Backdoor Attack, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zhang_mfe-etp_2024,
	title = {{MFE}-{ETP}: {A} {Comprehensive} {Evaluation} {Benchmark} for {Multi}-modal {Foundation} {Models} on {Embodied} {Task} {Planning}},
	shorttitle = {{MFE}-{ETP}},
	url = {http://arxiv.org/abs/2407.05047},
	doi = {10.48550/arXiv.2407.05047},
	abstract = {In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial Intelligence (EAI) have been advancing side by side at an unprecedented pace. The integration of the two has garnered significant attention from the AI research community. In this work, we attempt to provide an in-depth and comprehensive evaluation of the performance of MFM s on embodied task planning, aiming to shed light on their capabilities and limitations in this domain. To this end, based on the characteristics of embodied task planning, we first develop a systematic evaluation framework, which encapsulates four crucial capabilities of MFMs: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. Following this, we propose a new benchmark, named MFE-ETP, characterized its complex and variable task scenarios, typical yet diverse task types, task instances of varying difficulties, and rich test case types ranging from multiple embodied question answering to embodied task reasoning. Finally, we offer a simple and easy-to-use automatic evaluation platform that enables the automated testing of multiple MFMs on the proposed benchmark. Using the benchmark and evaluation platform, we evaluated several state-of-the-art MFMs and found that they significantly lag behind human-level performance. The MFE-ETP is a high-quality, large-scale, and challenging benchmark relevant to real-world tasks.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Zhang, Min and Fu, Xian and Hao, Jianye and Han, Peilong and Zhang, Hao and Shi, Lei and Tang, Hongyao and Zheng, Yan},
	month = oct,
	year = {2024},
	note = {arXiv:2407.05047 [cs]
TLDR: This work attempts to provide an in-depth and comprehensive evaluation of the performance of MFM s on embodied task planning, aiming to shed light on their capabilities and limitations in this domain.
remark: MFE-ETP评估多模态模型在具身任务规划中的表现。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{jiang_powerradio_2024,
	title = {{PowerRadio}: {Manipulate} {Sensor} {Measurementvia} {Power} {GND} {Radiation}},
	shorttitle = {{PowerRadio}},
	url = {http://arxiv.org/abs/2412.18103},
	doi = {10.48550/arXiv.2412.18103},
	abstract = {Sensors are key components enabling various applications, e.g., home intrusion detection and environmental monitoring. While various software defenses and physical protections are used to prevent sensor manipulation, this paper introduces a new threat vector, PowerRadio, that bypasses existing protections and changes sensor readings from a distance. PowerRadio leverages interconnected ground (GND) wires, a standard practice for electrical safety at home, to inject malicious signals. The injected signal is coupled by the sensor's analog measurement wire and eventually survives the noise filters, inducing incorrect measurement. We present three methods to manipulate sensors by inducing static bias, periodical signals, or pulses. For instance, we show adding stripes into the captured images of a surveillance camera or injecting inaudible voice commands into conference microphones. We study the underlying principles of PowerRadio and identify its root causes: (1) the lack of shielding between ground and data signal wires and (2) the asymmetry of circuit impedance that enables interference to bypass filtering. We validate PowerRadio against a surveillance system, broadcast systems, and various sensors. We believe that PowerRadio represents an emerging threat, exhibiting the advantages of both radiated and conducted EMI, e.g., expanding the effective attack distance of radiated EMI yet eliminating the requirement of line-of-sight or approaching physically. Our insights shall provide guidance for enhancing the sensors' security and power wiring during the design phases.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Jiang, Yan and Ji, Xiaoyu and Jiang, Yancheng and Wang, Kai and Xu, Chenren and Xu, Wenyuan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.18103 [eess]
GSCC: 0000000 
remark: PowerRadio通过电源地线辐射远程操控传感器。},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
}

@misc{kok_when_2024,
	title = {When {IoT} {Meet} {LLMs}: {Applications} and {Challenges}},
	shorttitle = {When {IoT} {Meet} {LLMs}},
	url = {http://arxiv.org/abs/2411.17722},
	doi = {10.48550/arXiv.2411.17722},
	abstract = {Recent advances in Large Language Models (LLMs) have positively and efficiently transformed workflows in many domains. One such domain with significant potential for LLM integration is the Internet of Things (IoT), where this integration brings new opportunities for improved decision making and system interaction. In this paper, we explore the various roles of LLMs in IoT, with a focus on their reasoning capabilities. We show how LLM-IoT integration can facilitate advanced decision making and contextual understanding in a variety of IoT scenarios. Furthermore, we explore the integration of LLMs with edge, fog, and cloud computing paradigms, and show how this synergy can optimize resource utilization, enhance real-time processing, and provide scalable solutions for complex IoT applications. To the best of our knowledge, this is the first comprehensive study covering IoT-LLM integration between edge, fog, and cloud systems. Additionally, we propose a novel system model for industrial IoT applications that leverages LLM-based collective intelligence to enable predictive maintenance and condition monitoring. Finally, we highlight key challenges and open issues that provide insights for future research in the field of LLM-IoT integration.},
	language = {en-US},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Kok, Ibrahim and Demirci, Orhan and Ozdemir, Suat},
	month = nov,
	year = {2024},
	note = {arXiv:2411.17722
GSCC: 0000000 
remark: LLM与IoT结合提升决策和系统交互。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}

@misc{yang_diffusion_2024,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	language = {en-US},
	urldate = {2025-01-25},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = dec,
	year = {2024},
	note = {arXiv:2209.00796 [cs]
GSCC: 0001506 
remark: 扩散模型综述及应用领域分析。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-12-25},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {GSCC: 0001656 
arXiv:2407.21783 [cs]
TLDR: It is found that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks, and performs competitively with the state-of-the-art on image, video, and speech recognition tasks.
remark: Llama 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{turing_computing_2009,
	address = {Dordrecht},
	title = {Computing {Machinery} and {Intelligence}},
	isbn = {978-1-4020-6710-5},
	url = {https://doi.org/10.1007/978-1-4020-6710-5_3},
	abstract = {I propose to consider the question, “Can machines think?”♣ This should begin with definitions of the meaning of the terms “machine” and “think”. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words “machine” and “think” are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, “Can machines think?” is to be sought in a statistical survey such as a Gallup poll.},
	language = {en},
	urldate = {2024-12-18},
	booktitle = {Parsing the {Turing} {Test}: {Philosophical} and {Methodological} {Issues} in the {Quest} for the {Thinking} {Computer}},
	publisher = {Springer Netherlands},
	author = {Turing, Alan M.},
	editor = {Epstein, Robert and Roberts, Gary and Beber, Grace},
	year = {2009},
	doi = {10.1007/978-1-4020-6710-5_3},
	note = {GSCC: 0026150 
remark: 机器能否思考需重新定义相关术语。},
	keywords = {Computing Machinery, Digital Computer, Performance Capacity, Real Robot, Turing Machine},
	pages = {23--65},
}

@misc{wang_trojanrobot_2025,
	title = {{TrojanRobot}: {Physical}-{World} {Backdoor} {Attacks} {Against} {VLM}-based {Robotic} {Manipulation}},
	shorttitle = {{TrojanRobot}},
	url = {http://arxiv.org/abs/2411.11683},
	doi = {10.48550/arXiv.2411.11683},
	abstract = {Robotic manipulation in the physical world is increasingly empowered by {\textbackslash}textit\{large language models\} (LLMs) and {\textbackslash}textit\{vision-language models\} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose {\textbackslash}textit\{TrojanRobot\}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, {\textbackslash}ie, {\textbackslash}textit\{permutation\}, {\textbackslash}textit\{stagnation\}, and {\textbackslash}textit\{intentional\} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link {\textbackslash}url\{https://trojanrobot.github.io\}.},
	language = {en-US},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Wang, Xianlong and Pan, Hewen and Zhang, Hangtao and Li, Minghui and Hu, Shengshan and Zhou, Ziqi and Xue, Lulu and Guo, Peijin and Wang, Yichen and Wan, Wei and Liu, Aishan and Zhang, Leo Yu},
	month = jan,
	year = {2025},
	note = {GSCC: 0000000 
arXiv:2411.11683 [cs]
TLDR: By embedding a backdoor visual language model into the visual perception module within the robotic system, this research successfully mislead the robotic arm’s operation in the physical world, given the presence of common items as triggers.
remark: 物理世界的VLM机器人后门攻击},
	keywords = {\# TrojanRobot, Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{li_safe_2024,
	title = {Safe {Planner}: {Empowering} {Safety} {Awareness} in {Large} {Pre}-{Trained} {Models} for {Robot} {Task} {Planning}},
	shorttitle = {Safe {Planner}},
	url = {http://arxiv.org/abs/2411.06920},
	doi = {10.48550/arXiv.2411.06920},
	abstract = {Robot task planning is an important problem for autonomous robots in long-horizon challenging tasks. As large pre-trained models have demonstrated superior planning ability, recent research investigates utilizing large models to achieve autonomous planning for robots in diverse tasks. However, since the large models are pre-trained with Internet data and lack the knowledge of real task scenes, large models as planners may make unsafe decisions that hurt the robots and the surrounding environments. To solve this challenge, we propose a novel Safe Planner framework, which empowers safety awareness in large pre-trained models to accomplish safe and executable planning. In this framework, we develop a safety prediction module to guide the high-level large model planner, and this safety module trained in a simulator can be effectively transferred to real-world tasks. The proposed Safe Planner framework is evaluated on both simulated environments and real robots. The experiment results demonstrate that Safe Planner not only achieves state-of-the-art task success rates, but also substantially improves safety during task execution. The experiment videos are shown in https://sites.google.com/view/safeplanner .},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Li, Siyuan and Ma, Zhe and Liu, Feifan and Lu, Jiani and Xiao, Qinqin and Sun, Kewu and Cui, Lingfei and Yang, Xirui and Liu, Peng and Wang, Xun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06920
GSCC: 0000000 
remark: 大模型安全规划提升机器人任务安全性。
TLDR: A novel Safe Planner framework is proposed, which empowers safety awareness in large pre-trained models to accomplish safe and executable planning and achieves state-of-the-art task success rates, but also substantially improves safety during task execution.},
	keywords = {Computer Science - Robotics, ⏳},
}

@misc{zhu_riskawarebench_2024,
	title = {{RiskAwareBench}: {Towards} {Evaluating} {Physical} {Risk} {Awareness} for {High}-level {Planning} of {LLM}-based {Embodied} {Agents}},
	shorttitle = {{RiskAwareBench}},
	url = {http://arxiv.org/abs/2408.04449},
	doi = {10.48550/arXiv.2408.04449},
	abstract = {The integration of large language models (LLMs) into robotics significantly enhances the capabilities of embodied agents in understanding and executing complex natural language instructions. However, the unmitigated deployment of LLM-based embodied systems in real-world environments may pose potential physical risks, such as property damage and personal injury. Existing security benchmarks for LLMs overlook risk awareness for LLM-based embodied agents. To address this gap, we propose RiskAwareBench, an automated framework designed to assess physical risks awareness in LLM-based embodied agents. RiskAwareBench consists of four modules: safety tips generation, risky scene generation, plan generation, and evaluation, enabling comprehensive risk assessment with minimal manual intervention. Utilizing this framework, we compile the PhysicalRisk dataset, encompassing diverse scenarios with associated safety tips, observations, and instructions. Extensive experiments reveal that most LLMs exhibit insufficient physical risk awareness, and baseline risk mitigation strategies yield limited enhancement, which emphasizes the urgency and cruciality of improving risk awareness in LLM-based embodied agents in the future.},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Zhu, Zihao and Wu, Bingzhe and Zhang, Zhengyou and Wu, Baoyuan},
	month = aug,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2408.04449 [cs]
remark: 提出评估LLM具身智能体物理风险感知的方法框架。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{fu_misusing_2023,
	title = {Misusing {Tools} in {Large} {Language} {Models} {With} {Visual} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/2310.03185},
	doi = {10.48550/arXiv.2310.03185},
	abstract = {Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always ({\textasciitilde}98\%) while maintaining high similarity to clean images ({\textasciitilde}0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.},
	language = {en-US},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K. and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence},
	month = oct,
	year = {2023},
	note = {GSCC: 0000023 
arXiv:2310.03185 [cs]
remark: 攻击者可用视觉对抗样本误导大语言模型使用工具。
TLDR: This work shows that an attacker can use visual adversarial examples to cause attacker-desired tool usage to cause a victim LLM to delete calendar events, leak private conversations and book hotels.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{yang_plug_2023,
	title = {Plug in the {Safety} {Chip}: {Enforcing} {Constraints} for {LLM}-driven {Robot} {Agents}},
	shorttitle = {Plug in the {Safety} {Chip}},
	url = {http://arxiv.org/abs/2309.09919},
	abstract = {Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the “dos”, the “don’ts” received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the “don’ts”: conveying explicit instructions about prohibited actions, assessing the robot’s comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.},
	language = {en},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Yang, Ziyi and Raman, Shreyas S. and Shah, Ankit and Tellex, Stefanie},
	month = nov,
	year = {2023},
	note = {GSCC: 0000026 
arXiv:2309.09919 [cs]
Issue: arXiv:2309.09919
remark: 提出基于LTL的机器人安全约束模块。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Formal Languages and Automata Theory, Computer Science - Robotics},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: {Compromising} {Real}-{World} {LLM}-{Integrated} {Applications} with {Indirect} {Prompt} {Injection}},
	shorttitle = {Not what you've signed up for},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	abstract = {Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	language = {en-US},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	month = may,
	year = {2023},
	note = {GSCC: 0000378 
arXiv:2302.12173 [cs]
remark: 间接提示注入攻击可危害大语言模型应用。
TLDR: This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{wang_all_2024,
	title = {All {Robots} in {One}: {A} {New} {Standard} and {Unified} {Dataset} for {Versatile}, {General}-{Purpose} {Embodied} {Agents}},
	shorttitle = {All {Robots} in {One}},
	url = {http://arxiv.org/abs/2408.10899},
	doi = {10.48550/arXiv.2408.10899},
	abstract = {Embodied AI is transforming how AI systems interact with the physical world, yet existing datasets are inadequate for developing versatile, general-purpose agents. These limitations include a lack of standardized formats, insufficient data diversity, and inadequate data volume. To address these issues, we introduce ARIO (All Robots In One), a new data standard that enhances existing datasets by offering a unified data format, comprehensive sensory modalities, and a combination of real-world and simulated data. ARIO aims to improve the training of embodied AI agents, increasing their robustness and adaptability across various tasks and environments. Building upon the proposed new standard, we present a large-scale unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064 tasks. The ARIO standard and dataset represent a significant step towards bridging the gaps of existing data resources. By providing a cohesive framework for data collection and representation, ARIO paves the way for the development of more powerful and versatile embodied AI agents, capable of navigating and interacting with the physical world in increasingly complex and diverse ways. The project is available on https://imaei.github.io/project\_pages/ario/},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Wang, Zhiqiang and Zheng, Hao and Nie, Yunshuang and Xu, Wenjun and Wang, Qingwei and Ye, Hua and Li, Zhe and Zhang, Kaidong and Cheng, Xuewen and Dong, Wanxi and Cai, Chang and Lin, Liang and Zheng, Feng and Liang, Xiaodan},
	month = aug,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2408.10899
TLDR: A large-scale unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064 tasks, is presented, representing a significant step towards bridging the gaps of existing data resources.
remark: ARIO 提供统一数据标准与多样数据集。},
	keywords = {Computer Science - Robotics},
}

@misc{noauthor_easyconnect_nodate,
	title = {{EasyConnect}},
	url = {https://121.17.149.195:64900/portal/#!/down_client},
	urldate = {2025-01-25},
}

@misc{gao_embodiedcity_2024,
	title = {{EmbodiedCity}: {A} {Benchmark} {Platform} for {Embodied} {Agent} in {Real}-world {City} {Environment}},
	shorttitle = {{EmbodiedCity}},
	url = {http://arxiv.org/abs/2410.09604},
	doi = {10.48550/arXiv.2410.09604},
	abstract = {Embodied artificial intelligence emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties.},
	language = {en-US},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Gao, Chen and Zhao, Baining and Zhang, Weichen and Mao, Jinzhu and Zhang, Jun and Zheng, Zhiheng and Man, Fanhang and Fang, Jianjie and Zhou, Zile and Cui, Jinqiang and Chen, Xinlei and Li, Yong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09604 [cs]
TLDR: A benchmark platform for embodied intelligence evaluation in real-world city environments is constructed and some popular large language models for embodied intelligence capabilities of different dimensions and difficulties are evaluated.
remark: 构建现实城市环境的具身智能评估平台。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{jin_marple_2024,
	title = {{MARPLE}: {A} {Benchmark} for {Long}-{Horizon} {Inference}},
	shorttitle = {{MARPLE}},
	url = {http://arxiv.org/abs/2410.01926},
	doi = {10.48550/arXiv.2410.01926},
	abstract = {Reconstructing past events requires reasoning across long time horizons. To figure out what happened, we need to use our prior knowledge about the world and human behavior and draw inferences from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic ``whodunit'' stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze what factors influence inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Jin, Emily and Huang, Zhuoyi and Fränken, Jan-Philipp and Liu, Weiyu and Cha, Hannah and Brockbank, Erik and Wu, Sarah and Zhang, Ruohan and Wu, Jiajun and Gerstenberg, Tobias},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01926 [cs]
TLDR: This work introduces MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence, and asks AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened.
remark: 论文引入长视距推断评估新基准MARPLE。},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_embodied_2024,
	title = {Embodied {Agent} {Interface}: {Benchmarking} {LLMs} for {Embodied} {Decision} {Making}},
	shorttitle = {Embodied {Agent} {Interface}},
	url = {http://arxiv.org/abs/2410.07166},
	doi = {10.48550/arXiv.2410.07166},
	abstract = {We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.},
	language = {en-US},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Li, Manling and Zhao, Shiyu and Wang, Qineng and Wang, Kangrui and Zhou, Yu and Srivastava, Sanjana and Gokmen, Cem and Lee, Tony and Li, Li Erran and Zhang, Ruohan and Liu, Weiyu and Liang, Percy and Fei-Fei, Li and Mao, Jiayuan and Wu, Jiajun},
	month = nov,
	year = {2024},
	note = {arXiv:2410.07166 [cs]
TLDR: A comprehensive assessment of LLMs' performance for different subtasks is offered, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.
remark: LLMs在具身决策中的评估框架及优化。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jia_improved_2024,
	title = {Improved {Techniques} for {Optimization}-{Based} {Jailbreaking} on {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.21018},
	doi = {10.48550/arXiv.2405.21018},
	abstract = {Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100\% attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.},
	language = {en-US},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Jia, Xiaojun and Pang, Tianyu and Du, Chao and Huang, Yihao and Gu, Jindong and Liu, Yang and Cao, Xiaochun and Lin, Min},
	month = jun,
	year = {2024},
	note = {arXiv:2405.21018 [cs]
GSCC: 0000015 
remark: 提出并改进了优化基越狱方法I-GCG。
TLDR: Improved techniques for optimization-based jailbreaks like GCG are presented and demonstrated that they can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100\% attack success rate.},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{hu_toward_2023,
	title = {Toward {General}-{Purpose} {Robots} via {Foundation} {Models}: {A} {Survey} and {Meta}-{Analysis}},
	shorttitle = {Toward {General}-{Purpose} {Robots} via {Foundation} {Models}},
	url = {https://arxiv.org/abs/2312.08782v2},
	abstract = {Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.},
	language = {en},
	urldate = {2024-01-11},
	journal = {arXiv.org},
	author = {Hu, Yafei and Xie, Quanting and Jain, Vidhi and Francis, Jonathan and Patrikar, Jay and Keetha, Nikhil and Kim, Seungchan and Xie, Yaqi and Zhang, Tianyi and Zhao, Shibo and Chong, Yu Quan and Wang, Chen and Sycara, Katia and Johnson-Roberson, Matthew and Batra, Dhruv and Wang, Xiaolong and Scherer, Sebastian and Kira, Zsolt and Xia, Fei and Bisk, Yonatan},
	month = dec,
	year = {2023},
	note = {GSCC: 0000059 
remark: 基础模型促进通用机器人发展探讨。},
	keywords = {/done},
}

@misc{abdin_phi-3_2024,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	doi = {10.48550/arXiv.2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench). Moreover, we also introduce phi-3-vision, a 4.2 billion parameter model based on phi-3-mini with strong reasoning capabilities for image and text prompts.},
	language = {en-US},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Qin and Cai, Martin and Mendes, Caio César Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Yen-Chun and Chen, Yi-Ling and Chopra, Parul and Dai, Xiyang and Del Giorno, Allie and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Iter, Dan and Gao, Mei and Gao, Min and Gao, Jianfeng and Garg, Amit and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Liu, Ce and Liu, Mengchen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Luo, Chong and Madan, Piyush and Mazzola, Matt and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Wang, Xin and Wang, Lijuan and Wang, Chunyu and Wang, Yu and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wu, Haiping and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Ziyi and Yang, Yifan and Yu, Donghan and Yuan, Lu and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
	month = may,
	year = {2024},
	note = {GSCC: 0000608 
arXiv:2404.14219 [cs]
remark: Phi-3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bai_qwen_2023,
	title = {Qwen {Technical} {Report}},
	url = {http://arxiv.org/abs/2309.16609},
	doi = {10.48550/arXiv.2309.16609},
	abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
	language = {en-US},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang, An and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
	month = sep,
	year = {2023},
	note = {GSCC: 0001622 
arXiv:2309.16609 [cs]
remark: Qwen
TLDR: Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts, and includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques.},
	keywords = {Computer Science - Computation and Language},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {GSCC: 0037765 
arXiv:2005.14165 [cs]
remark: GPT-3},
	keywords = {Computer Science - Computation and Language},
}

@misc{caffagni_revolution_2024,
	title = {The {Revolution} of {Multimodal} {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {The {Revolution} of {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.12451},
	doi = {10.48550/arXiv.2402.12451},
	abstract = {Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Caffagni, Davide and Cocchi, Federico and Barsellotti, Luca and Moratelli, Nicholas and Sarto, Sara and Baraldi, Lorenzo and Baraldi, Lorenzo and Cornia, Marcella and Cucchiara, Rita},
	month = jun,
	year = {2024},
	note = {GSCC: 0000031 
arXiv:2402.12451
remark: 综述多模态大语言模型的架构与应用。},
	keywords = {\# 架构 训练 任务, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	doi = {10.48550/arXiv.1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	language = {en-US},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {GSCC: 0010419 
arXiv:1608.04644 [cs]
remark: CW},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{chen_object_2020,
	title = {Object as {Hotspots}: {An} {Anchor}-{Free} {3D} {Object} {Detection} {Approach} via {Firing} of {Hotspots}},
	shorttitle = {Object as {Hotspots}},
	url = {http://arxiv.org/abs/1912.12791},
	doi = {10.48550/arXiv.1912.12791},
	abstract = {Accurate 3D object detection in LiDAR based point clouds suffers from the challenges of data sparsity and irregularities. Existing methods strive to organize the points regularly, e.g. voxelize, pass them through a designed 2D/3D neural network, and then define object-level anchors that predict offsets of 3D bounding boxes using collective evidences from all the points on the objects of interest. Contrary to the state-of-the-art anchor-based methods, based on the very nature of data sparsity, we observe that even points on an individual object part are informative about semantic information of the object. We thus argue in this paper for an approach opposite to existing methods using object-level anchors. Inspired by compositional models, which represent an object as parts and their spatial relations, we propose to represent an object as composition of its interior non-empty voxels, termed hotspots, and the spatial relations of hotspots. This gives rise to the representation of Object as Hotspots (OHS). Based on OHS, we further propose an anchor-free detection head with a novel ground truth assignment strategy that deals with inter-object point-sparsity imbalance to prevent the network from biasing towards objects with more points. Experimental results show that our proposed method works remarkably well on objects with a small number of points. Notably, our approach ranked 1st on KITTI 3D Detection Benchmark for cyclist and pedestrian detection, and achieved state-of-the-art performance on NuScenes 3D Detection Benchmark.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Chen, Qi and Sun, Lin and Wang, Zhixin and Jia, Kui and Yuille, Alan},
	month = oct,
	year = {2020},
	note = {GSCC: 0000198 
arXiv:1912.12791 [cs]
Issue: arXiv:1912.12791
remark: 提出无锚点基于热点的3D目标检测方法。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_multi-view_2017,
	title = {Multi-{View} {3D} {Object} {Detection} {Network} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1611.07759},
	doi = {10.48550/arXiv.1611.07759},
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	month = jun,
	year = {2017},
	note = {GSCC: 0003719 
arXiv:1611.07759 [cs]
Issue: arXiv:1611.07759
remark: 提出MV3D融合方案提升自动驾驶3D物体检测。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{choudhary_talk2bev_2023,
	title = {{Talk2BEV}: {Language}-enhanced {Bird}'s-eye {View} {Maps} for {Autonomous} {Driving}},
	shorttitle = {{Talk2BEV}},
	url = {http://arxiv.org/abs/2310.02251},
	doi = {10.48550/arXiv.2310.02251},
	abstract = {Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Choudhary, Tushar and Dewangan, Vikrant and Chandhok, Shivam and Priyadarshan, Shubham and Jain, Anushka and Singh, Arun K. and Srivastava, Siddharth and Jatavallabhula, Krishna Murthy and Krishna, K. Madhava},
	month = nov,
	year = {2023},
	note = {GSCC: 0000040 
arXiv:2310.02251 [cs]
remark: 融合语言视觉模型的自动驾驶BEV地图。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	language = {en-US},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {GSCC: 0005281 
arXiv:2204.02311 [cs]
remark: PaLM Model},
	keywords = {Computer Science - Computation and Language},
}

@misc{collaboration_open_2024,
	title = {Open {X}-{Embodiment}: {Robotic} {Learning} {Datasets} and {RT}-{X} {Models}},
	shorttitle = {Open {X}-{Embodiment}},
	url = {http://arxiv.org/abs/2310.08864},
	doi = {10.48550/arXiv.2310.08864},
	abstract = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.},
	language = {en-US},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Collaboration, Open X.-Embodiment and O'Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and Tung, Albert and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Gupta, Anchit and Wang, Andrew and Singh, Anikait and Garg, Animesh and Kembhavi, Aniruddha and Xie, Annie and Brohan, Anthony and Raffin, Antonin and Sharma, Archit and Yavary, Arefeh and Jain, Arhan and Balakrishna, Ashwin and Wahid, Ayzaan and Burgess-Limerick, Ben and Kim, Beomjoon and Schölkopf, Bernhard and Wulfe, Blake and Ichter, Brian and Lu, Cewu and Xu, Charles and Le, Charlotte and Finn, Chelsea and Wang, Chen and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Agia, Christopher and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Xu, Danfei and Morton, Daniel and Driess, Danny and Chen, Daphne and Pathak, Deepak and Shah, Dhruv and Büchler, Dieter and Jayaraman, Dinesh and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Foster, Ethan and Liu, Fangchen and Ceola, Federico and Xia, Fei and Zhao, Feiyu and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S. and Salhotra, Gautam and Yan, Ge and Feng, Gilbert and Schiavi, Giulio and Berseth, Glen and Kahn, Gregory and Wang, Guanzhi and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Bao, Henghui and Amor, Heni Ben and Christensen, Henrik I. and Furuta, Hiroki and Walke, Homer and Fang, Hongjie and Ha, Huy and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and Abou-Chakra, Jad and Kim, Jaehyung and Drake, Jaimyn and Peters, Jan and Schneider, Jan and Hsu, Jasmine and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jeffrey and Gao, Jensen and Hu, Jiaheng and Wu, Jiajun and Wu, Jialin and Sun, Jiankai and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Wu, Jimmy and Lu, Jingpei and Yang, Jingyun and Malik, Jitendra and Silvério, João and Hejna, Joey and Booher, Jonathan and Tompson, Jonathan and Yang, Jonathan and Salvador, Jordi and Lim, Joseph J. and Han, Junhyek and Wang, Kaiyuan and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Black, Kevin and Lin, Kevin and Zhang, Kevin and Ehsani, Kiana and Lekkala, Kiran and Ellis, Kirsty and Rana, Krishan and Srinivasan, Krishnan and Fang, Kuan and Singh, Kunal Pratap and Zeng, Kuo-Hao and Hatch, Kyle and Hsu, Kyle and Itti, Laurent and Chen, Lawrence Yunliang and Pinto, Lerrel and Fei-Fei, Li and Tan, Liam and Fan, Linxi "Jim" and Ott, Lionel and Lee, Lisa and Weihs, Luca and Chen, Magnum and Lepert, Marion and Memmel, Marius and Tomizuka, Masayoshi and Itkina, Masha and Castro, Mateo Guaman and Spero, Max and Du, Maximilian and Ahn, Michael and Yip, Michael C. and Zhang, Mingtong and Ding, Mingyu and Heo, Minho and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J. and Suenderhauf, Niko and Liu, Ning and Di Palo, Norman and Shafiullah, Nur Muhammad Mahi and Mees, Oier and Kroemer, Oliver and Bastani, Osbert and Sanketi, Pannag R. and Miller, Patrick "Tree" and Yin, Patrick and Wohlhart, Paul and Xu, Peng and Fagan, Peter David and Mitrano, Peter and Sermanet, Pierre and Abbeel, Pieter and Sundaresan, Priya and Chen, Qiuyu and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and Martín-Martín, Roberto and Baijal, Rohan and Scalise, Rosario and Hendrix, Rose and Lin, Roy and Qian, Runjia and Zhang, Ruohan and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Lin, Shan and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Sonawani, Shubham and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Karamcheti, Siddharth and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Ramamoorthy, Subramanian and Dasari, Sudeep and Belkhale, Suneel and Park, Sungjae and Nair, Suraj and Mirchandani, Suvir and Osa, Takayuki and Gupta, Tanmay and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Kollar, Thomas and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z. and Armstrong, Travis and Darrell, Trevor and Chung, Trinity and Jain, Vidhi and Vanhoucke, Vincent and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Wang, Xiaolong and Zhu, Xinghao and Geng, Xinyang and Liu, Xiyuan and Liangwei, Xu and Li, Xuanlin and Lu, Yao and Ma, Yecheng Jason and Kim, Yejin and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Wu, Yilin and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Cao, Yue and Wu, Yueh-Hua and Tang, Yujin and Zhu, Yuke and Zhang, Yunchu and Jiang, Yunfan and Li, Yunshuang and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Ma, Zehan and Xu, Zhuo and Cui, Zichen Jeff and Zhang, Zichen and Fu, Zipeng and Lin, Zipeng},
	month = apr,
	year = {2024},
	note = {GSCC: 0000281 
arXiv:2310.08864 [cs]
Issue: arXiv:2310.08864
remark: 提供标准化Open X-Embodiment数据集和模型，促进机器人通用策略研究。
TLDR: This paper provides datasets in standardized data formats and models to make it possible to explore the possibility of generalist X-robot policy in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies.},
	keywords = {\# Open X-Embodiment, /done, Computer Science - Robotics},
}

@misc{cui_receive_2023,
	title = {Receive, {Reason}, and {React}: {Drive} as {You} {Say} with {Large} {Language} {Models} in {Autonomous} {Vehicles}},
	shorttitle = {Receive, {Reason}, and {React}},
	url = {http://arxiv.org/abs/2310.08034},
	doi = {10.48550/arXiv.2310.08034},
	abstract = {The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions, and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles.},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Wang, Ziran},
	month = oct,
	year = {2023},
	note = {GSCC: 0000068 
arXiv:2310.08034 [cs]
remark: 将 LLM 集成到自动驾驶汽车的各种模块（如决策、运动规划和人车通信）},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
}

@misc{cui_large_2023,
	title = {Large {Language} {Models} for {Autonomous} {Driving}: {Real}-{World} {Experiments}},
	shorttitle = {Large {Language} {Models} for {Autonomous} {Driving}},
	url = {https://arxiv.org/abs/2312.09397v2},
	abstract = {Autonomous driving systems are increasingly popular in today's technological landscape, where vehicles with partial automation have already been widely available on the market, and the full automation era with "driverless" capabilities is near the horizon. However, accurately understanding humans' commands, particularly for autonomous vehicles that have only passengers instead of drivers, and achieving a high level of personalization remain challenging tasks in the development of autonomous driving systems. In this paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive (Talk2Drive) to process verbal commands from humans and make autonomous driving decisions with contextual information, satisfying their personalized preferences for safety, efficiency, and comfort. First, a speech recognition module is developed for Talk2Drive to interpret verbal inputs from humans to textual instructions, which are then sent to LLMs for reasoning. Then, appropriate commands for the Electrical Control Unit (ECU) are generated, achieving a 100\% success rate in executing codes. Real-world experiments show that our framework can substantially reduce the takeover rate for a diverse range of drivers by up to 90.1\%. To the best of our knowledge, Talk2Drive marks the first instance of employing an LLM-based system in a real-world autonomous driving environment.},
	language = {en},
	urldate = {2024-04-08},
	journal = {arXiv.org},
	author = {Cui, Can and Yang, Zichong and Zhou, Yupeng and Ma, Yunsheng and Lu, Juanwu and Li, Lingxi and Chen, Yaobin and Panchal, Jitesh and Wang, Ziran},
	month = dec,
	year = {2023},
	note = {GSCC: 0000016 
remark: 利用LLM实现自动驾驶个性化指令处理},
}

@misc{cui_robustness_2023,
	title = {On the {Robustness} of {Large} {Multimodal} {Models} {Against} {Image} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2312.03777},
	doi = {10.48550/arXiv.2312.03777},
	abstract = {Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thoroughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual adversarial inputs. However, our findings suggest that context provided to the model via prompts, such as questions in a QA pair helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable resilience to such attacks on the ScienceQA task with only an 8.10\% drop in performance compared to their visual counterparts which dropped 99.73\%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished attack effectiveness and improvements in image classification accuracy. This research highlights a previously under-explored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.},
	language = {en-US},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Cui, Xuanming and Aparcedo, Alejandro and Jang, Young Kyun and Lim, Ser-Nam},
	month = dec,
	year = {2023},
	note = {GSCC: 0000024 
arXiv:2312.03777 [cs]
remark: 大视觉语言模型对抗攻击研究。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deng_sophon_2024,
	title = {{SOPHON}: {Non}-{Fine}-{Tunable} {Learning} to {Restrain} {Task} {Transferability} {For} {Pre}-trained {Models}},
	shorttitle = {{SOPHON}},
	url = {http://arxiv.org/abs/2404.12699},
	doi = {10.48550/arXiv.2404.12699},
	abstract = {Instead of building deep learning models from scratch, developers are more and more relying on adapting pre-trained models to their customized tasks. However, powerful pre-trained models may be misused for unethical or illegal tasks, e.g., privacy inference and unsafe content generation. In this paper, we introduce a pioneering learning paradigm, non-fine-tunable learning, which prevents the pre-trained model from being fine-tuned to indecent tasks while preserving its performance on the original task. To fulfill this goal, we propose SOPHON, a protection framework that reinforces a given pre-trained model to be resistant to being fine-tuned in pre-defined restricted domains. Nonetheless, this is challenging due to a diversity of complicated fine-tuning strategies that may be adopted by adversaries. Inspired by model-agnostic meta-learning, we overcome this difficulty by designing sophisticated fine-tuning simulation and fine-tuning evaluation algorithms. In addition, we carefully design the optimization process to entrap the pre-trained model within a hard-to-escape local optimum regarding restricted domains. We have conducted extensive experiments on two deep learning modes (classification and generation), seven restricted domains, and six model architectures to verify the effectiveness of SOPHON. Experiment results verify that fine-tuning SOPHON-protected models incurs an overhead comparable to or even greater than training from scratch. Furthermore, we confirm the robustness of SOPHON to three fine-tuning methods, five optimizers, various learning rates and batch sizes. SOPHON may help boost further investigations into safe and responsible AI.},
	language = {en-US},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Deng, Jiangyi and Pang, Shengyuan and Chen, Yanjiao and Xia, Liangming and Bai, Yijie and Weng, Haiqin and Xu, Wenyuan},
	month = apr,
	year = {2024},
	note = {GSCC: 0000004 
arXiv:2404.12699 [cs]
remark: SOPHON框架防止预训练模型被调优至不良任务。},
	keywords = {Computer Science - Machine Learning},
}

@misc{driess_palm-e_2023,
	title = {{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model}},
	shorttitle = {{PaLM}-{E}},
	url = {http://arxiv.org/abs/2303.03378},
	doi = {10.48550/arXiv.2303.03378},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	month = mar,
	year = {2023},
	note = {GSCC: 0001517 
arXiv:2303.03378 [cs]
remark: 提出结合现实感知的多模态语言模型。
TLDR: This work proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts to enable general inference in the real world.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{elsner_taming_2023,
	title = {Taming the {Panda} with {Python}: {A} {Powerful} {Duo} for {Seamless} {Robotics} {Programming} and {Integration}},
	volume = {24},
	issn = {23527110},
	shorttitle = {Taming the {Panda} with {Python}},
	url = {http://arxiv.org/abs/2307.07633},
	doi = {10.1016/j.softx.2023.101532},
	abstract = {Franka Emika robots have gained significant popularity in research and education due to their exceptional versatility and advanced capabilities. This work introduces panda-py - a Python interface and framework designed to empower Franka Emika robotics with accessible and efficient programming. The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively. By leveraging Python's simplicity and readability, users can quickly grasp the necessary programming concepts for robot control and manipulation. Moreover, integrating panda-py with other widely used Python packages in domains such as computer vision and machine learning amplifies the robot's capabilities. Researchers can seamlessly leverage the vast ecosystem of Python libraries, thereby enabling advanced perception, decision-making, and control functionalities. This compatibility facilitates the efficient development of sophisticated robotic applications, integrating state-of-the-art techniques from diverse domains without the added complexity of ROS.},
	urldate = {2024-06-06},
	journal = {SoftwareX},
	author = {Elsner, Jean},
	month = dec,
	year = {2023},
	note = {GSCC: 0000004 
arXiv:2307.07633 [cs]
remark: Python接口panda-py简化Franka Emika机器人的编程。
TLDR: The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively, and facilitates the efficient development of sophisticated robotic applications.},
	keywords = {Computer Science - Robotics},
	pages = {101532},
}

@techreport{hadi_large_2023,
	type = {preprint},
	title = {Large {Language} {Models}: {A} {Comprehensive} {Survey} of its {Applications}, {Challenges}, {Limitations}, and {Future} {Prospects}},
	shorttitle = {Large {Language} {Models}},
	url = {https://www.techrxiv.org/doi/full/10.36227/techrxiv.23589741.v4},
	abstract = {{\textless}p{\textgreater}Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre- trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical, education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for enhancing the robustness and controllability of LLMs, and addressing bias, fairness, and generation quality issues. Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field, this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey{\textless}/p{\textgreater}},
	language = {en-US},
	urldate = {2024-02-26},
	author = {Hadi, Muhammad Usman and Tashi, Qasem Al and Qureshi, Rizwan and Shah, Abbas and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali},
	month = nov,
	year = {2023},
	doi = {10.36227/techrxiv.23589741.v4},
	note = {GSCC: 0000207 
remark: 大语言模型应用与挑战综述},
}

@misc{hau_shadow-catcher_2021,
	title = {Shadow-{Catcher}: {Looking} {Into} {Shadows} to {Detect} {Ghost} {Objects} in {Autonomous} {Vehicle} {3D} {Sensing}},
	shorttitle = {Shadow-{Catcher}},
	url = {http://arxiv.org/abs/2008.12008},
	doi = {10.48550/arXiv.2008.12008},
	abstract = {LiDAR-driven 3D sensing allows new generations of vehicles to achieve advanced levels of situation awareness. However, recent works have demonstrated that physical adversaries can spoof LiDAR return signals and deceive 3D object detectors to erroneously detect "ghost" objects. Existing defenses are either impractical or focus only on vehicles. Unfortunately, it is easier to spoof smaller objects such as pedestrians and cyclists, but harder to defend against and can have worse safety implications. To address this gap, we introduce Shadow-Catcher, a set of new techniques embodied in an end-to-end prototype to detect both large and small ghost object attacks on 3D detectors. We characterize a new semantically meaningful physical invariant (3D shadows) which Shadow-Catcher leverages for validating objects. Our evaluation on the KITTI dataset shows that Shadow-Catcher consistently achieves more than 94\% accuracy in identifying anomalous shadows for vehicles, pedestrians, and cyclists, while it remains robust to a novel class of strong "invalidation" attacks targeting the defense system. Shadow-Catcher can achieve real-time detection, requiring only between 0.003s-0.021s on average to process an object in a 3D point cloud on commodity hardware and achieves a 2.17x speedup compared to prior work},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Hau, Zhongyuan and Demetriou, Soteris and Muñoz-González, Luis and Lupu, Emil C.},
	month = may,
	year = {2021},
	note = {GSCC: 0000030 
arXiv:2008.12008 [cs]
Issue: arXiv:2008.12008
remark: Shadow-Catcher通过3D阴影检测激光雷达幻影攻击。},
	keywords = {Computer Science - Cryptography and Security},
}

@article{he_scoring_2024,
	title = {Scoring {Metrics} of {Assessing} {Voiceprint} {Distinctiveness} {Based} on {Speech} {Content} and {Rate}},
	issn = {1941-0018},
	url = {https://ieeexplore.ieee.org/abstract/document/10478153},
	doi = {10.1109/TDSC.2024.3380603},
	abstract = {A voiceprint is the distinctive pattern of human voices widely used for authentication in voice assistants. This paper investigates the impact of speech contents and speech rates on the distinctiveness of voiceprint, and has obtained answers to three questions by studying 2457 speakers and 21,500,000 test samples: 1) What are the influential factors that users can control to affect the distinctiveness of voiceprints? 2) How to quantify the distinctiveness for given speeches, e.g., the speech of wake-up words when activating voice assistants? 3) How to help users select wake-up words and adjust the speech rate to improve distinctiveness levels? To answer those questions, we break down speeches into phones, and experimentally obtain the correlation between false recognition rates and the richness, order, length, and elements of the phones. Then, we define the PROLE Score that can reflect the voice distinctiveness, and evaluate 30 wake-up words of 19 commercial voice assistant products to provide recommendations on selecting secure voiceprint words. We also measure the correlation between false recognition rates and speech rates, and define the TER Score that reveals the distance of distinctiveness from the secure voiceprint, and it guides users to adjust their speech rate to a secure value.},
	language = {en-US},
	urldate = {2024-06-04},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {He, Ruiwen and Cheng, Yushi and Ze, Junning and Li, Xinfeng and Ji, Xiaoyu and Xu, Wenyuan},
	year = {2024},
	note = {GSCC: 0000000 
Conference Name: IEEE Transactions on Dependable and Secure Computing
remark: 声纹独特性评估及提升方法。
TLDR: The impact of speech contents and speech rates on the distinctiveness of voiceprint is investigated, and the PROLE Score that can reflect the voice distinctiveness and the TER Score that reveals the distance of distinctiveness from the secure voiceprint are defined.},
	keywords = {AI security, Analytical models, Authentication, Internet, Personal voice assistants, Phonetics, Spectrogram, Speech recognition, speaker verifition, statistical analysis},
	pages = {1--18},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	language = {en-US},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {GSCC: 0001713 
arXiv:2203.15556
TLDR: This work trains a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data, and reaches a state-of-the-art average accuracy, greater than a 7\% improvement over Gopher.
remark: 在相同计算预算下优化模型大小和训练数据量。},
	keywords = {\# Deepmind, /done, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {GSCC: 0009428 
arXiv:2106.09685 [cs]
Issue: arXiv:2106.09685
remark: 提出了大模型低秩快速适配方法LoRA。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{huang_language_2023,
	title = {Language {Is} {Not} {All} {You} {Need}: {Aligning} {Perception} with {Language} {Models}},
	shorttitle = {Language {Is} {Not} {All} {You} {Need}},
	url = {https://arxiv.org/abs/2302.14045v2},
	abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
	month = feb,
	year = {2023},
	note = {GSCC: 0000451 
remark: 提出多模态大型语言模型Kosmos-1。},
}

@misc{huang_what_2021,
	title = {What {Makes} {Multi}-modal {Learning} {Better} than {Single} ({Provably})},
	url = {http://arxiv.org/abs/2106.04538},
	doi = {10.48550/arXiv.2106.04538},
	abstract = {The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking. Can multi-modal learning provably perform better than uni-modal? In this paper, we answer this question under a most popular multi-modal fusion framework, which firstly encodes features from different modalities into a common latent space and seamlessly maps the latent representations into the task space. We prove that learning with multiple modalities achieves a smaller population risk than only using its subset of modalities. The main intuition is that the former has a more accurate estimate of the latent space representation. To the best of our knowledge, this is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from the generalization perspective. Combining with experiment results, we show that multi-modal learning does possess an appealing formal guarantee.},
	language = {en},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Huang, Yu and Du, Chenzhuang and Xue, Zihui and Chen, Xuanyao and Zhao, Hang and Huang, Longbo},
	month = oct,
	year = {2021},
	note = {GSCC: 0000268 
arXiv:2106.04538 [cs]
remark: 多模态学习理论上胜于单模态。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{james_pyrep_2019,
	title = {{PyRep}: {Bringing} {V}-{REP} to {Deep} {Robot} {Learning}},
	shorttitle = {{PyRep}},
	url = {http://arxiv.org/abs/1906.11176},
	doi = {10.48550/arXiv.1906.11176},
	abstract = {PyRep is a toolkit for robot learning research, built on top of the virtual robotics experimentation platform (V-REP). Through a series of modifications and additions, we have created a tailored version of V-REP built with robot learning in mind. The new PyRep toolkit offers three improvements: (1) a simple and flexible API for robot control and scene manipulation, (2) a new rendering engine, and (3) speed boosts upwards of 10,000x in comparison to the previous Python Remote API. With these improvements, we believe PyRep is the ideal toolkit to facilitate rapid prototyping of learning algorithms in the areas of reinforcement learning, imitation learning, state estimation, mapping, and computer vision.},
	language = {en-US},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {James, Stephen and Freese, Marc and Davison, Andrew J.},
	month = jun,
	year = {2019},
	note = {GSCC: 0000150 
arXiv:1906.11176 [cs]
Issue: arXiv:1906.11176
remark: PyRep是面向机器人学习的工具包。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{james_rlbench_2019,
	title = {{RLBench}: {The} {Robot} {Learning} {Benchmark} \& {Learning} {Environment}},
	shorttitle = {{RLBench}},
	url = {http://arxiv.org/abs/1909.12271},
	abstract = {We present a challenging new benchmark and learning-environment for robot learning: RLBench. The benchmark features 100 completely unique, hand-designed tasks ranging in difficulty, from simple target reaching and door opening, to longer multi-stage tasks, such as opening an oven and placing a tray in it. We provide an array of both proprioceptive observations and visual observations, which include rgb, depth, and segmentation masks from an over-the-shoulder stereo camera and an eye-in-hand monocular camera. Uniquely, each task comes with an infinite supply of demos through the use of motion planners operating on a series of waypoints given during task creation time; enabling an exciting flurry of demonstration-based learning. RLBench has been designed with scalability in mind; new tasks, along with their motion-planned demos, can be easily created and then verified by a series of tools, allowing users to submit their own tasks to the RLBench task repository. This large-scale benchmark aims to accelerate progress in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning. With the benchmark's breadth of tasks and demonstrations, we propose the first large-scale few-shot challenge in robotics. We hope that the scale and diversity of RLBench offers unparalleled research opportunities in the robot learning community and beyond.},
	language = {en-US},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {James, Stephen and Ma, Zicong and Arrojo, David Rovick and Davison, Andrew J.},
	month = sep,
	year = {2019},
	note = {GSCC: 0000520 
arXiv:1909.12271 [cs]
Issue: arXiv:1909.12271
remark: RLBench：一个具有100个任务的机器人学习基准测试平台。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jankowski_red-teaming_2024,
	title = {Red-{Teaming} {Segment} {Anything} {Model}},
	url = {http://arxiv.org/abs/2404.02067},
	doi = {10.48550/arXiv.2404.02067},
	abstract = {Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation.},
	language = {en-US},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Jankowski, Krzysztof and Sobieski, Bartlomiej and Kwiatkowski, Mateusz and Szulc, Jakub and Janik, Michal and Baniecki, Hubert and Biecek, Przemyslaw},
	month = apr,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2404.02067 [cs]
TLDR: A multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks, showing the effectiveness of popular white-box attacks and resistance to black-box attacks and introducing a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels.
remark: 红队分析表明SAM需要增强安全性。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{ji_poltergeist_2021,
	title = {Poltergeist: {Acoustic} {Adversarial} {Machine} {Learning} against {Cameras} and {Computer} {Vision}},
	shorttitle = {Poltergeist},
	url = {https://ieeexplore.ieee.org/document/9519394},
	doi = {10.1109/SP40001.2021.00091},
	abstract = {Autonomous vehicles increasingly exploit computer-vision-based object detection systems to perceive environments and make critical driving decisions. To increase the quality of images, image stabilizers with inertial sensors are added to alleviate image blurring caused by camera jitters. However, such a trend opens a new attack surface. This paper identifies a system-level vulnerability resulting from the combination of the emerging image stabilizer hardware susceptible to acoustic manipulation and the object detection algorithms subject to adversarial examples. By emitting deliberately designed acoustic signals, an adversary can control the output of an inertial sensor, which triggers unnecessary motion compensation and results in a blurred image, even if the camera is stable. The blurred images can then induce object misclassification affecting safety-critical decision making. We model the feasibility of such acoustic manipulation and design an attack framework that can accomplish three types of attacks, i.e., hiding, creating, and altering objects. Evaluation results demonstrate the effectiveness of our attacks against four academic object detectors (YOLO V3/V4/V5 and Fast R-CNN), and one commercial detector (Apollo). We further introduce the concept of AMpLe attacks, a new class of system-level security vulnerabilities resulting from a combination of adversarial machine learning and physics-based injection of information-carrying signals into hardware.},
	urldate = {2024-12-19},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Ji, Xiaoyu and Cheng, Yushi and Zhang, Yuepeng and Wang, Kai and Yan, Chen and Xu, Wenyuan and Fu, Kevin},
	month = may,
	year = {2021},
	note = {GSCC: 0000071 
ISSN: 2375-1207
TLDR: A system-level vulnerability resulting from the combination of the emerging image stabilizer hardware susceptible to acoustic manipulation and the object detection algorithms subject to adversarial examples is identified and the concept of AMpLe attacks is introduced.
remark: 声学攻击可致摄像头图像模糊，影响检测。},
	keywords = {Acoustics, Cameras, Computer vision, Detectors, Hardware, Inertial sensors, Object detection},
	pages = {160--175},
}

@article{ji_sensor-based_2024,
	title = {Sensor-based {IoT} data privacy protection},
	volume = {1},
	copyright = {2024 Springer Nature Limited},
	issn = {2948-1201},
	url = {https://www.nature.com/articles/s44287-024-00073-2},
	doi = {10.1038/s44287-024-00073-2},
	abstract = {Sensors are extensively used in the Internet of Things (IoT) applications, enhancing daily convenience but also raising concerns about privacy leakage. To address this, we advocate for protecting data privacy at the moment it is generated by sensors, rather than trying to secure it afterwards.},
	language = {en},
	number = {7},
	urldate = {2024-12-19},
	journal = {Nature Reviews Electrical Engineering},
	author = {Ji, Xiaoyu and Zhu, Wenjun and Xiao, Shilin and Xu, Wenyuan},
	month = jul,
	year = {2024},
	note = {GSCC: 0000000 
Publisher: Nature Publishing Group
remark: 传感器生成时即保护数据隐私。},
	keywords = {Computer science, Electrical and electronic engineering, Information technology},
	pages = {427--428},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	language = {en-US},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {GSCC: 0001055 
arXiv:2310.06825
TLDR: This work introduces Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency, which leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost.
remark: Mistral 7B},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jiang_mixtral_2024,
	title = {Mixtral of {Experts}},
	url = {http://arxiv.org/abs/2401.04088},
	doi = {10.48550/arXiv.2401.04088},
	abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
	language = {en-US},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = jan,
	year = {2024},
	note = {GSCC: 0001166 
arXiv:2401.04088 [cs]
remark: Mistral 8x7B
TLDR: This work introduces Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model that vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks and provides a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks.},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{jiang_wight_2022,
	title = {{WIGHT}: {Wired} {Ghost} {Touch} {Attack} on {Capacitive} {Touchscreens}},
	shorttitle = {{WIGHT}},
	url = {https://ieeexplore.ieee.org/abstract/document/9833740},
	doi = {10.1109/SP46214.2022.9833740},
	abstract = {The security of capacitive touchscreens is crucial since they have become the primary human-machine interface on smart devices. To the best of our knowledge, this paper presents WIGHT, the first wired attack that creates ghost touches on capacitive touchscreens via charging cables, and can manipulate the victim devices with undesired consequences, e.g., allowing malicious Bluetooth connections, accepting files with viruses, etc. Our study calls for attention to a new threat vector against touchscreens that only requires connecting to a malicious charging port, which could be a public charging station, and is effective across various power adapters and even USB data blockers. Despite the fact that smartphones employ abundant noise reduction and voltage management techniques, we manage to inject carefully crafted signals that can induce ghost touches within a chosen range. The underlying principle is to inject common-mode noises over the power line to avoid being effectively filtered yet affect the touch measurement mechanism, and synchronize the malicious noise with the screen measurement scanning cycles to place the ghost touches at target locations. We achieve three types of attacks: injection attacks that create ghost touches without users touching the screen, alteration attacks that change the detected legitimate touch position, and Denial-of-Service attacks that prevent the device from identifying legitimate touches. Our evaluation on 6 smartphones, 1 tablet, 2 standalone touchscreen panels, 6 power adapters, and 13 charging cables demonstrates the feasibility of all three type attacks.},
	language = {en-US},
	urldate = {2024-07-10},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Jiang, Yan and Ji, Xiaoyu and Wang, Kai and Yan, Chen and Mitev, Richard and Sadeghi, Ahmad-Reza and Xu, Wenyuan},
	month = may,
	year = {2022},
	note = {GSCC: 0000019 
ISSN: 2375-1207
remark: 利用充电线攻击制造电容屏幕鬼触。
TLDR: WIGHT is presented, the first wired attack that creates ghost touches on capacitive touchscreens via charging cables, and can manipulate the victim devices with undesired consequences, e.g., allowing malicious Bluetooth connections, accepting files with viruses, etc.},
	keywords = {Noise measurement, Power measurement, Security, Software, Synchronization, Touch sensitive screens, Touchscreen, Universal Serial Bus, conducted noise, ghost touch},
	pages = {984--1001},
}

@misc{jin_surrealdriver_2023,
	title = {{SurrealDriver}: {Designing} {Generative} {Driver} {Agent} {Simulation} {Framework} in {Urban} {Contexts} based on {Large} {Language} {Model}},
	shorttitle = {{SurrealDriver}},
	url = {http://arxiv.org/abs/2309.13193},
	doi = {10.48550/arXiv.2309.13193},
	abstract = {Simulation plays a critical role in the research and development of autonomous driving and intelligent transportation systems. However, the current simulation platforms exhibit limitations in the realism and diversity of agent behaviors, which impede the transfer of simulation outcomes to the real world. In this paper, we propose a generative driver agent simulation framework based on large language models (LLMs), capable of perceiving complex traffic scenarios and providing realistic driving maneuvers. Notably, we conducted interviews with 24 drivers and used their detailed descriptions of driving behavior as chain-of-thought prompts to develop a `coach agent' module, which can evaluate and assist driver agents in accumulating driving experience and developing human-like driving styles. Through practical simulation experiments and user experiments, we validate the feasibility of this framework in generating reliable driver agents and analyze the roles of each module. The results show that the framework with full architect decreased the collision rate by 81.04\% and increased the human-likeness by 50\%. Our research proposes the first urban context driver agent simulation framework based on LLMs and provides valuable insights into the future of agent simulation for complex tasks.},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Jin, Ye and Shen, Xiaoxi and Peng, Huiling and Liu, Xiaoan and Qin, Jingli and Li, Jiayang and Xie, Jintao and Gao, Peizhong and Zhou, Guyue and Gong, Jiangtao},
	month = sep,
	year = {2023},
	note = {GSCC: 0000049 
arXiv:2309.13193 [cs]
remark: 基于LLM的驾驶仿真框架（短期记忆、长期准则和安全标准）
TLDR: This research proposes the first urban context driver agent simulation framework based on LLMs, capable of perceiving complex traffic scenarios and providing realistic driving maneuvers and provides valuable insights into the future of agent simulation for complex tasks.},
	keywords = {Computer Science - Human-Computer Interaction, H.5.2},
}

@article{jin_unity_2024,
	title = {Unity is {Strength}? {Benchmarking} the {Robustness} of {Fusion}-based {3D} {Object} {Detection} against {Physical} {Sensor} {Attack}},
	abstract = {As a safety-critical application, Autonomous Driving (AD) has received growing attention from security researchers. AD heavily relies on sensors for perception. However, sensors themselves are susceptible to various threats since they are exposed to the environments and vulnerable to malicious or interfering signals. To cope with situations where a sensor might malfunction, Multi Sensor Fusion (MSF) was proposed as a general strategy to enhance the robustness of perception models.},
	language = {en},
	journal = {Unity is Strength},
	author = {Jin, Zizhi},
	year = {2024},
	note = {GSCC: 0000001 
remark: 评估融合3D检测对传感器攻击的鲁棒性。},
}

@inproceedings{jin_pla-lidar_2023,
	title = {{PLA}-{LiDAR}: {Physical} {Laser} {Attacks} against {LiDAR}-based {3D} {Object} {Detection} in {Autonomous} {Vehicle}},
	shorttitle = {{PLA}-{LiDAR}},
	url = {https://ieeexplore.ieee.org/abstract/document/10179458},
	doi = {10.1109/SP46215.2023.10179458},
	abstract = {Autonomous vehicles and robots increasingly exploit LiDAR-based 3D object detection systems to detect obstacles in environment. Correct detection and classification are important to ensure safe driving. Though existing work has demonstrated the feasibility of manipulating point clouds to spoof 3D object detectors, most of the attempts are conducted digitally. In this paper, we investigate the possibility of physically fooling LiDAR-based 3D object detection by injecting adversarial point clouds using lasers. First, we develop a laser transceiver that can inject up to 4200 points, which is 20 times more than prior work, and can measure the scanning cycle of victim LiDARs to schedule the spoofing laser signals. By designing a control signal method that converts the coordinates of point clouds to control signals and an adversarial point cloud optimization method with physical constraints of LiDARs and attack capabilities, we manage to inject spoofing point cloud with desired point cloud shapes into the victim LiDAR physically. We can launch four types of attacks, i.e., naive hiding, record-based creating, optimization-based hiding, and optimization-based creating. Extensive experiments demonstrate the effectiveness of our attacks against two commercial LiDAR and three detectors. We also discuss defense strategies at the sensor and AV system levels.},
	language = {en-US},
	urldate = {2024-06-04},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Jin, Zizhi and Ji, Xiaoyu and Cheng, Yushi and Yang, Bo and Yan, Chen and Xu, Wenyuan},
	month = may,
	year = {2023},
	note = {GSCC: 0000053 
ISSN: 2375-1207
remark: 利用激光攻击欺骗LiDAR实现3D物体检测。
TLDR: This paper develops a laser transceiver that can inject up to 4200 points, which is 20 times more than prior work, and can measure the scanning cycle of victim LiDARs to schedule the spoofing laser signals, and designs a control signal method that converts the coordinates of point clouds to control signals.},
	keywords = {Laser radar, Laser theory, Measurement by laser beam, Object detection, Point cloud compression, Shape, Three-dimensional displays},
	pages = {1822--1839},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	language = {en},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {GSCC: 0002525 
arXiv:2001.08361 [cs, stat]
remark: 语言模型性能随规模呈幂律增长。},
	keywords = {\# Scaling Laws, /done, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	copyright = {2021 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
	language = {en},
	number = {6},
	urldate = {2024-10-29},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = jun,
	year = {2021},
	note = {GSCC: 0004735 
Publisher: Nature Publishing Group
TLDR: Some of the prevailing trends in embedding physics into machine learning are reviewed, some of the current capabilities and limitations are presented and diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems are discussed.
remark: 物理约束学习用于科学问题求解。},
	keywords = {Applied mathematics, Computational science},
	pages = {422--440},
}

@misc{kawaharazuka_real-world_2024,
	title = {Real-{World} {Robot} {Applications} of {Foundation} {Models}: {A} {Review}},
	shorttitle = {Real-{World} {Robot} {Applications} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2402.05741},
	abstract = {Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.},
	language = {en-US},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Kawaharazuka, Kento and Matsushima, Tatsuya and Gambardella, Andrew and Guo, Jiaxian and Paxton, Chris and Zeng, Andy},
	month = feb,
	year = {2024},
	note = {GSCC: 0000030 
arXiv:2402.05741 [cs]
Issue: arXiv:2402.05741
remark: 基础模型在机器人中的实际应用综述。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{ke_segment_2023,
	title = {Segment {Anything} in {High} {Quality}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5f828e38160f31935cfe9f67503ad17c-Abstract-Conference.html},
	language = {en},
	urldate = {2024-07-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ke, Lei and Ye, Mingqiao and Danelljan, Martin and Liu, Yifan and Tai, Yu-Wing and Tang, Chi-Keung and Yu, Fisher},
	month = dec,
	year = {2023},
	note = {GSCC: 0000282 
remark: HQ-SAM},
	pages = {29914--29934},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	language = {en-US},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {GSCC: 0007585 
arXiv:2304.02643 [cs]
remark: Segment Anything},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ku_joint_2018,
	title = {Joint {3D} {Proposal} {Generation} and {Object} {Detection} from {View} {Aggregation}},
	url = {http://arxiv.org/abs/1712.02294},
	doi = {10.48550/arXiv.1712.02294},
	abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	month = jul,
	year = {2018},
	note = {GSCC: 0001834 
arXiv:1712.02294 [cs]
Issue: arXiv:1712.02294
remark: AVOD融合激光点云与图像用于3D目标检测。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{laydevant_hardware_2023,
	title = {The hardware is the software},
	url = {http://arxiv.org/abs/2310.18335},
	doi = {10.48550/arXiv.2310.18335},
	abstract = {Human brains and bodies are not hardware running software: the hardware is the software. We reason that because the microscopic physics of artificial-intelligence hardware and of human biological "hardware" is distinct, neuromorphic engineers need to be cautious (and yet also creative) in how we take inspiration from biological intelligence. We should focus primarily on principles and design ideas that respect -- and embrace -- the underlying hardware physics of non-biological intelligent systems, rather than abstracting it away. We see a major role for neuroscience in neuromorphic computing as identifying the physics-agnostic principles of biological intelligence -- that is the principles of biological intelligence that can be gainfully adapted and applied to any physical hardware.},
	language = {en-US},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Laydevant, Jeremie and Wright, Logan G. and Wang, Tianyu and McMahon, Peter L.},
	month = oct,
	year = {2023},
	note = {GSCC: 0000007 
arXiv:2310.18335
TLDR: This paper argues that because the physics of artificial intelligence hardware and of human biological "hardware" is distinct, neuromorphic engineers need to be selective in the inspiration they take from neuroscience.
remark: 硬件即软件，重视生物智能原则。},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@misc{li_multimodal_2023,
	title = {Multimodal {Foundation} {Models}: {From} {Specialists} to {General}-{Purpose} {Assistants}},
	shorttitle = {Multimodal {Foundation} {Models}},
	url = {http://arxiv.org/abs/2309.10020},
	doi = {10.48550/arXiv.2309.10020},
	abstract = {This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.},
	language = {en},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
	month = sep,
	year = {2023},
	note = {GSCC: 0000195 
arXiv:2309.10020 [cs]
remark: 视觉大语言模型的演进与分类综述},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_logonet_2023,
	title = {{LoGoNet}: {Towards} {Accurate} {3D} {Object} {Detection} with {Local}-to-{Global} {Cross}-{Modal} {Fusion}},
	shorttitle = {{LoGoNet}},
	url = {http://arxiv.org/abs/2303.03595},
	doi = {10.48550/arXiv.2303.03595},
	abstract = {LiDAR-camera fusion methods have shown impressive performance in 3D object detection. Recent advanced multi-modal methods mainly perform global fusion, where image features and point cloud features are fused across the whole scene. Such practice lacks fine-grained region-level information, yielding suboptimal fusion performance. In this paper, we present the novel Local-to-Global fusion network (LoGoNet), which performs LiDAR-camera fusion at both local and global levels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous literature, while we exclusively use point centroids to more precisely represent the position of voxel features, thus achieving better cross-modal alignment. As to the Local Fusion (LoF), we first divide each proposal into uniform grids and then project these grid centers to the images. The image features around the projected grid points are sampled to be fused with position-decorated point cloud features, maximally utilizing the rich contextual information around the proposals. The Feature Dynamic Aggregation (FDA) module is further proposed to achieve information interaction between these locally and globally fused features, thus producing more informative multi-modal features. Extensive experiments on both Waymo Open Dataset (WOD) and KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D detection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection leaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy that, for the first time, the detection performance on three classes surpasses 80 APH (L2) simultaneously. Code will be available at {\textbackslash}url\{https://github.com/sankin97/LoGoNet\}.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Li, Xin and Ma, Tao and Hou, Yuenan and Shi, Botian and Yang, Yuchen and Liu, Youquan and Wu, Xingjiao and Chen, Qin and Li, Yikang and Qiao, Yu and He, Liang},
	month = mar,
	year = {2023},
	note = {GSCC: 0000108 
arXiv:2303.03595 [cs]
Issue: arXiv:2303.03595
remark: LoGoNet实现局部到全局融合的3D目标检测。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_learning_2023,
	title = {Learning {Normality} is {Enough}: {A} {Software}-based {Mitigation} against {Inaudible} {Voice} {Attacks}},
	isbn = {978-1-939133-37-3},
	shorttitle = {Learning {Normality} is {Enough}},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/li-xinfeng},
	language = {en},
	urldate = {2023-11-08},
	author = {Li, Xinfeng and Ji, Xiaoyu and Yan, Chen and Li, Chaohao and Li, Yichen and Zhang, Zhenning and Xu, Wenyuan},
	year = {2023},
	note = {GSCC: 0000014 
remark: NormDetect基于软件的超声波攻击的缓解措施，可以立即应用于各种设备，无需任何硬件修改。},
	pages = {2455--2472},
}

@misc{li_safegen_2024,
	title = {{SafeGen}: {Mitigating} {Unsafe} {Content} {Generation} in {Text}-to-{Image} {Models}},
	shorttitle = {{SafeGen}},
	url = {http://arxiv.org/abs/2404.06666},
	doi = {10.48550/arXiv.2404.06666},
	abstract = {Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial prompts inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate unsafe content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate unsafe visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets demonstrate SafeGen's effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1\% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.},
	language = {en-US},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Li, Xinfeng and Yang, Yuchen and Deng, Jiangyi and Yan, Chen and Chen, Yanjiao and Ji, Xiaoyu and Xu, Wenyuan},
	month = apr,
	year = {2024},
	note = {GSCC: 0000015 
arXiv:2404.06666 [cs]
remark: 提出SafeGen框架抑制文本生成模型的危险内容。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{li_sensorllm_2024,
	title = {{SensorLLM}: {Aligning} {Large} {Language} {Models} with {Motion} {Sensors} for {Human} {Activity} {Recognition}},
	shorttitle = {{SensorLLM}},
	url = {https://arxiv.org/abs/2410.10624v1},
	abstract = {In this work, we bridge the gap between wearable sensor technology and personalized AI assistants by enabling Large Language Models (LLMs) to understand time-series tasks like human activity recognition (HAR). Despite the strong reasoning and generalization capabilities of LLMs, leveraging them for sensor data tasks remains largely unexplored. This gap stems from challenges like the lack of semantic context in time-series data, computational limitations, and LLMs' difficulty processing numerical inputs. To address these issues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential for sensor data tasks. In the Sensor-Language Alignment Stage, we introduce special tokens for each sensor channel and automatically generate trend-descriptive text to align sensor data with textual inputs, enabling SensorLLM to capture numerical changes, channel-specific information, and sensor data of varying lengths-capabilities that existing LLMs typically struggle with, all without the need for human annotations. Next, in Task-Aware Tuning Stage, we refine the model for HAR classification using the frozen LLM and alignment module, achieving performance on par with or surpassing state-of-the-art models. We further demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, enabling it to generalize across diverse datasets for HAR tasks. We strongly believe our work lays the stepstone for future time-series and text alignment research, offering a path toward foundation models for sensor data.},
	language = {en},
	urldate = {2024-10-19},
	journal = {arXiv.org},
	author = {Li, Zechen and Deldari, Shohreh and Chen, Linyao and Xue, Hao and Salim, Flora D.},
	month = oct,
	year = {2024},
	note = {GSCC: 0000001 
remark: 大模型与传感器结合识别人体活动。
TLDR: 请填写DOI地址TLDR。},
}

@misc{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	doi = {10.48550/arXiv.2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
	language = {en-US},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	note = {GSCC: 0004629 
arXiv:2304.08485
TLDR: This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.
remark: LLAVA},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_mtd-gpt_2023,
	title = {{MTD}-{GPT}: {A} {Multi}-{Task} {Decision}-{Making} {GPT} {Model} for {Autonomous} {Driving} at {Unsignalized} {Intersections}},
	shorttitle = {{MTD}-{GPT}},
	url = {http://arxiv.org/abs/2307.16118},
	doi = {10.48550/arXiv.2307.16118},
	abstract = {Autonomous driving technology is poised to transform transportation systems. However, achieving safe and accurate multi-task decision-making in complex scenarios, such as unsignalized intersections, remains a challenge for autonomous vehicles. This paper presents a novel approach to this issue with the development of a Multi-Task Decision-Making Generative Pre-trained Transformer (MTD-GPT) model. Leveraging the inherent strengths of reinforcement learning (RL) and the sophisticated sequence modeling capabilities of the Generative Pre-trained Transformer (GPT), the MTD-GPT model is designed to simultaneously manage multiple driving tasks, such as left turns, straight-ahead driving, and right turns at unsignalized intersections. We initially train a single-task RL expert model, sample expert data in the environment, and subsequently utilize a mixed multi-task dataset for offline GPT training. This approach abstracts the multi-task decision-making problem in autonomous driving as a sequence modeling task. The MTD-GPT model is trained and evaluated across several decision-making tasks, demonstrating performance that is either superior or comparable to that of state-of-the-art single-task decision-making models.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Liu, Jiaqi and Hang, Peng and qi, Xiao and Wang, Jianqiang and Sun, Jian},
	month = jul,
	year = {2023},
	note = {GSCC: 0000029 
arXiv:2307.16118 [cs]
remark: MTD-GPT模型实现无信号交叉口轨迹预测},
	keywords = {Computer Science - Robotics},
}

@misc{liu_visualagentbench_2024,
	title = {{VisualAgentBench}: {Towards} {Large} {Multimodal} {Models} as {Visual} {Foundation} {Agents}},
	shorttitle = {{VisualAgentBench}},
	url = {http://arxiv.org/abs/2408.06327},
	doi = {10.48550/arXiv.2408.06327},
	abstract = {Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train {\textbackslash}\& test data, and part of fine-tuned open LMMs are available at {\textbackslash}url\{https://github.com/THUDM/VisualAgentBench\}.},
	language = {en-US},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Liu, Xiao and Zhang, Tianjie and Gu, Yu and Iong, Iat Long and Xu, Yifan and Song, Xixuan and Zhang, Shudan and Lai, Hanyu and Liu, Xinyi and Zhao, Hanlin and Sun, Jiadai and Yang, Xinyue and Yang, Yu and Qi, Zehan and Yao, Shuntian and Sun, Xueqiao and Cheng, Siyi and Zheng, Qinkai and Yu, Hao and Zhang, Hanchen and Hong, Wenyi and Ding, Ming and Pan, Lihang and Gu, Xiaotao and Zeng, Aohan and Du, Zhengxiao and Song, Chan Hee and Su, Yu and Dong, Yuxiao and Tang, Jie},
	month = aug,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2408.06327 [cs]
TLDR: This work introduces VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities.
remark: 引入视觉代理基准测试LMMs在多场景评估。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_prompting_2023,
	title = {Prompting {Frameworks} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Prompting {Frameworks} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.12785},
	doi = {10.48550/arXiv.2311.12785},
	abstract = {Since the launch of ChatGPT, a powerful AI Chatbot developed by OpenAI, large language models (LLMs) have made significant advancements in both academia and industry, bringing about a fundamental engineering paradigm shift in many areas. While LLMs are powerful, it is also crucial to best use their power where "prompt'' plays a core role. However, the booming LLMs themselves, including excellent APIs like ChatGPT, have several inherent limitations: 1) temporal lag of training data, and 2) the lack of physical capabilities to perform external actions. Recently, we have observed the trend of utilizing prompt-based tools to better utilize the power of LLMs for downstream tasks, but a lack of systematic literature and standardized terminology, partly due to the rapid evolution of this field. Therefore, in this work, we survey related prompting tools and promote the concept of the "Prompting Framework" (PF), i.e. the framework for managing, simplifying, and facilitating interaction with large language models. We define the lifecycle of the PF as a hierarchical structure, from bottom to top, namely: Data Level, Base Level, Execute Level, and Service Level. We also systematically depict the overall landscape of the emerging PF field and discuss potential future research and challenges. To continuously track the developments in this area, we maintain a repository at https://github.com/lxx0628/Prompting-Framework-Survey, which can be a useful resource sharing platform for both academic and industry in this field.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Liu, Xiaoxia and Wang, Jingyi and Sun, Jun and Yuan, Xiaohan and Dong, Guoliang and Di, Peng and Wang, Wenhai and Wang, Dongxia},
	month = nov,
	year = {2023},
	note = {GSCC: 0000027 
arXiv:2311.12785 [cs]
remark: 大语言模型提示框架综述
TLDR: This work surveys related prompting tools and promotes the concept of the "Prompting Framework", the framework for managing, simplifying, and facilitating interaction with large language models, and defines the lifecycle of the PF as a hierarchical structure.},
	keywords = {Computer Science - Software Engineering},
}

@misc{liu_mm-safetybench_2024,
	title = {{MM}-{SafetyBench}: {A} {Benchmark} for {Safety} {Evaluation} of {Multimodal} {Large} {Language} {Models}},
	shorttitle = {{MM}-{SafetyBench}},
	url = {http://arxiv.org/abs/2311.17600},
	doi = {10.48550/arXiv.2311.17600},
	abstract = {The security concerns surrounding Large Language Models (LLMs) have been extensively explored, yet the safety of Multimodal Large Language Models (MLLMs) remains understudied. In this paper, we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images, as if the text query itself were malicious. To address this, we introduce MM-SafetyBench, a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. We have compiled a dataset comprising 13 scenarios, resulting in a total of 5,040 text-image pairs. Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned. In response, we propose a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks. Our work underscores the need for a concerted effort to strengthen and enhance the safety measures of open-source MLLMs against potential malicious exploits. The resource is available at https://github.com/isXinLiu/MM-SafetyBench},
	language = {en-US},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu},
	month = jun,
	year = {2024},
	note = {GSCC: 0000028 
arXiv:2311.17600
TLDR: A novel visual prompt attack that exploits query-relevant images to jailbreak the open-source LMMs and shows LLMs can be easily at-tacked by this approach, even if the employed Large Language Models are safely aligned.
remark: 引入MM-SafetyBench评估多模态模型安全性。},
	keywords = {\# ECCV 2024, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_safety_2024,
	title = {Safety of {Multimodal} {Large} {Language} {Models} on {Images} and {Texts}},
	url = {http://arxiv.org/abs/2402.00357},
	doi = {10.48550/arXiv.2402.00357},
	abstract = {Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text. We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey. Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs. Next, we comprehensively present attack and defense techniques related to MLLMs' safety. Finally, we analyze several unsolved issues and discuss promising research directions. The latest papers are continually collected at https://github.com/isXinLiu/MLLM-Safety-Collection.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
	month = jun,
	year = {2024},
	note = {GSCC: 0000007 
arXiv:2402.00357
TLDR: This paper systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text, and comprehensively present attack and defense techniques related to MLLMs' safety.
remark: 多模态大模型安全评估与防御综述。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⏳},
}

@inproceedings{long_private_2023,
	title = {Private {Eye}: {On} the {Limits} of {Textual} {Screen} {Peeking} via {Eyeglass} {Reflections} in {Video} {Conferencing}},
	shorttitle = {Private {Eye}},
	url = {https://ieeexplore.ieee.org/abstract/document/10179423},
	doi = {10.1109/SP46215.2023.10179423},
	abstract = {Personal video conferencing has become a new norm after COVID-19 caused a seismic shift from in-person meetings and phone calls to video conferencing for daily communications and sensitive business. Video leaks participants’ on-screen information because eyeglasses and other reflective objects unwittingly expose partial screen contents. Using mathematical modeling and human subjects experiments, this research explores the extent to which emerging webcams might leak recognizable textual and graphical information gleaming from eyeglass reflections captured by webcams. The primary goal of our work is to measure, compute, and predict the factors, limits, and thresholds of recognizability as webcam technology evolves in the future. Our work explores and characterizes the viable threat models based on optical attacks using multi-frame super resolution techniques on sequences of video frames. Our models and experimental results in a controlled lab setting show it is possible to reconstruct and recognize with over 75\% accuracy on-screen texts that have heights as small as 10 mm with a 720p webcam. We further apply this threat model to web textual contents with varying attacker capabilities to find thresholds at which text becomes recognizable. Our user study with 20 participants suggests present-day 720p webcams are sufficient for adversaries to reconstruct textual content on big-font websites. Our models further show that the evolution towards 4K cameras will tip the threshold of text leakage to reconstruction of most header texts on popular websites. Besides textual targets, a case study on recognizing a closed-world dataset of Alexa top 100 websites with 720p webcams shows a maximum recognition accuracy of 94\% with 10 participants even without using machine-learning models. Our research proposes near-term mitigations including a software prototype that users can use to blur the eyeglass areas of their video streams. For possible long-term defenses, we advocate an individual reflection testing procedure to assess threats under various settings, and justify the importance of following the principle of least privilege for privacy-sensitive scenarios.},
	language = {en-US},
	urldate = {2024-06-04},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Long, Yan and Yan, Chen and Xiao, Shilin and Prasad, Shivan and Xu, Wenyuan and Fu, Kevin},
	month = may,
	year = {2023},
	note = {GSCC: 0000006 
ISSN: 2375-1207
remark: 通过眼镜反光重建屏幕文本并提出缓解措施。},
	keywords = {Computational modeling, Reflection, Target recognition, Text recognition, Threat modeling, Virtual assistants, Webcams},
	pages = {3432--3449},
}

@misc{ma_sqa3d_2023,
	title = {{SQA3D}: {Situated} {Question} {Answering} in {3D} {Scenes}},
	shorttitle = {{SQA3D}},
	url = {http://arxiv.org/abs/2210.07474},
	doi = {10.48550/arXiv.2210.07474},
	abstract = {We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20\%, while amateur human participants can reach 90.06\%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan},
	month = apr,
	year = {2023},
	note = {GSCC: 0000103 
arXiv:2210.07474 [cs]
TLDR: A new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D), which requires the tested agent to first understand its situation in the 3D scene, then reason about its surrounding environment and answer a question under that situation.
remark: SQA3D挑战体现在3D场景中智能问答。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{praveen_kumar_machine_2019,
	title = {Machine learning algorithms for wireless sensor networks: {A} survey},
	volume = {49},
	issn = {1566-2535},
	shorttitle = {Machine learning algorithms for wireless sensor networks},
	url = {https://www.sciencedirect.com/science/article/pii/S156625351830277X},
	doi = {10.1016/j.inffus.2018.09.013},
	abstract = {Wireless sensor network (WSN) is one of the most promising technologies for some real-time applications because of its size, cost-effective and easily deployable nature. Due to some external or internal factors, WSN may change dynamically and therefore it requires depreciating dispensable redesign of the network. The traditional WSN approaches have been explicitly programmed which make the networks hard to respond dynamically. To overcome such scenarios, machine learning (ML) techniques can be applied to react accordingly. ML is the process of self-learning from the experiences and acts without human intervention or re-program. The survey of the ML techniques for WSNs is presented in [1], covering period of 2002–2013. In this survey, we present various ML-based algorithms for WSNs with their advantages, drawbacks, and parameters effecting the network lifetime, covering the period from 2014–March 2018. In addition, we also discuss ML algorithms for synchronization, congestion control, mobile sink scheduling and energy harvesting. Finally, we present a statistical analysis of the survey, the reasons for selection of a particular ML techniques to address an issue in WSNs followed by some discussion on the open issues.},
	language = {en-US},
	urldate = {2024-03-21},
	journal = {Information Fusion},
	author = {Praveen Kumar, D. and Amgoth, Tarachand and Annavarapu, Chandra Sekhara Rao},
	month = sep,
	year = {2019},
	note = {GSCC: 0000734 
remark: 无线传感器网络中的机器学习算法综述。},
	keywords = {Data aggregation, Energy efficiency, Machine learning, Network lifetime, Wireless sensor networks},
	pages = {1--25},
}

@article{qian_orchestrating_2020,
	title = {Orchestrating the {Development} {Lifecycle} of {Machine} {Learning}-based {IoT} {Applications}: {A} {Taxonomy} and {Survey}},
	volume = {53},
	issn = {0360-0300},
	shorttitle = {Orchestrating the {Development} {Lifecycle} of {Machine} {Learning}-based {IoT} {Applications}},
	url = {https://dl.acm.org/doi/10.1145/3398020},
	doi = {10.1145/3398020},
	abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
	number = {4},
	urldate = {2023-12-21},
	journal = {ACM Computing Surveys},
	author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
	year = {2020},
	note = {GSCC: 0000103 
remark: ML结合IoT应用开发流程综述。},
	keywords = {IoT, deep learning, machine learning, orchestration},
	pages = {82:1--82:47},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://arxiv.org/abs/2103.00020v1},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {GSCC: 0026975 
remark: CLIP},
}

@misc{rivkin_sage_2023,
	title = {{SAGE}: {Smart} home {Agent} with {Grounded} {Execution}},
	shorttitle = {{SAGE}},
	url = {https://arxiv.org/abs/2311.00772v1},
	abstract = {This article introduces SAGE (Smart home Agent with Grounded Execution), a framework designed to maximize the flexibility of smart home assistants by replacing manually-defined inference logic with an LLM-powered autonomous agent system. SAGE integrates information about user preferences, device states, and external factors (such as weather and TV schedules) through the orchestration of a collection of tools. SAGE's capabilities include learning user preferences from natural-language utterances, interacting with devices by reading their API documentation, writing code to continuously monitor devices, and understanding natural device references. To evaluate SAGE, we develop a benchmark of 43 highly challenging smart home tasks, where SAGE successfully achieves 23 tasks, significantly outperforming existing LLM-enabled baselines (5/43).},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Rivkin, Dmitriy and Hogan, Francois and Feriani, Amal and Konar, Abhisek and Sigal, Adam and Liu, Steve and Dudek, Greg},
	month = nov,
	year = {2023},
	note = {GSCC: 0000004 
remark: SAGE提高智能家居助手灵活性。},
}

@inproceedings{rohmer_v-rep_2013,
	title = {V-{REP}: {A} versatile and scalable robot simulation framework},
	shorttitle = {V-{REP}},
	url = {https://ieeexplore.ieee.org/document/6696520},
	doi = {10.1109/IROS.2013.6696520},
	abstract = {From exploring planets to cleaning homes, the reach and versatility of robotics is vast. The integration of actuation, sensing and control makes robotics systems powerful, but complicates their simulation. This paper introduces a versatile, scalable, yet powerful general-purpose robot simulation framework called V-REP. The paper discusses the utility of a portable and flexible simulation framework that allows for direct incorporation of various control techniques. This renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity. It also increases productivity by offering built-in and ready-to-use functionalities, as well as a multitude of programming approaches. This allows for a multitude of applications including rapid algorithm development, system verification, rapid prototyping, and deployment for cases such as safety/remote monitoring, training and education, hardware control, and factory automation simulation.},
	language = {en-US},
	urldate = {2024-07-18},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
	month = nov,
	year = {2013},
	note = {GSCC: 0002051 
ISSN: 2153-0866
TLDR: A versatile, scalable, yet powerful general-purpose robot simulation framework called V-REP, which allows for direct incorporation of various control techniques and renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity.
remark: 介绍V-REP机器人仿真框架的多功能性与可扩展性。},
	keywords = {Computational modeling, Hardware, Joints, Load modeling, Robots, Sensors, Shape},
	pages = {1321--1326},
}

@misc{schlarmann_adversarial_2023,
	title = {On the {Adversarial} {Robustness} of {Multi}-{Modal} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2308.10741},
	doi = {10.48550/arXiv.2308.10741},
	abstract = {Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.},
	language = {en-US},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Schlarmann, Christian and Hein, Matthias},
	month = aug,
	year = {2023},
	note = {GSCC: 0000077 
arXiv:2308.10741 [cs]
remark: 多模态模型易受对抗性攻击。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{sha_languagempc_2023,
	title = {{LanguageMPC}: {Large} {Language} {Models} as {Decision} {Makers} for {Autonomous} {Driving}},
	shorttitle = {{LanguageMPC}},
	url = {http://arxiv.org/abs/2310.03026},
	doi = {10.48550/arXiv.2310.03026},
	abstract = {Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Sha, Hao and Mu, Yao and Jiang, Yuxuan and Chen, Li and Xu, Chenfeng and Luo, Ping and Li, Shengbo Eben and Tomizuka, Masayoshi and Zhan, Wei and Ding, Mingyu},
	month = oct,
	year = {2023},
	note = {GSCC: 0000131 
arXiv:2310.03026 [cs]
remark: LLMs+MPC提升自动驾驶决策能力
TLDR: This paper devise cognitive pathways to enable comprehensive reasoning with Large Language Models, and develop algorithms for translating LLM decisions into actionable driving commands, an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shan_prompt-specific_2023,
	title = {Prompt-{Specific} {Poisoning} {Attacks} on {Text}-to-{Image} {Generative} {Models}},
	url = {http://arxiv.org/abs/2310.13828},
	doi = {10.48550/arXiv.2310.13828},
	abstract = {Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts. We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in {\textless}100 poison samples. Nightshade poison effects "bleed through" to related concepts, and multiple attacks can composed together in a single prompt. Surprisingly, we show that a moderate number of Nightshade attacks can destabilize general features in a text-to-image generative model, effectively disabling its ability to generate meaningful images. Finally, we propose the use of Nightshade` and similar tools as a last defense for content creators against web scrapers that ignore opt-out/do-not-crawl directives, and discuss possible implications for model trainers and content creators.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Zheng, Haitao and Zhao, Ben Y.},
	month = oct,
	year = {2023},
	note = {GSCC: 0000040 
arXiv:2310.13828 [cs]
Issue: arXiv:2310.13828
remark: 生成模型面临精准数据投毒攻击风险。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{shao_lmdrive_2023,
	title = {{LMDrive}: {Closed}-{Loop} {End}-to-{End} {Driving} with {Large} {Language} {Models}},
	shorttitle = {{LMDrive}},
	url = {http://arxiv.org/abs/2312.07488},
	doi = {10.48550/arXiv.2312.07488},
	abstract = {Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach "Artificial General Intelligence". On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans. To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes, models, and datasets can be found at https://github.com/opendilab/LMDrive},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Shao, Hao and Hu, Yuxuan and Wang, Letian and Waslander, Steven L. and Liu, Yu and Li, Hongsheng},
	month = dec,
	year = {2023},
	note = {GSCC: 0000099 
arXiv:2312.07488 [cs]
remark: LLM指导的闭环端到端自动驾驶框架。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{shen_sok_2022,
	title = {{SoK}: {On} the {Semantic} {AI} {Security} in {Autonomous} {Driving}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2203.05314},
	doi = {10.48550/arXiv.2203.05314},
	abstract = {Autonomous Driving (AD) systems rely on AI components to make safety and correct driving decisions. Unfortunately, today's AI algorithms are known to be generally vulnerable to adversarial attacks. However, for such AI component-level vulnerabilities to be semantically impactful at the system level, it needs to address non-trivial semantic gaps both (1) from the system-level attack input spaces to those at AI component level, and (2) from AI component-level attack impacts to those at the system level. In this paper, we define such research space as semantic AI security as opposed to generic AI security. Over the past 5 years, increasingly more research works are performed to tackle such semantic AI security challenges in AD context, which has started to show an exponential growth trend. In this paper, we perform the first systematization of knowledge of such growing semantic AD AI security research space. In total, we collect and analyze 53 such papers, and systematically taxonomize them based on research aspects critical for the security field. We summarize 6 most substantial scientific gaps observed based on quantitative comparisons both vertically among existing AD AI security works and horizontally with security works from closely-related domains. With these, we are able to provide insights and potential future directions not only at the design level, but also at the research goal, methodology, and community levels. To address the most critical scientific methodology-level gap, we take the initiative to develop an open-source, uniform, and extensible system-driven evaluation platform, named PASS, for the semantic AD AI security research community. We also use our implemented platform prototype to showcase the capabilities and benefits of such a platform using representative semantic AD AI attacks.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Shen, Junjie and Wang, Ningfei and Wan, Ziwen and Luo, Yunpeng and Sato, Takami and Hu, Zhisheng and Zhang, Xinyang and Guo, Shengjian and Zhong, Zhenyu and Li, Kang and Zhao, Ziming and Qiao, Chunming and Chen, Qi Alfred},
	month = mar,
	year = {2022},
	note = {GSCC: 0000043 
arXiv:2203.05314 [cs]
Issue: arXiv:2203.05314
TLDR: To address the most critical scientific methodology-level gap, the first systematization of knowledge of such growing semantic AD AI security research space is performed, and an open-source, uniform, and extensible system-driven evaluation platform is developed, named PASS, for the semantic ADAI security research community.
remark: 自动驾驶中语义AI安全现状及挑战。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Robotics},
}

@misc{shen_large_2023,
	title = {Large {Language} {Models} {Empowered} {Autonomous} {Edge} {AI} for {Connected} {Intelligence}},
	url = {http://arxiv.org/abs/2307.02779},
	doi = {10.48550/arXiv.2307.02779},
	abstract = {The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automatically generating code to train new models via edge federated learning. Experimental results demonstrate the system's remarkable ability to accurately comprehend user demands, efficiently execute AI models with minimal cost, and effectively create high-performance AI models through federated learning.},
	urldate = {2023-11-21},
	publisher = {arXiv},
	author = {Shen, Yifei and Shao, Jiawei and Zhang, Xinjie and Lin, Zehong and Pan, Hao and Li, Dongsheng and Zhang, Jun and Letaief, Khaled B.},
	month = jul,
	year = {2023},
	note = {GSCC: 0000070 
arXiv:2307.02779 [cs, eess, math]
remark: 边缘AI通过GPT实现智能连接。
TLDR: This article presents a vision of autonomous edge AI systems that automatically organize, adapt, and optimize themselves to meet users' diverse requirements, leveraging the power of large language models (LLMs), i.e., Generative Pretrained Transformer (GPT).},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Electrical Engineering and Systems Science - Signal Processing},
}

@article{spreitzer_systematic_2018,
	title = {Systematic {Classification} of {Side}-{Channel} {Attacks}: {A} {Case} {Study} for {Mobile} {Devices}},
	volume = {20},
	issn = {1553-877X},
	shorttitle = {Systematic {Classification} of {Side}-{Channel} {Attacks}},
	url = {https://ieeexplore.ieee.org/abstract/document/8141882},
	doi = {10.1109/COMST.2017.2779824},
	abstract = {Side-channel attacks on mobile devices have gained increasing attention since their introduction in 2007. While traditional side-channel attacks, such as power analysis attacks and electromagnetic analysis attacks, required physical presence of the attacker as well as expensive equipment, an (unprivileged) application is all it takes to exploit the leaking information on modern mobile devices. Given the vast amount of sensitive information that are stored on smartphones, the ramifications of side-channel attacks affect both the security and privacy of users and their devices. In this paper, we propose a new categorization system for side-channel attacks, which is necessary as side-channel attacks have evolved significantly since their scientific investigations during the smart card era in the 1990s. Our proposed classification system allows to analyze side-channel attacks systematically, and facilitates the development of novel countermeasures. Besides this new categorization system, the extensive survey of existing attacks and attack strategies provides valuable insights into the evolving field of side-channel attacks, especially when focusing on mobile devices. We conclude by discussing open issues and challenges in this context and outline possible future research directions.},
	number = {1},
	urldate = {2024-12-19},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Spreitzer, Raphael and Moonsamy, Veelasha and Korak, Thomas and Mangard, Stefan},
	year = {2018},
	note = {GSCC: 0000310 
Conference Name: IEEE Communications Surveys \& Tutorials
TLDR: This paper proposes a new categorization system for side-channel attacks, necessary as side- channel attacks have evolved significantly since their scientific investigations during the smart card era in the 1990s, and facilitates the development of novel countermeasures.},
	keywords = {Android, Cloud computing, Side-channel attacks, Smart cards, Smart phones, classification, information leakage, mobile devices, smartphones, survey},
	pages = {465--488},
}

@misc{sun_revisiting_2017,
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	url = {http://arxiv.org/abs/1707.02968},
	doi = {10.48550/arXiv.1707.02968},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	language = {en},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = aug,
	year = {2017},
	note = {GSCC: 0003173 
arXiv:1707.02968 [cs]
version: 2
remark: 数据数量提升深度视觉学习性能。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	language = {en-US},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {GSCC: 0011642 
arXiv:2302.13971 [cs]
remark: llama},
	keywords = {Computer Science - Computation and Language},
}

@misc{touvron_llama_2023-1,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	language = {en-US},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {GSCC: 0011106 
arXiv:2307.09288 [cs]
remark: llama2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{vora_pointpainting_2019,
	title = {{PointPainting}: {Sequential} {Fusion} for {3D} {Object} {Detection}},
	shorttitle = {{PointPainting}},
	url = {https://arxiv.org/abs/1911.10150v2},
	abstract = {Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to fill this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidar-only method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird's-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.},
	language = {en},
	urldate = {2024-09-23},
	journal = {arXiv.org},
	author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
	month = nov,
	year = {2019},
	note = {GSCC: 0001045 
remark: 提出了PointPainting基于图像语义分割增强LiDAR检测。},
}

@misc{wang_large_2024,
	title = {Large {Language} {Models} for {Robotics}: {Opportunities}, {Challenges}, and {Perspectives}},
	shorttitle = {Large {Language} {Models} for {Robotics}},
	url = {https://arxiv.org/abs/2401.04334v1},
	abstract = {Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.},
	language = {en},
	urldate = {2024-01-11},
	journal = {arXiv.org},
	author = {Wang, Jiaqi and Wu, Zihao and Li, Yiwei and Jiang, Hanqi and Shu, Peng and Shi, Enze and Hu, Huawen and Ma, Chong and Liu, Yiheng and Wang, Xuhui and Yao, Yincheng and Liu, Xuan and Zhao, Huaqin and Liu, Zhengliang and Dai, Haixing and Zhao, Lin and Ge, Bao and Li, Xiang and Liu, Tianming and Zhang, Shu},
	month = jan,
	year = {2024},
	note = {GSCC: 0000072 
remark: 探讨大型语言模型在机器人学中的应用。},
}

@inproceedings{wang_volttack_2023,
	title = {Volttack: {Control} {IoT} {Devices} by {Manipulating} {Power} {Supply} {Voltage}},
	shorttitle = {Volttack},
	url = {https://ieeexplore.ieee.org/abstract/document/10179340},
	doi = {10.1109/SP46215.2023.10179340},
	abstract = {This paper analyzes the security of Internet of Things (IoT) devices from the perspective of sensing and actuating. Particularly, we discover a vulnerability in power supply modules and propose Volttack attacks. To launch a Volttack attack, attackers may compromise the power source and inject malicious signals through the power supply module, which is indispensable in most devices. Eventually, Volttack attacks may cause the sensor measurement irrelevant to reality or maneuver the actuator in a way disregarding the desired command. To understand Volttack, we systematically analyze the underlying principle of power supply signals affecting the electronic components, which are building blocks to constitute the sensor or actuator modules. Derived from these findings, we implement and validate Volttack on off-the-shelf products: 6 sensors and 3 actuators, which are used in applications ranging from automobile braking systems, industrial process control to robotic arms. The consequences of manipulating the sensor measurement or actuation include doubled car braking distance and a natural gas leak. The root cause of such a vulnerability stems from the common belief that noises from the power line are unintentional, and our work aims to call for attention to enhancing the security of power supply modules and adding countermeasures to mitigate the attacks.},
	language = {en-US},
	urldate = {2024-06-04},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Wang, Kai and Xiao, Shilin and Ji, Xiaoyu and Yan, Chen and Li, Chaohao and Xu, Wenyuan},
	month = may,
	year = {2023},
	note = {GSCC: 0000004 
ISSN: 2375-1207
remark: 通过操控电压攻击物联网设备。
TLDR: This paper analyzes the security of Internet of Things devices from the perspective of sensing and actuating and discovers a vulnerability in power supply modules and proposes Volttack attacks, which cause the sensor measurement irrelevant to reality or maneuver the actuator in a way disregarding the desired command.},
	keywords = {Actuators, Electronic components, Internet of Things, Power supplies, Process control, Robot sensing systems, Sensors},
	pages = {1771--1788},
}

@misc{warden_machine_2022,
	title = {Machine {Learning} {Sensors}},
	url = {http://arxiv.org/abs/2206.03266},
	doi = {10.48550/arXiv.2206.03266},
	abstract = {Machine learning sensors represent a paradigm shift for the future of embedded machine learning applications. Current instantiations of embedded machine learning (ML) suffer from complex integration, lack of modularity, and privacy and security concerns from data movement. This article proposes a more data-centric paradigm for embedding sensor intelligence on edge devices to combat these challenges. Our vision for "sensor 2.0" entails segregating sensor input data and ML processing from the wider system at the hardware level and providing a thin interface that mimics traditional sensors in functionality. This separation leads to a modular and easy-to-use ML sensor device. We discuss challenges presented by the standard approach of building ML processing into the software stack of the controlling microprocessor on an embedded system and how the modularity of ML sensors alleviates these problems. ML sensors increase privacy and accuracy while making it easier for system builders to integrate ML into their products as a simple component. We provide examples of prospective ML sensors and an illustrative datasheet as a demonstration and hope that this will build a dialogue to progress us towards sensor 2.0.},
	language = {en-US},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Warden, Pete and Stewart, Matthew and Plancher, Brian and Banbury, Colby and Prakash, Shvetank and Chen, Emma and Asgar, Zain and Katti, Sachin and Reddi, Vijay Janapa},
	month = jun,
	year = {2022},
	note = {GSCC: 0000023 
arXiv:2206.03266 [cs, eess]
remark: ML传感器2.0范式},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{woo_unified_2024,
	title = {Unified {Training} of {Universal} {Time} {Series} {Forecasting} {Transformers}},
	url = {http://arxiv.org/abs/2402.02592},
	doi = {10.48550/arXiv.2402.02592},
	abstract = {Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.},
	language = {en-US},
	urldate = {2024-11-13},
	publisher = {arXiv},
	author = {Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
	month = may,
	year = {2024},
	note = {GSCC: 0000095 
arXiv:2402.02592
TLDR: This work presents novel enhancements to the conventional time series Transformer architecture, resulting in the proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai), which achieves competitive or superior performance when compared to full-shot models.
remark: 统一模型提升多领域时间序列预测效果。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wu_language_2023,
	title = {Language {Prompt} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2309.04379},
	doi = {10.48550/arXiv.2309.04379},
	abstract = {A new trend in the computer vision community is to capture objects of interest following flexible human command represented by a natural language prompt. However, the progress of using language prompts in driving scenarios is stuck in a bottleneck due to the scarcity of paired prompt-instance data. To address this challenge, we propose the first object-centric language prompt set for driving scenes within 3D, multi-view, and multi-frame space, named NuPrompt. It expands Nuscenes dataset by constructing a total of 35,367 language descriptions, each referring to an average of 5.3 object tracks. Based on the object-text pairs from the new benchmark, we formulate a new prompt-based driving task, {\textbackslash}ie, employing a language prompt to predict the described object trajectory across views and frames. Furthermore, we provide a simple end-to-end baseline model based on Transformer, named PromptTrack. Experiments show that our PromptTrack achieves impressive performance on NuPrompt. We hope this work can provide more new insights for the autonomous driving community. Dataset and Code will be made public at {\textbackslash}href\{https://github.com/wudongming97/Prompt4Driving\}\{https://github.com/wudongming97/Prompt4Driving\}.},
	language = {en-US},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Wu, Dongming and Han, Wencheng and Wang, Tiancai and Liu, Yingfei and Zhang, Xiangyu and Shen, Jianbing},
	month = sep,
	year = {2023},
	note = {GSCC: 0000063 
arXiv:2309.04379 [cs]
remark: 基于语言提示的自动驾驶对象跟踪
TLDR: The first object-centric language prompt set for driving scenes within 3D, multi-view, and multi-frame space, named NuPrompt is proposed, and a new prompt-based driving task is formulated, employing a language prompt to predict the described object trajectory across views and frames.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_virtual_2023,
	title = {Virtual {Sparse} {Convolution} for {Multimodal} {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2303.02314},
	doi = {10.48550/arXiv.2303.02314},
	abstract = {Recently, virtual/pseudo-point-based 3D object detection that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, introducing a huge amount of redundant computation during detection. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection precision. This paper proposes a fast yet effective backbone, termed VirConvNet, based on a new operator VirConv (Virtual Sparse Convolution), for virtual-point-based 3D object detection. VirConv consists of two key designs: (1) StVD (Stochastic Voxel Discard) and (2) NRConv (Noise-Resistant Submanifold Convolution). StVD alleviates the computation problem by discarding large amounts of nearby redundant voxels. NRConv tackles the noise problem by encoding voxel features in both 2D image and 3D LiDAR space. By integrating VirConv, we first develop an efficient pipeline VirConv-L based on an early fusion design. Then, we build a high-precision pipeline VirConv-T based on a transformed refinement scheme. Finally, we develop a semi-supervised pipeline VirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test leaderboard, our VirConv-L achieves 85\% AP with a fast running speed of 56ms. Our VirConv-T and VirConv-S attains a high-precision of 86.3\% and 87.2\% AP, and currently rank 2nd and 1st, respectively. The code is available at https://github.com/hailanyi/VirConv.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Wu, Hai and Wen, Chenglu and Shi, Shaoshuai and Li, Xin and Wang, Cheng},
	month = mar,
	year = {2023},
	note = {GSCC: 0000142 
arXiv:2303.02314 [cs]
Issue: arXiv:2303.02314
remark: 提出VirConvNet解决虚拟点3D检测的效率与噪声问题。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_research_2020,
	title = {Research on {Artificial} {Intelligence} {Enhancing} {Internet} of {Things} {Security}: {A} {Survey}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Research on {Artificial} {Intelligence} {Enhancing} {Internet} of {Things} {Security}},
	url = {https://ieeexplore.ieee.org/abstract/document/9172062},
	doi = {10.1109/ACCESS.2020.3018170},
	abstract = {Through three development routes of authentication, communication, and computing, the Internet of Things (IoT) has become a variety of innovative integrated solutions for specific applications. However, due to the openness, extensiveness and resource constraints of IoT, each layer of the three-tier IoT architecture suffers from a variety of security threats. In this work, we systematically review the particularity and complexity of IoT security protection, and then find that Artificial Intelligence (AI) methods such as Machine Learning (ML) and Deep Learning (DL) can provide new powerful capabilities to meet the security requirements of IoT. We analyze the technical feasibility of AI in solving IoT security problems and summarize a general process of AI solutions for IoT security. For four serious IoT security threats: device authentication, Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks defense, intrusion detection and malware detection, we summarize representative AI solutions and compare the different algorithms and technologies used by various solutions. It should be noted that although AI provides many new capabilities for the security protection of IoT, it also brings new potential challenges and possible negative effects to IoT in terms of data, algorithm and architecture. In the future, how to solve these challenges can serve as potential research directions.},
	urldate = {2024-02-28},
	journal = {IEEE Access},
	author = {Wu, Hui and Han, Haiting and Wang, Xiao and Sun, Shengli},
	year = {2020},
	note = {GSCC: 0000200 
Conference Name: IEEE Access
remark: AI增强IoT安全性研究综述。
TLDR: This work systematically review the particularity and complexity of IoT security protection, and finds that Artificial Intelligence methods such as Machine Learning (ML) and Deep Learning (DL) can provide new powerful capabilities to meet the security requirements of IoT.},
	keywords = {Artificial intelligence, Authentication, Computer architecture, Computer crime, Internet of Things, Software, deep learning, machine learning, security},
	pages = {153826--153848},
}

@misc{wu_uniid_2023,
	title = {{UniID}: {Spoofing} {Face} {Authentication} {System} by {Universal} {Identity}},
	shorttitle = {{UniID}},
	url = {https://www.ndss-symposium.org/ndss-paper/uniid-spoofing-face-authentication-system-by-universal-identity/},
	language = {en-US},
	urldate = {2024-06-04},
	journal = {NDSS Symposium},
	author = {Wu, Zhihao},
	year = {2023},
	note = {GSCC: 0000003 
remark: 人脸对抗样本},
}

@misc{xiao_robot_2023,
	title = {Robot {Learning} in the {Era} of {Foundation} {Models}: {A} {Survey}},
	shorttitle = {Robot {Learning} in the {Era} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2311.14379},
	doi = {10.48550/arXiv.2311.14379},
	abstract = {The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.},
	language = {en-US},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Xiao, Xuan and Liu, Jiahang and Wang, Zhipeng and Zhou, Yanmin and Qi, Yong and Cheng, Qian and He, Bin and Jiang, Shuo},
	month = nov,
	year = {2023},
	note = {GSCC: 0000021 
arXiv:2311.14379 [cs]
Issue: arXiv:2311.14379
remark: 综述基础模型在机器人学习中的现状及未来方向。
TLDR: This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.},
	keywords = {Computer Science - Robotics},
}

@misc{xu_penetrative_2023,
	title = {Penetrative {AI}: {Making} {LLMs} {Comprehend} the {Physical} {World}},
	shorttitle = {Penetrative {AI}},
	url = {https://arxiv.org/abs/2310.09605v1},
	abstract = {Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "{\textbackslash}textit\{Penetrative AI\}". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the knowledge they learned during training for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Xu, Huatao and Han, Liying and Li, Mo and Srivastava, Mani},
	month = oct,
	year = {2023},
	note = {GSCC: 0000047 
remark: LLMs通过IoT理解物理世界。},
}

@inproceedings{xu_limu-bert_2021,
	address = {New York, NY, USA},
	series = {{SenSys} '21},
	title = {{LIMU}-{BERT}: {Unleashing} the {Potential} of {Unlabeled} {Data} for {IMU} {Sensing} {Applications}},
	isbn = {978-1-4503-9097-2},
	shorttitle = {{LIMU}-{BERT}},
	url = {https://doi.org/10.1145/3485730.3485937},
	doi = {10.1145/3485730.3485937},
	abstract = {Deep learning greatly empowers Inertial Measurement Unit (IMU) sensors for various mobile sensing applications, including human activity recognition, human-computer interaction, localization and tracking, and many more. Most existing works require substantial amounts of well-curated labeled data to train IMU-based sensing models, which incurs high annotation and training costs. Compared with labeled data, unlabeled IMU data are abundant and easily accessible. In this work, we present LIMU-BERT, a novel representation learning model that can make use of unlabeled IMU data and extract generalized rather than task-specific features. LIMU-BERT adopts the principle of self-supervised training of the natural language model BERT to effectively capture temporal relations and feature distributions in IMU sensor measurements. However, the original BERT is not adaptive to mobile IMU data. By meticulously observing the characteristics of IMU sensors, we propose a series of techniques and accordingly adapt LIMU-BERT to IMU sensing tasks. The designed models are lightweight and easily deployable on mobile devices. With the representations learned via LIMU-BERT, task-specific models trained with limited labeled samples can achieve superior performances. We extensively evaluate LIMU-BERT with four open datasets. The results show that the LIMU-BERT enhanced models significantly outperform existing approaches in two typical IMU sensing applications.},
	urldate = {2023-11-21},
	booktitle = {Proceedings of the 19th {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Huatao and Zhou, Pengfei and Tan, Rui and Li, Mo and Shen, Guobin},
	year = {2021},
	note = {GSCC: 0000110 
remark: LIMU-BERT提升IMU感知应用性能。
TLDR: This work presents LIMU-BERT, a novel representation learning model that can make use of unlabeled IMU data and extract generalized rather than task-specific features and significantly outperform existing approaches in two typical IMU sensing applications.},
	keywords = {BERT, IMU, Mobile Sensing, Representation Learning},
	pages = {220--233},
}

@misc{xu_pointllm_2023,
	title = {{PointLLM}: {Empowering} {Large} {Language} {Models} to {Understand} {Point} {Clouds}},
	shorttitle = {{PointLLM}},
	url = {http://arxiv.org/abs/2308.16911},
	doi = {10.48550/arXiv.2308.16911},
	abstract = {The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment results show that PointLLM demonstrates superior performance over existing 2D baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM outperforms human annotators in over 50\% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM .},
	language = {en},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
	month = aug,
	year = {2023},
	note = {GSCC: 0000110 
arXiv:2308.16911 [cs]
remark: PointLLM让大模型理解点云数据。
TLDR: Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50\% of the samples.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xue_ulip_2022,
	title = {{ULIP}: {Learning} a {Unified} {Representation} of {Language}, {Images}, and {Point} {Clouds} for {3D} {Understanding}},
	shorttitle = {{ULIP}},
	url = {https://arxiv.org/abs/2212.05171v4},
	abstract = {The recognition capabilities of current state-of-the-art 3D models are limited by datasets with a small number of annotated data and a pre-defined set of categories. In its 2D counterpart, recent advances have shown that similar problems can be significantly alleviated by employing knowledge from other modalities, such as language. Inspired by this, leveraging multimodal information for 3D modality could be promising to improve 3D understanding under the restricted data regime, but this line of research is not well studied. Therefore, we introduce ULIP to learn a unified representation of images, texts, and 3D point clouds by pre-training with object triplets from the three modalities. To overcome the shortage of training triplets, ULIP leverages a pre-trained vision-language model that has already learned a common visual and textual space by training with massive image-text pairs. Then, ULIP learns a 3D representation space aligned with the common image-text space, using a small number of automatically synthesized triplets. ULIP is agnostic to 3D backbone networks and can easily be integrated into any 3D architecture. Experiments show that ULIP effectively improves the performance of multiple recent 3D backbones by simply pre-training them on ShapeNet55 using our framework, achieving state-of-the-art performance in both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanObjectNN. ULIP also improves the performance of PointMLP by around 3\% in 3D classification on ScanObjectNN, and outperforms PointCLIP by 28.8\% on top-1 accuracy for zero-shot 3D classification on ModelNet40. Our code and pre-trained models are released at https://github.com/salesforce/ULIP.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Xue, Le and Gao, Mingfei and Xing, Chen and Martín-Martín, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio},
	month = dec,
	year = {2022},
	note = {GSCC: 0000222 
remark: ULIP实现3D多模态统一表示学习。},
}

@misc{xue_ulip-2_2023,
	title = {{ULIP}-2: {Towards} {Scalable} {Multimodal} {Pre}-training for {3D} {Understanding}},
	shorttitle = {{ULIP}-2},
	url = {http://arxiv.org/abs/2305.08275},
	doi = {10.48550/arXiv.2305.08275},
	abstract = {Recent advancements in multimodal pre-training methods have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing multimodal pre-training frameworks to gather multimodal data for 3D applications lack scalability and comprehensiveness, potentially constraining the full potential of multimodal learning. The main bottleneck lies in the language modality's scalability and comprehensiveness. To address this, we introduce ULIP-2, a tri-modal pre-training framework that leverages state-of-the-art large multimodal models to automatically generate holistic language counterparts for 3D objects. It does not require any 3D annotations, and is therefore scalable to large datasets. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. ULIP-2 achieves significant improvements on downstream zero-shot classification on ModelNet40 (74.0\% in top-1 accuracy); on the real-world ScanObjectNN benchmark, it obtains 91.5\% in overall accuracy with only 1.4 million parameters, signifying a breakthrough in scalable multimodal 3D representation learning without human 3D annotations. The code, along with the generated tri-modal datasets, can be found at https://github.com/salesforce/ULIP.},
	language = {en},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Xue, Le and Yu, Ning and Zhang, Shu and Li, Junnan and Martín-Martín, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio},
	month = may,
	year = {2023},
	note = {GSCC: 0000093 
arXiv:2305.08275 [cs]
remark: 提升3D理解的多模态预训练框架。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_qwen2_2024,
	title = {Qwen2 {Technical} {Report}},
	url = {http://arxiv.org/abs/2407.10671},
	doi = {10.48550/arXiv.2407.10671},
	abstract = {This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.},
	language = {en-US},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Dong, Guanting and Wei, Haoran and Lin, Huan and Tang, Jialong and Wang, Jialin and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Ma, Jianxin and Yang, Jianxin and Xu, Jin and Zhou, Jingren and Bai, Jinze and He, Jinzheng and Lin, Junyang and Dang, Kai and Lu, Keming and Chen, Keqin and Yang, Kexin and Li, Mei and Xue, Mingfeng and Ni, Na and Zhang, Pei and Wang, Peng and Peng, Ru and Men, Rui and Gao, Ruize and Lin, Runji and Wang, Shijie and Bai, Shuai and Tan, Sinan and Zhu, Tianhang and Li, Tianhao and Liu, Tianyu and Ge, Wenbin and Deng, Xiaodong and Zhou, Xiaohuan and Ren, Xingzhang and Zhang, Xinyu and Wei, Xipin and Ren, Xuancheng and Liu, Xuejing and Fan, Yang and Yao, Yang and Zhang, Yichang and Wan, Yu and Chu, Yunfei and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Guo, Zhifang and Fan, Zhihao},
	month = jul,
	year = {2024},
	note = {GSCC: 0000637 
arXiv:2407.10671 [cs]
remark: Qwen2
TLDR: A comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model, which surpasses most prior open-weight models and demonstrates robust multilingual capabilities.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yang_octopus_2024,
	title = {Octopus: {Embodied} {Vision}-{Language} {Programmer} from {Environmental} {Feedback}},
	shorttitle = {Octopus},
	url = {http://arxiv.org/abs/2310.08588},
	doi = {10.48550/arXiv.2310.08588},
	abstract = {Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.},
	language = {en-US},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Yang, Jingkang and Dong, Yuhao and Liu, Shuai and Li, Bo and Wang, Ziyue and Jiang, Chencheng and Tan, Haoran and Kang, Jiamu and Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei},
	month = oct,
	year = {2024},
	note = {GSCC: 0000054 
arXiv:2310.08588 [cs]
TLDR: Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation, is introduced and compelling results are presented, showing that the proposed RLEF refines the agent's decision-making.
remark: Octopus通过代码生成实现视觉语言代理操控。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_track_2023,
	title = {Track {Anything}: {Segment} {Anything} {Meets} {Videos}},
	shorttitle = {Track {Anything}},
	url = {http://arxiv.org/abs/2304.11968},
	doi = {10.48550/arXiv.2304.11968},
	abstract = {Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on \{https://github.com/gaomingqi/Track-Anything\}. We hope this work can facilitate related research.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Yang, Jinyu and Gao, Mingqi and Li, Zhe and Gao, Shang and Wang, Fangjing and Zheng, Feng},
	month = apr,
	year = {2023},
	note = {GSCC: 0000204 
arXiv:2304.11968 [cs]
remark: Track Anything
TLDR: The Track Anything Model (TAM) is proposed, which achieves high-performance interactive tracking and segmentation in videos with very little human participation, and achieves satisfactory results in one-pass inference.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_learning_2024,
	title = {Learning {Interactive} {Real}-{World} {Simulators}},
	url = {http://arxiv.org/abs/2310.06114},
	doi = {10.48550/arXiv.2310.06114},
	abstract = {Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as ``open the drawer'' and low-level controls such as "move by x, y" from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.},
	language = {en-US},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Kaelbling, Leslie and Schuurmans, Dale and Abbeel, Pieter},
	month = jan,
	year = {2024},
	note = {GSCC: 0000043 
arXiv:2310.06114 [cs]
remark: 具身智能 世界模型
TLDR: This work uses the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation, and shows that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications.},
	keywords = {\# 真实世界模拟器, Computer Science - Artificial Intelligence},
}

@misc{yang_llm4drive_2024,
	title = {{LLM4Drive}: {A} {Survey} of {Large} {Language} {Models} for {Autonomous} {Driving}},
	shorttitle = {{LLM4Drive}},
	url = {http://arxiv.org/abs/2311.01043},
	doi = {10.48550/arXiv.2311.01043},
	abstract = {Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their "black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about {\textbackslash}textit\{Large Language Models for Autonomous Driving (LLM4AD)\}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.},
	language = {en-US},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
	month = aug,
	year = {2024},
	note = {GSCC: 0000025 
arXiv:2311.01043 [cs]
TLDR: This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field.
remark: 大语言模型在自动驾驶领域的研究综述。},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{yu_l3mvn_2023,
	title = {{L3MVN}: {Leveraging} {Large} {Language} {Models} for {Visual} {Target} {Navigation}},
	shorttitle = {{L3MVN}},
	url = {http://arxiv.org/abs/2304.05501},
	doi = {10.1109/IROS55552.2023.10342512},
	abstract = {Visual target navigation in unknown environments is a crucial problem in robotics. Despite extensive investigation of classical and learning-based approaches in the past, robots lack common-sense knowledge about household objects and layouts. Prior state-of-the-art approaches to this task rely on learning the priors during the training and typically require significant expensive resources and time for learning. To address this, we propose a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching. Specifically, we introduce two paradigms: (i) zero-shot and (ii) feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently. Our analysis demonstrates the notable zero-shot generalization and transfer capabilities from the use of language. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and generalization. Ablation analysis also indicates that the common-sense knowledge from the language model leads to more efficient semantic exploration. Finally, we provide a real robot experiment to verify the applicability of our framework in real-world scenarios. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/l3mvn.},
	language = {en-US},
	urldate = {2024-12-26},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yu, Bangguo and Kasaei, Hamidreza and Cao, Ming},
	month = oct,
	year = {2023},
	note = {GSCC: 0000055 
arXiv:2304.05501 [cs]
TLDR: This work proposes a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching and introduces two paradigms: zero-shot and feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently.
remark: 利用大语言模型提升机器人视觉导航能力。},
	keywords = {Computer Science - Robotics},
	pages = {3554--3560},
}

@misc{zhang_uni-navid_2024,
	title = {Uni-{NaVid}: {A} {Video}-based {Vision}-{Language}-{Action} {Model} for {Unifying} {Embodied} {Navigation} {Tasks}},
	shorttitle = {Uni-{NaVid}},
	url = {http://arxiv.org/abs/2412.06224},
	doi = {10.48550/arXiv.2412.06224},
	abstract = {A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.},
	language = {en-US},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Zhang, Jiazhao and Wang, Kunyu and Wang, Shaoan and Li, Minghan and Liu, Haoran and Wei, Songlin and Wang, Zhongyuan and Zhang, Zhizheng and Wang, He},
	month = dec,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2412.06224 [cs]
GSCC: 0000000 
remark: Uni-NaVid模型统一各种导航任务。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhou_esc_2023,
	title = {{ESC}: {Exploration} with {Soft} {Commonsense} {Constraints} for {Zero}-shot {Object} {Navigation}},
	shorttitle = {{ESC}},
	url = {http://arxiv.org/abs/2301.13166},
	doi = {10.48550/arXiv.2301.13166},
	abstract = {The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288\% relative Success Rate improvement than CoW on MP3D).},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Zhou, Kaiwen and Zheng, Kaizhi and Pryor, Connor and Shen, Yilin and Jin, Hongxia and Getoor, Lise and Wang, Xin Eric},
	month = jul,
	year = {2023},
	note = {GSCC: 0000093 
arXiv:2301.13166 [cs]
TLDR: A novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments is presented.
remark: 软性常识约束提升零样本目标导航能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{liu_autodan_2023,
	title = {{AutoDAN}: {Generating} {Stealthy} {Jailbreak} {Prompts} on {Aligned} {Large} {Language} {Models}},
	shorttitle = {{AutoDAN}},
	url = {https://openreview.net/forum?id=7Jwpw4qKkb},
	abstract = {The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.},
	language = {en},
	urldate = {2024-12-26},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	month = oct,
	year = {2023},
	note = {GSCC: 0000311
remark: 自动生成隐蔽越狱提示的AutoDAN攻击。},
}

@misc{he_you_2023,
	title = {You {Only} {Prompt} {Once}: {On} the {Capabilities} of {Prompt} {Learning} on {Large} {Language} {Models} to {Tackle} {Toxic} {Content}},
	shorttitle = {You {Only} {Prompt} {Once}},
	url = {http://arxiv.org/abs/2308.05596},
	doi = {10.48550/arXiv.2308.05596},
	abstract = {The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five model architectures and eight datasets demonstrating that LLMs with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. We find that prompt learning achieves around 10{\textbackslash}\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of \$F\_1\$-score). Finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {He, Xinlei and Zannettou, Savvas and Shen, Yun and Zhang, Yang},
	month = aug,
	year = {2023},
	note = {GSCC: 0000042 
arXiv:2308.05596 [cs]
remark: 利用大语言模型通过提示学习应对有害内容。},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
}

@misc{inan_llama_2023,
	title = {Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}},
	shorttitle = {Llama {Guard}},
	url = {http://arxiv.org/abs/2312.06674},
	doi = {10.48550/arXiv.2312.06674},
	abstract = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
	language = {en-US},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
	month = dec,
	year = {2023},
	note = {GSCC: 0000248 
arXiv:2312.06674 [cs]
remark: Llama Guard
TLDR: Llama Guard is introduced, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases and demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{mehrotra_tree_2024,
	title = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
	shorttitle = {Tree of {Attacks}},
	url = {http://arxiv.org/abs/2312.02119},
	doi = {10.48550/arXiv.2312.02119},
	abstract = {While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80\% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
	month = oct,
	year = {2024},
	note = {GSCC: 0000134 
arXiv:2312.02119 [cs]
GSCC: 0000134 
TLDR: This work presents Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM and generates prompts that jailbreak state-of-the-art LLMs for more than 80\% of the prompts.
remark: TAP方法自动生成Jailbreak攻击，提高成功率。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{shen_anything_2024,
	title = {"{Do} {Anything} {Now}": {Characterizing} and {Evaluating} {In}-{The}-{Wild} {Jailbreak} {Prompts} on {Large} {Language} {Models}},
	shorttitle = {"{Do} {Anything} {Now}"},
	url = {http://arxiv.org/abs/2308.03825},
	doi = {10.48550/arXiv.2308.03825},
	abstract = {The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.},
	language = {en-US},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
	month = may,
	year = {2024},
	note = {GSCC: 0000360 
arXiv:2308.03825
TLDR: This paper conducts a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023 and identifies five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days.
remark: 分析和评估LLM中的越狱提示攻击。},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{keysan_can_2023,
	title = {Can you text what is happening? {Integrating} pre-trained language encoders into trajectory prediction models for autonomous driving},
	shorttitle = {Can you text what is happening?},
	url = {http://arxiv.org/abs/2309.05282},
	doi = {10.48550/arXiv.2309.05282},
	abstract = {In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder. First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Keysan, Ali and Look, Andreas and Kosman, Eitan and Gürsun, Gonca and Wagner, Jörg and Yao, Yu and Rakitsch, Barbara},
	month = sep,
	year = {2023},
	note = {GSCC: 0000021 
arXiv:2309.05282 [cs]
remark: 文本表征提升自动驾驶预测性能
TLDR: A novel text-based representation of traffic scenes is proposed and a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chen_2nav_2023,
	title = {\${A}{\textasciicircum}2\${Nav}: {Action}-{Aware} {Zero}-{Shot} {Robot} {Navigation} by {Exploiting} {Vision}-and-{Language} {Ability} of {Foundation} {Models}},
	shorttitle = {\${A}{\textasciicircum}2\${Nav}},
	url = {http://arxiv.org/abs/2308.07997},
	doi = {10.48550/arXiv.2308.07997},
	abstract = {We study the task of zero-shot vision-and-language navigation (ZS-VLN), a practical yet challenging problem in which an agent learns to navigate following a path described by language instructions without requiring any path-instruction annotation data. Normally, the instructions have complex grammatical structures and often contain various action descriptions (e.g., "proceed beyond", "depart from"). How to correctly understand and execute these action demands is a critical problem, and the absence of annotated data makes it even more challenging. Note that a well-educated human being can easily understand path instructions without the need for any special training. In this paper, we propose an action-aware zero-shot VLN method (\$A{\textasciicircum}2\$Nav) by exploiting the vision-and-language ability of foundation models. Specifically, the proposed method consists of an instruction parser and an action-aware navigation policy. The instruction parser utilizes the advanced reasoning ability of large language models (e.g., GPT-3) to decompose complex navigation instructions into a sequence of action-specific object navigation sub-tasks. Each sub-task requires the agent to localize the object and navigate to a specific goal position according to the associated action demand. To accomplish these sub-tasks, an action-aware navigation policy is learned from freely collected action-specific datasets that reveal distinct characteristics of each action demand. We use the learned navigation policy for executing sub-tasks sequentially to follow the navigation instruction. Extensive experiments show \$A{\textasciicircum}2\$Nav achieves promising ZS-VLN performance and even surpasses the supervised learning methods on R2R-Habitat and RxR-Habitat datasets.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Chen, Peihao and Sun, Xinyu and Zhi, Hongyan and Zeng, Runhao and Li, Thomas H. and Liu, Gaowen and Tan, Mingkui and Gan, Chuang},
	month = aug,
	year = {2023},
	note = {GSCC: 0000021 
arXiv:2308.07997 [cs]
TLDR: This paper proposes an action-aware zero-shot VLN method (\$A{\textasciicircum}2\$Nav) by exploiting the vision-and-language ability of foundation models and uses the advanced reasoning ability of large language models to decompose complex navigation instructions into a sequence of action-specific object navigation sub-tasks.
remark: \$A{\textasciicircum}2\$Nav利用大模型实现零样本导航。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{dorbala_clip-nav_2022,
	title = {{CLIP}-{Nav}: {Using} {CLIP} for {Zero}-{Shot} {Vision}-and-{Language} {Navigation}},
	shorttitle = {{CLIP}-{Nav}},
	url = {http://arxiv.org/abs/2211.16649},
	doi = {10.48550/arXiv.2211.16649},
	abstract = {Household environments are visually diverse. Embodied agents performing Vision-and-Language Navigation (VLN) in the wild must be able to handle this diversity, while also following arbitrary language instructions. Recently, Vision-Language models like CLIP have shown great performance on the task of zero-shot object recognition. In this work, we ask if these models are also capable of zero-shot language grounding. In particular, we utilize CLIP to tackle the novel problem of zero-shot VLN using natural language referring expressions that describe target objects, in contrast to past work that used simple language templates describing object classes. We examine CLIP's capability in making sequential navigational decisions without any dataset-specific finetuning, and study how it influences the path that an agent takes. Our results on the coarse-grained instruction following task of REVERIE demonstrate the navigational capability of CLIP, surpassing the supervised baseline in terms of both success rate (SR) and success weighted by path length (SPL). More importantly, we quantitatively show that our CLIP-based zero-shot approach generalizes better to show consistent performance across environments when compared to SOTA, fully supervised learning approaches when evaluated via Relative Change in Success (RCS).},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Dorbala, Vishnu Sashank and Sigurdsson, Gunnar and Piramuthu, Robinson and Thomason, Jesse and Sukhatme, Gaurav S.},
	month = nov,
	year = {2022},
	note = {GSCC: 0000053 
arXiv:2211.16649 [cs]
TLDR: This work examines CLIP's capability in making sequential navigational decisions without any dataset-specific finetuning, and study how it influences the path that an agent takes, and demonstrates the navigational capability of CLIP.
remark: CLIP实现零样本视觉语言导航。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{lin_advances_2024,
	title = {Advances in {Embodied} {Navigation} {Using} {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Advances in {Embodied} {Navigation} {Using} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.00530},
	doi = {10.48550/arXiv.2311.00530},
	abstract = {In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Lin, Jinzhou and Gao, Han and Feng, Xuxiang and Xu, Rongtao and Wang, Changwei and Zhang, Man and Guo, Li and Xu, Shibiao},
	month = jun,
	year = {2024},
	note = {GSCC: 0000002 
arXiv:2311.00530 [cs]
TLDR: An exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation is offered, which reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets.
remark: 大模型提升具身智能导航能力。},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{qiao_march_2023,
	title = {March in {Chat}: {Interactive} {Prompting} for {Remote} {Embodied} {Referring} {Expression}},
	shorttitle = {March in {Chat}},
	url = {http://arxiv.org/abs/2308.10141},
	doi = {10.48550/arXiv.2308.10141},
	abstract = {Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent years, from room-based to object-based and indoor to outdoor. The REVERIE (Remote Embodied Referring Expression) is interesting since it only provides high-level instructions to the agent, which are closer to human commands in practice. Nevertheless, this poses more challenges than other VLN tasks since it requires agents to infer a navigation plan only based on a short instruction. Large Language Models (LLMs) show great potential in robot action planning by providing proper prompts. Still, this strategy has not been explored under the REVERIE settings. There are several new challenges. For example, the LLM should be environment-aware so that the navigation plan can be adjusted based on the current visual observation. Moreover, the LLM planned actions should be adaptable to the much larger and more complex REVERIE environment. This paper proposes a March-in-Chat (MiC) model that can talk to the LLM on the fly and plan dynamically based on a newly proposed Room-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the previous state-of-the-art by large margins by SPL and RGSPL metrics on the REVERIE benchmark.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Qiao, Yanyuan and Qi, Yuankai and Yu, Zheng and Liu, Jing and Wu, Qi},
	month = aug,
	year = {2023},
	note = {GSCC: 0000030 
arXiv:2308.10141 [cs]
remark: MiC模型提升高层指令导航性能。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rajvanshi_saynav_2024,
	title = {{SayNav}: {Grounding} {Large} {Language} {Models} for {Dynamic} {Planning} to {Navigation} in {New} {Environments}},
	shorttitle = {{SayNav}},
	url = {http://arxiv.org/abs/2309.04077},
	doi = {10.48550/arXiv.2309.04077},
	abstract = {Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8\% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Rajvanshi, Abhinav and Sikka, Karan and Lin, Xiao and Lee, Bhoram and Chiu, Han-Pang and Velasquez, Alvaro},
	month = apr,
	year = {2024},
	note = {GSCC: 0000028 
arXiv:2309.04077 [cs]
TLDR: SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8\% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments.
remark: SayNav利用LLM动态规划实现复杂导航。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhou_navgpt_2023,
	title = {{NavGPT}: {Explicit} {Reasoning} in {Vision}-and-{Language} {Navigation} with {Large} {Language} {Models}},
	shorttitle = {{NavGPT}},
	url = {http://arxiv.org/abs/2305.16986},
	doi = {10.48550/arXiv.2305.16986},
	abstract = {Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.},
	language = {en-US},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Zhou, Gengze and Hong, Yicong and Wu, Qi},
	month = oct,
	year = {2023},
	note = {GSCC: 0000111 
arXiv:2305.16986 [cs]
GSCC: 0000111 
TLDR: The NavGPT is introduced, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN).
remark: 大模型NavGPT在视觉导航中展现出推理能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kuang_openfmnav_2024,
	title = {{OpenFMNav}: {Towards} {Open}-{Set} {Zero}-{Shot} {Object} {Navigation} via {Vision}-{Language} {Foundation} {Models}},
	shorttitle = {{OpenFMNav}},
	url = {http://arxiv.org/abs/2402.10670},
	abstract = {Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.},
	language = {en-US},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Kuang, Yuxuan and Lin, Hai and Jiang, Meng},
	month = feb,
	year = {2024},
	note = {GSCC: 0000012 
arXiv:2402.10670 [cs]
Issue: arXiv:2402.10670
remark: OpenFMNav提升零样本导航在开放环境的能力。},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics},
}

@misc{shah_lm-nav_2022,
	title = {{LM}-{Nav}: {Robotic} {Navigation} with {Large} {Pre}-{Trained} {Models} of {Language}, {Vision}, and {Action}},
	shorttitle = {{LM}-{Nav}},
	url = {http://arxiv.org/abs/2207.04429},
	doi = {10.48550/arXiv.2207.04429},
	language = {en-US},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
	month = jul,
	year = {2022},
	note = {GSCC: 0000416 
arXiv:2207.04429 [cs]
remark: 无需注释，使用大模型实现机器导航。
TLDR: This work presents a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user, constructed entirely out of pre-trained models for navigation, image-language association, and language modeling.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{singh_progprompt_2022,
	title = {{ProgPrompt}: {Generating} {Situated} {Robot} {Task} {Plans} using {Large} {Language} {Models}},
	shorttitle = {{ProgPrompt}},
	url = {http://arxiv.org/abs/2209.11302},
	doi = {10.48550/arXiv.2209.11302},
	abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
	language = {en-US},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	month = sep,
	year = {2022},
	note = {GSCC: 0000667 
arXiv:2209.11302 [cs]
remark: 利用大型语言模型生成适应不同环境和任务的机器人行为计划。},
	keywords = {\# ProgPrompt, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yao_poisonprompt_2023,
	title = {{PoisonPrompt}: {Backdoor} {Attack} on {Prompt}-based {Large} {Language} {Models}},
	shorttitle = {{PoisonPrompt}},
	url = {http://arxiv.org/abs/2310.12439},
	doi = {10.48550/arXiv.2310.12439},
	abstract = {Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Yao, Hongwei and Lou, Jian and Qin, Zhan},
	month = dec,
	year = {2023},
	note = {GSCC: 0000035 
arXiv:2310.12439 [cs]
Issue: arXiv:2310.12439
remark: POISONPROMPT攻击大语言模型前门漏洞。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yin_vlattack_2023,
	title = {{VLAttack}: {Multimodal} {Adversarial} {Attacks} on {Vision}-{Language} {Tasks} via {Pre}-trained {Models}},
	shorttitle = {{VLAttack}},
	url = {http://arxiv.org/abs/2310.04655},
	doi = {10.48550/arXiv.2310.04655},
	abstract = {Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multimodal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level. We conduct extensive experiments to attack three widely-used VL pretrained models for six tasks on eight datasets. Experimental results show that the proposed VLAttack framework achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a significant blind spot in the deployment of pre-trained VL models. Codes will be released soon.},
	language = {en-US},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Yin, Ziyi and Ye, Muchao and Zhang, Tianrong and Du, Tianyu and Zhu, Jinguo and Liu, Han and Chen, Jinghui and Wang, Ting and Ma, Fenglong},
	month = nov,
	year = {2023},
	note = {GSCC: 0000020 
arXiv:2310.04655 [cs]
Issue: arXiv:2310.04655
remark: 利用预训练模型对视觉语言任务进行多模态对抗攻击。
TLDR: This paper proposes VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels, and proposes a novel iterative cross-search attack method to update adversarial image-text pairs periodically.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{yu_meta-world_2021,
	title = {Meta-{World}: {A} {Benchmark} and {Evaluation} for {Multi}-{Task} and {Meta} {Reinforcement} {Learning}},
	shorttitle = {Meta-{World}},
	url = {http://arxiv.org/abs/1910.10897},
	doi = {10.48550/arXiv.1910.10897},
	abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
	language = {en-US},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Narayan, Avnish and Shively, Hayden and Bellathur, Adithya and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
	month = jun,
	year = {2021},
	note = {GSCC: 0001171 
arXiv:1910.10897 [cs, stat]
remark: 提出50项任务的元强化学习基准。},
	keywords = {\# Meta-World, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{yu_mm-llms_2024,
	title = {{MM}-{LLMs}: {Recent} {Advances} in {MultiModal} {Large} {Language} {Models}},
	shorttitle = {{MM}-{LLMs}},
	url = {http://arxiv.org/abs/2401.13601},
	doi = {10.48550/arXiv.2401.13601},
	abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Yu, Yahan and Dong, Jiahua and Li, Chenxing and Su, Dan and Chu, Chenhui and Yu, Dong},
	month = may,
	year = {2024},
	note = {GSCC: 0000173 
arXiv:2401.13601
TLDR: A taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations is introduced, and the performance of selected MM-LLMs on mainstream benchmarks and key training recipes to enhance the potency of MM-LLMs are reviewed.
remark: 综述多模态大语言模型的设计与进展。},
	keywords = {\# 结构 训练 分类, /done, Computer Science - Computation and Language},
}

@misc{zhai_sigmoid_2023,
	title = {Sigmoid {Loss} for {Language} {Image} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2303.15343},
	doi = {10.48550/arXiv.2303.15343},
	abstract = {We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5\% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at https://github.com/google-research/big\_vision and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.},
	language = {en-US},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
	month = sep,
	year = {2023},
	note = {GSCC: 0000521 
arXiv:2303.15343
remark: SigLIP},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_trafficgpt_2023,
	title = {{TrafficGPT}: {Viewing}, {Processing} and {Interacting} with {Traffic} {Foundation} {Models}},
	shorttitle = {{TrafficGPT}},
	url = {http://arxiv.org/abs/2309.06719},
	doi = {10.48550/arXiv.2309.06719},
	abstract = {With the promotion of chatgpt to the public, Large language models indeed showcase remarkable common sense, reasoning, and planning skills, frequently providing insightful guidance. These capabilities hold significant promise for their application in urban traffic management and control. However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges. In parallel, specialized traffic foundation models exist but are typically designed for specific tasks with limited input-output interactions. Combining these models with LLMs presents an opportunity to enhance their capacity for tackling complex traffic-related problems and providing insightful suggestions. To bridge this gap, we present TrafficGPT, a fusion of ChatGPT and traffic foundation models. This integration yields the following key enhancements: 1) empowering ChatGPT with the capacity to view, analyze, process traffic data, and provide insightful decision support for urban transportation system management; 2) facilitating the intelligent deconstruction of broad and complex tasks and sequential utilization of traffic foundation models for their gradual completion; 3) aiding human decision-making in traffic control through natural language dialogues; and 4) enabling interactive feedback and solicitation of revised outcomes. By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic management but also offers a novel approach to leveraging AI capabilities in this domain. The TrafficGPT demo can be found in https://github.com/lijlansg/TrafficGPT.git.},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Zhang, Siyao and Fu, Daocheng and Zhang, Zhao and Yu, Bin and Cai, Pinlong},
	month = sep,
	year = {2023},
	note = {GSCC: 0000060 
arXiv:2309.06719 [cs]
remark: ChatGPT与交通信息模型结合。
TLDR: By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic management but also offers a novel approach to leveraging AI capabilities in this domain.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{zhang_large_2024,
	title = {Large {Language} {Models} for {Time} {Series}: {A} {Survey}},
	shorttitle = {Large {Language} {Models} for {Time} {Series}},
	url = {http://arxiv.org/abs/2402.01801},
	doi = {10.48550/arXiv.2402.01801},
	abstract = {Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.},
	language = {en-US},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Zhang, Xiyuan and Chowdhury, Ranak Roy and Gupta, Rajesh K. and Shang, Jingbo},
	month = may,
	year = {2024},
	note = {GSCC: 0000020 
arXiv:2402.01801
TLDR: An in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis are provided, addressing the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data.
remark: LLMs在时间序列分析中的应用与挑战。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang_sirens_2023,
	title = {Siren's {Song} in the {AI} {Ocean}: {A} {Survey} on {Hallucination} in {Large} {Language} {Models}},
	shorttitle = {Siren's {Song} in the {AI} {Ocean}},
	url = {http://arxiv.org/abs/2309.01219},
	abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
	urldate = {2023-12-26},
	publisher = {arXiv},
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	month = sep,
	year = {2023},
	note = {GSCC: 0000815 
arXiv:2309.01219 [cs]
Issue: arXiv:2309.01219
remark: 大型语言模型幻觉现象的检测与缓解。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{zhao_evaluating_2023,
	title = {On {Evaluating} {Adversarial} {Robustness} of {Large} {Vision}-{Language} {Models}},
	url = {https://arxiv.org/abs/2305.16934v2},
	abstract = {Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
	month = may,
	year = {2023},
	note = {GSCC: 0000163 
remark: 评估大视觉语言模型的对抗鲁棒性。},
}

@misc{zhong_language-guided_2023,
	title = {Language-{Guided} {Traffic} {Simulation} via {Scene}-{Level} {Diffusion}},
	url = {http://arxiv.org/abs/2306.06344},
	doi = {10.48550/arXiv.2306.06344},
	abstract = {Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effectiveness of our proposed method in generating realistic, query-compliant traffic simulations.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Zhong, Ziyuan and Rempe, Davis and Chen, Yuxiao and Ivanovic, Boris and Cao, Yulong and Xu, Danfei and Pavone, Marco and Ray, Baishakhi},
	month = oct,
	year = {2023},
	note = {GSCC: 0000057 
arXiv:2306.06344 [cs]
remark: "利用语言指引和场景级扩散模型进行自动驾驶交通仿真。CTG++"
TLDR: CTG++ is presented, a scene-level conditional diffusion model that can be guided by language instructions that generates realistic and controllable traffic and harnesses a large language model to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ma_when_2024,
	title = {When {LLMs} step into the {3D} {World}: {A} {Survey} and {Meta}-{Analysis} of {3D} {Tasks} via {Multi}-modal {Large} {Language} {Models}},
	shorttitle = {When {LLMs} step into the {3D} {World}},
	url = {http://arxiv.org/abs/2405.10255},
	doi = {10.48550/arXiv.2405.10255},
	abstract = {As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.},
	language = {en-US},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Ma, Xianzheng and Bhalgat, Yash and Smart, Brandon and Chen, Shuai and Li, Xinghui and Ding, Jian and Gu, Jindong and Chen, Dave Zhenyu and Peng, Songyou and Bian, Jia-Wang and Torr, Philip H. and Pollefeys, Marc and Nießner, Matthias and Reid, Ian D. and Chang, Angel X. and Laina, Iro and Prisacariu, Victor Adrian},
	month = may,
	year = {2024},
	note = {GSCC: 0000009 
arXiv:2405.10255
TLDR: This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data, and examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue.
remark: 大语言模型在3D任务中应用的研究进展概述。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{madry_towards_2019,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	doi = {10.48550/arXiv.1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en-US},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = sep,
	year = {2019},
	note = {GSCC: 0013935 
arXiv:1706.06083 [cs, stat]
remark: PGD对抗样本攻击},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{mccarthy_towards_2024,
	title = {Towards {Generalist} {Robot} {Learning} from {Internet} {Video}: {A} {Survey}},
	shorttitle = {Towards {Generalist} {Robot} {Learning} from {Internet} {Video}},
	url = {http://arxiv.org/abs/2404.19664},
	doi = {10.48550/arXiv.2404.19664},
	abstract = {Scaling deep learning to huge internet-scraped datasets has yielded remarkably general capabilities in natural language processing and visual understanding and generation. In contrast, data is scarce and expensive to collect in robotics. This has seen robot learning struggle to match the generality of capabilities observed in other domains. Learning from Videos (LfV) methods seek to address this data bottleneck by augmenting traditional robot data with large internet-scraped video datasets. Such video data may provide the model with foundational information regarding physical behaviours and the physics of the world. This holds great promise for improving the generality of our robots. In this survey, we present an overview of the emerging field of LfV. We outline fundamental concepts, including the benefits and challenges of LfV. We provide a comprehensive review of current methods for: extracting knowledge from large-scale internet video; tackling key LfV challenges; and boosting downstream reinforcement and robot learning via the use of video data. LfV datasets and benchmarks are also reviewed. The survey closes with a critical discussion of challenges and opportunities. Here, we advocate for scalable foundation model approaches that can leverage the full range of available internet video to aid the learning of robot policies and dynamics models. We hope this survey can inform and catalyse further LfV research, facilitating progress towards the development of general-purpose robots.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {McCarthy, Robert and Tan, Daniel C. H. and Schmidt, Dominik and Acero, Fernando and Herr, Nathan and Du, Yilun and Thuruthel, Thomas G. and Li, Zhibin},
	month = oct,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2404.19664
TLDR: An overview of the emerging field of LfV is presented, including a comprehensive review of current methods for extracting knowledge from large-scale internet video; tackling key LfV challenges; and boosting downstream reinforcement and robot learning via the use of video data.
remark: 互联网视频增强机器人学习的前景与挑战综述。},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{min_recent_2023,
	title = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-trained {Language} {Models}: {A} {Survey}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-trained {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3605943},
	doi = {10.1145/3605943},
	abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.BERT 和 GPT 等大型预训练语言模型 （PLM） 极大地改变了自然语言处理 （NLP） 领域。对于许多 NLP 任务，利用 PLM 的方法已经实现了最先进的性能。关键思想是从通用任务中学习一次通用的、潜在的语言表示，然后在不同的 NLP 任务之间共享它。语言建模是通用任务，具有丰富的自监督文本，可用于广泛的训练。本文介绍了 PLM 架构的关键基本概念，并全面介绍了向 PLM 驱动的 NLP 技术的转变。它调查了应用预训练、微调、提示和文本生成方法的工作。此外，它还讨论了 PLM 的局限性和未来研究的建议方向。},
	number = {2},
	urldate = {2023-12-21},
	journal = {ACM Computing Surveys},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	year = {2023},
	note = {GSCC: 0001023 
remark: 预训练语言模型在NLP的进展。},
	keywords = {Large language models, foundational models, generative AI, neural networks},
	pages = {30:1--30:40},
}

@misc{minderer_simple_2022,
	title = {Simple {Open}-{Vocabulary} {Object} {Detection} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2205.06230},
	doi = {10.48550/arXiv.2205.06230},
	abstract = {Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
	month = jul,
	year = {2022},
	note = {GSCC: 0000138 
arXiv:2205.06230 [cs]
remark: OWL-VIT
TLDR: This paper proposes a strong recipe for transferring image-text models to open-vocabulary object detection using a standard Vision Transformer architecture with minimal modifications, contrastive image- text pre-training, and end-to-end detection fine-tuning.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{naseh_risks_2023,
	title = {On the {Risks} of {Stealing} the {Decoding} {Algorithms} of {Language} {Models}},
	url = {http://arxiv.org/abs/2303.04729},
	doi = {10.48550/arXiv.2303.04729},
	abstract = {A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., \${\textbackslash}\$0.8\$, \${\textbackslash}\$1\$, \${\textbackslash}\$4\$, and \${\textbackslash}\$40\$ for the four versions of GPT-3.},
	language = {en-US},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Naseh, Ali and Krishna, Kalpesh and Iyyer, Mohit and Houmansadr, Amir},
	month = apr,
	year = {2023},
	note = {GSCC: 0000008 
arXiv:2303.04729 [cs]
remark: 揭示解码算法和参数被低成本窃取的风险。},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{neupane_security_2024,
	title = {Security {Considerations} in {AI}-{Robotics}: {A} {Survey} of {Current} {Methods}, {Challenges}, and {Opportunities}},
	shorttitle = {Security {Considerations} in {AI}-{Robotics}},
	url = {http://arxiv.org/abs/2310.08565},
	abstract = {Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.},
	language = {en-US},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Neupane, Subash and Mitra, Shaswata and Fernandez, Ivan A. and Saha, Swayamjit and Mittal, Sudip and Chen, Jingdao and Pillai, Nisha and Rahimi, Shahram},
	month = jan,
	year = {2024},
	note = {GSCC: 0000014 
arXiv:2310.08565 [cs]
Issue: arXiv:2310.08565
remark: AI-机器人系统安全综述。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhu_languagebind_2023,
	title = {{LanguageBind}: {Extending} {Video}-{Language} {Pretraining} to {N}-modality by {Language}-based {Semantic} {Alignment}},
	shorttitle = {{LanguageBind}},
	url = {http://arxiv.org/abs/2310.01852},
	doi = {10.48550/arXiv.2310.01852},
	abstract = {The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N{\textgreater}=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with complete semantics rather than truncated segments from long videos, and all the video, depth, infrared, and audio modalities are aligned to their textual descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8\% R@1 on the MSR-VTT dataset with only 15\% of the parameters in the zero-shot video-text retrieval task. Beyond this, our LanguageBind has greatly improved in the zero-shot video, audio, depth, and infrared understanding tasks. For instance, LanguageBind surpassing InterVideo by 1.9\% on MSR-VTT, 8.8\% on MSVD, 6.3\% on DiDeMo, and 4.4\% on ActivityNet. On the LLVIP and NYU-D datasets, LanguageBind outperforms ImageBind with 23.8\% and 11.1\% top-1 accuracy. Code address: https://github.com/PKU-YuanGroup/LanguageBind.},
	language = {en-US},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and Wang, HongFa and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and Zhang, Wancai and Li, Zhifeng and Liu, Wei and Yuan, Li},
	month = nov,
	year = {2023},
	note = {GSCC: 0000132 
arXiv:2310.01852 [cs]
remark: 将视频-语言预训练扩展至多模态。
TLDR: This work proposes LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics, and freezes the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhu_tpatch_2023,
	title = {\{{TPatch}\}: {A} {Triggered} {Physical} {Adversarial} {Patch}},
	isbn = {978-1-939133-37-3},
	shorttitle = {\{{TPatch}\}},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/zhu},
	language = {en},
	urldate = {2023-11-06},
	author = {Zhu, Wenjun and Ji, Xiaoyu and Cheng, Yushi and Zhang, Shibo and Xu, Wenyuan},
	year = {2023},
	note = {GSCC: 0000018 
remark: TPatch由声学信号触发的物理对抗补丁。与其他对抗性补丁不同，TPatch 在正常情况下保持良性，但可以通过针对摄像机的信号注入攻击引入的设计失真来触发发起隐藏、创建或改变攻击。},
	pages = {661--678},
}

@misc{zou_universal_2023,
	title = {Universal and {Transferable} {Adversarial} {Attacks} on {Aligned} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.15043},
	doi = {10.48550/arXiv.2307.15043},
	abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
	language = {en},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	month = dec,
	year = {2023},
	note = {GSCC: 0000959 
arXiv:2307.15043 [cs]
remark: 通过攻击后缀使对齐模型生成不良内容。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{wei_jailbroken_2023,
	title = {Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?},
	shorttitle = {Jailbroken},
	url = {http://arxiv.org/abs/2307.02483},
	doi = {10.48550/arXiv.2307.02483},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.},
	language = {en},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	month = jul,
	year = {2023},
	note = {GSCC: 0000677 
arXiv:2307.02483 [cs]
remark: 研究大型语言模型安全训练失败原因。
TLDR: The analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{luo_jailbreakv-28k_2024,
	title = {{JailBreakV}-{28K}: {A} {Benchmark} for {Assessing} the {Robustness} of {MultiModal} {Large} {Language} {Models} against {Jailbreak} {Attacks}},
	shorttitle = {{JailBreakV}-{28K}},
	url = {http://arxiv.org/abs/2404.03027},
	doi = {10.48550/arXiv.2404.03027},
	abstract = {With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.},
	language = {en-US},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
	month = jul,
	year = {2024},
	note = {GSCC: 0000041 
arXiv:2404.03027
TLDR: This paper introduces JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks and highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities.
remark: 引入JailBreakV-28K评估多模态模型安全性。},
	keywords = {\# COLM 2024, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{liu_jailbreaking_2024,
	title = {Jailbreaking {ChatGPT} via {Prompt} {Engineering}: {An} {Empirical} {Study}},
	shorttitle = {Jailbreaking {ChatGPT} via {Prompt} {Engineering}},
	url = {http://arxiv.org/abs/2305.13860},
	doi = {10.48550/arXiv.2305.13860},
	abstract = {Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.},
	language = {en-US},
	urldate = {2024-04-06},
	publisher = {arXiv},
	author = {Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
	month = mar,
	year = {2024},
	note = {GSCC: 0000361 
arXiv:2305.13860 [cs]
remark: 研究ChatGPT越狱提示词的分类及效果。
TLDR: The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{shayegani_jailbreak_2023,
	title = {Jailbreak in pieces: {Compositional} {Adversarial} {Attacks} on {Multi}-{Modal} {Language} {Models}},
	shorttitle = {Jailbreak in pieces},
	url = {http://arxiv.org/abs/2307.14539},
	doi = {10.48550/arXiv.2307.14539},
	abstract = {We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.},
	language = {en-US},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
	month = oct,
	year = {2023},
	note = {GSCC: 0000095 
arXiv:2307.14539
TLDR: By introducing adversarial embedding space attacks, this paper emphasizes the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems.
remark: 介绍了针对视-语言模型的越狱攻击。},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{mo_iot-lm_2024,
	title = {{IoT}-{LM}: {Large} {Multisensory} {Language} {Models} for the {Internet} of {Things}},
	shorttitle = {{IoT}-{LM}},
	url = {http://arxiv.org/abs/2407.09801},
	doi = {10.48550/arXiv.2407.09801},
	abstract = {The Internet of Things (IoT) network integrating billions of smart physical devices embedded with sensors, software, and communication technologies is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, and audio to recognize the states of humans and physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To realize this potential, we introduce IoT-LM, an open-source large multisensory language model tailored for the IoT ecosystem. IoT-LM is enabled by two technical contributions: the first is MultiIoT, the most expansive unified IoT dataset to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks prepared for multisensory pre-training and instruction-tuning. The second is a new multisensory multitask adapter layer to condition pre-trained large language models on multisensory IoT data. Not only does IoT-LM yield substantial improvements on 8 supervised IoT classification tasks, but it also demonstrates new interactive question-answering, reasoning, and dialog capabilities conditioned on IoT sensors. We release IoT-LM's data sources and new multisensory language modeling framework.},
	language = {en-US},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Mo, Shentong and Salakhutdinov, Russ and Morency, Louis-Philippe and Liang, Paul Pu},
	month = jul,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2407.09801
TLDR: IoT-LM, an open-source large multisensory language model tailored for the IoT ecosystem, is introduced and demonstrates new interactive question-answering, reasoning, and dialog capabilities conditioned on IoT sensors.
remark: IoT-LM利用IoT数据提升智能问答能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{an_iot-llm_2024,
	title = {{IoT}-{LLM}: {Enhancing} {Real}-{World} {IoT} {Task} {Reasoning} with {Large} {Language} {Models}},
	shorttitle = {{IoT}-{LLM}},
	url = {http://arxiv.org/abs/2410.02429},
	doi = {10.48550/arXiv.2410.02429},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities across textual and visual domains but often generate outputs that violate physical laws, revealing a gap in their understanding of the physical world. Inspired by human cognition, where perception is fundamental to reasoning, we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT task reasoning in the physical world. In this work, we systematically study LLMs capability to address real-world IoT tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions, and expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning. To evaluate the performance, We design a new benchmark with five real-world IoT tasks with different data types and reasoning difficulties and provide the benchmarking results on six open-source and close-source LLMs. Experimental results demonstrate the limitations of existing LLMs with naive textual inputs that cannot perform these tasks effectively. We show that IoT-LLM significantly enhances the performance of IoT tasks reasoning of LLM, such as GPT-4, achieving an average improvement of 65\% across various tasks against previous methods. The results also showcase LLMs ability to comprehend IoT data and the physical law behind data by providing a reasoning process. Limitations of our work are claimed to inspire future research in this new era.},
	language = {en-US},
	urldate = {2024-10-28},
	publisher = {arXiv},
	author = {An, Tuo and Zhou, Yunjiao and Zou, Han and Yang, Jianfei},
	month = oct,
	year = {2024},
	note = {GSCC: 0000002 
arXiv:2410.02429
TLDR: IoT-LLM significantly enhances the performance of IoT tasks reasoning of LLM, such as GPT-4, achieving an average improvement of 65\% across various tasks against previous methods.
remark: 增强大模型的物联网任务推理能力。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chhipa_investigating_2024,
	title = {Investigating {Robustness} of {Open}-{Vocabulary} {Foundation} {Object} {Detectors} under {Distribution} {Shifts}},
	url = {http://arxiv.org/abs/2405.14874},
	doi = {10.48550/arXiv.2405.14874},
	abstract = {The challenge of Out-Of-Distribution (OOD) robustness remains a critical hurdle towards deploying deep vision models. Open-vocabulary object detection extends the capabilities of traditional object detection frameworks to recognize and classify objects beyond predefined categories. Investigating OOD robustness in open-vocabulary object detection is essential to increase the trustworthiness of these models. This study presents a comprehensive robustness evaluation of zero-shot capabilities of three recent open-vocabulary foundation object detection models, namely OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on the COCO-O and COCO-C benchmarks encompassing distribution shifts highlight the challenges of the models' robustness. Source code shall be made available to the research community on GitHub.},
	language = {en-US},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Chhipa, Prakash Chandra and De, Kanjar and Chippa, Meenakshi Subhash and Saini, Rajkumar and Liwicki, Marcus},
	month = jun,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2405.14874 [cs]
remark: 探讨开放词汇目标检测的分布外鲁棒性。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{moroncelli_integrating_2024,
	title = {Integrating {Reinforcement} {Learning} with {Foundation} {Models} for {Autonomous} {Robotics}: {Methods} and {Perspectives}},
	shorttitle = {Integrating {Reinforcement} {Learning} with {Foundation} {Models} for {Autonomous} {Robotics}},
	url = {https://arxiv.org/abs/2410.16411v1},
	abstract = {Foundation models (FMs), large deep learning models pre-trained on vast, unlabeled datasets, exhibit powerful capabilities in understanding complex patterns and generating sophisticated outputs. However, they often struggle to adapt to specific tasks. Reinforcement learning (RL), which allows agents to learn through interaction and feedback, offers a compelling solution. Integrating RL with FMs enables these models to achieve desired outcomes and excel at particular tasks. Additionally, RL can be enhanced by leveraging the reasoning and generalization capabilities of FMs. This synergy is revolutionizing various fields, including robotics. FMs, rich in knowledge and generalization, provide robots with valuable information, while RL facilitates learning and adaptation through real-world interactions. This survey paper comprehensively explores this exciting intersection, examining how these paradigms can be integrated to advance robotic intelligence. We analyze the use of foundation models as action planners, the development of robotics-specific foundation models, and the mutual benefits of combining FMs with RL. Furthermore, we present a taxonomy of integration approaches, including large language models, vision-language models, diffusion models, and transformer-based RL models. We also explore how RL can utilize world representations learned from FMs to enhance robotic task execution. Our survey aims to synthesize current research and highlight key challenges in robotic reasoning and control, particularly in the context of integrating FMs and RL--two rapidly evolving technologies. By doing so, we seek to spark future research and emphasize critical areas that require further investigation to enhance robotics. We provide an updated collection of papers based on our taxonomy, accessible on our open-source project website at: https://github.com/clmoro/Robotics-RL-FMs-Integration.},
	language = {en},
	urldate = {2024-10-26},
	journal = {arXiv.org},
	author = {Moroncelli, Angelo and Soni, Vishal and Shahid, Asad Ali and Maccarini, Marco and Forgione, Marco and Piga, Dario and Spahiu, Blerina and Roveda, Loris},
	month = oct,
	year = {2024},
	note = {GSCC: 0000000 
remark: 基础模型与强化学习结合提升机器人智能。},
}

@misc{li_inaudible_2023,
	title = {Inaudible {Adversarial} {Perturbation}: {Manipulating} the {Recognition} of {User} {Speech} in {Real} {Time}},
	shorttitle = {Inaudible {Adversarial} {Perturbation}},
	url = {http://arxiv.org/abs/2308.01040},
	doi = {10.48550/arXiv.2308.01040},
	abstract = {Automatic speech recognition (ASR) systems have been shown to be vulnerable to adversarial examples (AEs). Recent success all assumes that users will not notice or disrupt the attack process despite the existence of music/noise-like sounds and spontaneous responses from voice assistants. Nonetheless, in practical user-present scenarios, user awareness may nullify existing attack attempts that launch unexpected sounds or ASR usage. In this paper, we seek to bridge the gap in existing research and extend the attack to user-present scenarios. We propose VRIFLE, an inaudible adversarial perturbation (IAP) attack via ultrasound delivery that can manipulate ASRs as a user speaks. The inherent differences between audible sounds and ultrasounds make IAP delivery face unprecedented challenges such as distortion, noise, and instability. In this regard, we design a novel ultrasonic transformation model to enhance the crafted perturbation to be physically effective and even survive long-distance delivery. We further enable VRIFLE's robustness by adopting a series of augmentation on user and real-world variations during the generation process. In this way, VRIFLE features an effective real-time manipulation of the ASR output from different distances and under any speech of users, with an alter-and-mute strategy that suppresses the impact of user disruption. Our extensive experiments in both digital and physical worlds verify VRIFLE's effectiveness under various configurations, robustness against six kinds of defenses, and universality in a targeted manner. We also show that VRIFLE can be delivered with a portable attack device and even everyday-life loudspeakers.},
	language = {en-US},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Li, Xinfeng and Yan, Chen and Lu, Xuancun and Zeng, Zihan and Ji, Xiaoyu and Xu, Wenyuan},
	month = sep,
	year = {2023},
	note = {GSCC: 0000006 
arXiv:2308.01040 [cs, eess]
remark: 利用超声波实现隐蔽实时对抗语音攻击。
TLDR: This paper proposes VRIFLE, an inaudible adversarial perturbation attack via ultrasound delivery that can manipulate ASRs as a user speaks and features an effective real-time manipulation of the ASR output from different distances and under any speech of users, with an alter-and-mute strategy that suppresses the impact of user disruption.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{liu_improved_2024,
	title = {Improved {Baselines} with {Visual} {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2310.03744},
	doi = {10.48550/arXiv.2310.03744},
	abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
	language = {en-US},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	month = may,
	year = {2024},
	note = {GSCC: 0001554 
arXiv:2310.03744
remark: LLAVA1.5},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_images_2024,
	title = {Images are {Achilles}' {Heel} of {Alignment}: {Exploiting} {Visual} {Vulnerabilities} for {Jailbreaking} {Multimodal} {Large} {Language} {Models}},
	shorttitle = {Images are {Achilles}' {Heel} of {Alignment}},
	url = {http://arxiv.org/abs/2403.09792},
	doi = {10.48550/arXiv.2403.09792},
	abstract = {In this paper, we study the harmlessness alignment problem of multimodal large language models (MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate (ASR) of 90.26\% for LLaVA-1.5 and 71.60\% for Gemini Pro Vision. Our code and data will be publicly released.},
	language = {en-US},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Li, Yifan and Guo, Hangyu and Zhou, Kun and Zhao, Wayne Xin and Wen, Ji-Rong},
	month = apr,
	year = {2024},
	note = {GSCC: 0000032 
arXiv:2403.09792
TLDR: A novel jailbreak method named HADES is proposed, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images.
remark: 图像可利用MMLLs对齐漏洞进行破解。},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {https://arxiv.org/abs/2305.05665v2},
	abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	month = may,
	year = {2023},
	note = {GSCC: 0000763 
remark: 提出跨六模态联合嵌入方法ImageBind。},
	keywords = {\# ImageBind},
}

@misc{shin_illusion_2017,
	title = {Illusion and {Dazzle}: {Adversarial} {Optical} {Channel} {Exploits} against {Lidars} for {Automotive} {Applications}},
	shorttitle = {Illusion and {Dazzle}},
	url = {https://eprint.iacr.org/2017/613},
	abstract = {With the advancement in computing, sensing, and vehicle electronics, autonomous vehicles are being realized. For autonomous driving, environment perception sensors such as radars, lidars, and vision sensors play core roles as the eyes of a vehicle; therefore, their reliability cannot be compromised. In this work, we present a spoofing by relaying attack, which can not only induce illusions in the lidar output but can also cause the illusions to appear closer than the location of a spoofing device. In a recent work, the former attack is shown to be effective, but the latter one was never shown. Additionally, we present a novel saturation attack against lidars, which can completely incapacitate a lidar from sensing a certain direction. The effectiveness of both the approaches is experimentally verified against Velodyne's VLP-16.This work is the same as the paper submitted at CHES 2017.},
	urldate = {2023-05-30},
	author = {Shin, Hocheol and Kim, Dohyun and Kwon, Yujin and Kim, Yongdae},
	year = {2017},
	note = {GSCC: 0000295 
Report Number: 613
remark: 激光雷达幻觉和饱和攻击方法研究。},
	keywords = {attack, autonomous car, lidar, saturating, sensor, spoofing},
}

@misc{song_how_2023,
	title = {How to {Bridge} the {Gap} between {Modalities}: {A} {Comprehensive} {Survey} on {Multimodal} {Large} {Language} {Model}},
	shorttitle = {How to {Bridge} the {Gap} between {Modalities}},
	url = {http://arxiv.org/abs/2311.07594},
	doi = {10.48550/arXiv.2311.07594},
	abstract = {This review paper explores Multimodal Large Language Models (MLLMs), which integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data such as text and vision. MLLMs demonstrate capabilities like generating image narratives and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society. Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement. This paper aims to explore modality alignment methods for LLMs and their existing capabilities. Implementing modality alignment allows LLMs to address environmental issues and enhance accessibility. The study surveys existing modal alignment methods in MLLMs into four groups: (1) Multimodal Converters that change data into something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs perceive different types of data; (3) Tools Assistance for changing data into one common format, usually text; and (4) Data-Driven methods that teach LLMs to understand specific types of data in a dataset. This field is still in a phase of exploration and experimentation, and we will organize and update various existing research methods for multimodal information alignment.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Song, Shezheng and Li, Xiaopeng and Li, Shasha and Zhao, Shan and Yu, Jie and Ma, Jun and Mao, Xiaoguang and Zhang, Weimin},
	month = dec,
	year = {2023},
	note = {GSCC: 0000024 
arXiv:2311.07594
TLDR: This paper aims to explore modality alignment methods for LLMs and their existing capabilities, and surveys existing modal alignment methods in MLLMs into four groups: Multimodal Converters that change data into something LLMs can understand, and Data-Driven methods that teach LLMs to understand specific types of data in a dataset.
remark: 多模态大模型技术及方法的综述。},
	keywords = {\# 对齐方法, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{ding_hilm-d_2023,
	title = {{HiLM}-{D}: {Towards} {High}-{Resolution} {Understanding} in {Multimodal} {Large} {Language} {Models} for {Autonomous} {Driving}},
	shorttitle = {{HiLM}-{D}},
	url = {http://arxiv.org/abs/2309.05186},
	doi = {10.48550/arXiv.2309.05186},
	abstract = {Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8\% in BLEU-4 for captioning and 17.2\% in mIoU for detection.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Ding, Xinpeng and Han, Jianhua and Xu, Hang and Zhang, Wei and Li, Xiaomeng},
	month = sep,
	year = {2023},
	note = {GSCC: 0000052 
arXiv:2309.05186 [cs]
remark: 高分辨率风险对象定位和意图建议预测
TLDR: This work proposes HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLs for the ROLISP task, and serves as a plug-and-play module, seamlessly fitting into current MLL Ms.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wen_hard_2023,
	title = {Hard {Prompts} {Made} {Easy}: {Gradient}-{Based} {Discrete} {Optimization} for {Prompt} {Tuning} and {Discovery}},
	shorttitle = {Hard {Prompts} {Made} {Easy}},
	url = {http://arxiv.org/abs/2302.03668},
	doi = {10.48550/arXiv.2302.03668},
	abstract = {The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = jun,
	year = {2023},
	note = {GSCC: 0000210 
arXiv:2302.03668 [cs]
remark: 提出基于梯度优化的硬提示词自动生成方法。},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{guan_hallusionbench_2024,
	title = {{HallusionBench}: {An} {Advanced} {Diagnostic} {Suite} for {Entangled} {Language} {Hallucination} and {Visual} {Illusion} in {Large} {Vision}-{Language} {Models}},
	shorttitle = {{HallusionBench}},
	url = {http://arxiv.org/abs/2310.14566},
	doi = {10.48550/arXiv.2310.14566},
	abstract = {We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42\% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16\%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.},
	language = {en-US},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
	month = mar,
	year = {2024},
	note = {GSCC: 0000120 
arXiv:2310.14566 [cs]
remark: HallusionBench评估视觉语言模型的图像语境推理能力。},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_grounding_2023,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {http://arxiv.org/abs/2303.05499},
	doi = {10.48550/arXiv.2303.05499},
	abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = mar,
	year = {2023},
	note = {GSCC: 0001445 
arXiv:2303.05499 [cs]
remark: 将DINO与语言预训练结合，用于开放集目标检测。
TLDR: An open-set object detector, called Grounding DINO, is presented by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions, and performs remarkably well on all three settings.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vong_grounded_2024,
	title = {Grounded language acquisition through the eyes and ears of a single child},
	volume = {383},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adi1374},
	doi = {10.1126/science.adi1374},
	abstract = {Starting around 6 to 9 months of age, children begin acquiring their first words, linking spoken words to their visual counterparts. How much of this knowledge is learnable from sensory input with relatively generic learning mechanisms, and how much requires stronger inductive biases? Using longitudinal head-mounted camera recordings from one child aged 6 to 25 months, we trained a relatively generic neural network on 61 hours of correlated visual-linguistic data streams, learning feature-based representations and cross-modal associations. Our model acquires many word-referent mappings present in the child’s everyday experience, enables zero-shot generalization to new visual referents, and aligns its visual and linguistic conceptual systems. These results show how critical aspects of grounded word meaning are learnable through joint representation and associative learning from one child’s input.
          , 
            Editor’s summary
            
              How do young children learn to associate new words with specific objects or visually represented concepts? This hotly debated question in early language acquisition has been traditionally examined in laboratories, limiting generalizability to real-world settings. Vong
              et al
              . investigated the question in an unprecedented, longitudinal manner using head-mounted video recordings from a single child’s first-person experiences in naturalistic settings. By applying machine learning, they introduced the Child’s View for Contrastive Learning (CVCL) model, pairing video frames that co-occurred with uttered words, and embedded the images and words in shared representational spaces. CVCL represents sets of visually similar things from one concept (e.g., puzzles) through distinct subclusters (animal versus alphabet puzzles). It combines associative and representation learning that fills gaps in language acquisition research and theories. —Ekeoma Uzogara
            
          , 
            Machine learning advances research into early language acquisition in children.},
	language = {en},
	number = {6682},
	urldate = {2024-11-13},
	journal = {Science},
	author = {Vong, Wai Keen and Wang, Wentao and Orhan, A. Emin and Lake, Brenden M.},
	month = feb,
	year = {2024},
	note = {GSCC: 0000054 
remark: 儿童通过感知输入习得语言关联。},
	pages = {504--511},
}

@misc{nascimento_gpt---loop_2023,
	title = {{GPT}-in-the-{Loop}: {Adaptive} {Decision}-{Making} for {Multiagent} {Systems}},
	shorttitle = {{GPT}-in-the-{Loop}},
	url = {https://arxiv.org/abs/2308.10435v1},
	abstract = {This paper introduces the "GPT-in-the-loop" approach, a novel method combining the advanced reasoning capabilities of Large Language Models (LLMs) like Generative Pre-trained Transformers (GPT) with multiagent (MAS) systems. Venturing beyond traditional adaptive approaches that generally require long training processes, our framework employs GPT-4 for enhanced problem-solving and explanation skills. Our experimental backdrop is the smart streetlight Internet of Things (IoT) application. Here, agents use sensors, actuators, and neural networks to create an energy-efficient lighting system. By integrating GPT-4, these agents achieve superior decision-making and adaptability without the need for extensive training. We compare this approach with both traditional neuroevolutionary methods and solutions provided by software engineers, underlining the potential of GPT-driven multiagent systems in IoT. Structurally, the paper outlines the incorporation of GPT into the agent-driven Framework for the Internet of Things (FIoT), introduces our proposed GPT-in-the-loop approach, presents comparative results in the IoT context, and concludes with insights and future directions.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
	month = aug,
	year = {2023},
	note = {GSCC: 0000011 
remark: GPT提升多智能体系统决策能力。},
}

@misc{mao_gpt-driver_2023,
	title = {{GPT}-{Driver}: {Learning} to {Drive} with {GPT}},
	shorttitle = {{GPT}-{Driver}},
	url = {http://arxiv.org/abs/2310.01415},
	doi = {10.48550/arXiv.2310.01415},
	abstract = {We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coordinate positions. Furthermore, we propose a novel prompting-reasoning-finetuning strategy to stimulate the numerical reasoning potential of the LLM. With this strategy, the LLM can describe highly precise trajectory coordinates and also its internal decision-making process in natural language. We evaluate our approach on the large-scale nuScenes dataset, and extensive experiments substantiate the effectiveness, generalization ability, and interpretability of our GPT-based motion planner. Code is now available at https://github.com/PointsCoder/GPT-Driver.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Mao, Jiageng and Qian, Yuxi and Ye, Junjie and Zhao, Hang and Wang, Yue},
	month = dec,
	year = {2023},
	note = {GSCC: 0000176 
arXiv:2310.01415 [cs]
remark: GPT模型转化为自动驾驶运动规划器。
TLDR: A novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs) and proposes a novel prompting-reasoning-finetuning strategy to stimulate the numerical reasoning potential of the LLM.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	language = {en-US},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {GSCC: 0006720 
arXiv:2303.08774 [cs]
remark: GPT-4},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{jiang_glitchhiker_2023,
	title = {\{{GlitchHiker}\}: {Uncovering} {Vulnerabilities} of {Image} {Signal} {Transmission} with \{{IEMI}\}},
	isbn = {978-1-939133-37-3},
	shorttitle = {\{{GlitchHiker}\}},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/jiang-qinhong},
	language = {en},
	urldate = {2023-11-08},
	author = {Jiang, Qinhong and Ji, Xiaoyu and Yan, Chen and Xie, Zhixin and Lou, Haina and Xu, Wenyuan},
	year = {2023},
	note = {GSCC: 0000023 
remark: GlitchHiker攻击可以使用故意电磁干扰主动诱导不同位置、宽度和数量的摄像机的受控故障图像。},
	pages = {7249--7266},
}

@misc{king_get_2023,
	title = {"{Get} ready for a party": {Exploring} smarter smart spaces with help from large language models},
	shorttitle = {"{Get} ready for a party"},
	url = {https://arxiv.org/abs/2303.14143v1},
	abstract = {The right response to someone who says "get ready for a party" is deeply influenced by meaning and context. For a smart home assistant (e.g., Google Home), the ideal response might be to survey the available devices in the home and change their state to create a festive atmosphere. Current practical systems cannot service such requests since they require the ability to (1) infer meaning behind an abstract statement and (2) map that inference to a concrete course of action appropriate for the context (e.g., changing the settings of specific devices). In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions. We first explore the feasibility of a system that places an LLM at the center of command inference and action planning, showing that LLMs have the capacity to infer intent behind vague, context-dependent commands like "get ready for a party" and respond with concrete, machine-parseable instructions that can be used to control smart devices. We furthermore demonstrate a proof-of-concept implementation that puts an LLM in control of real devices, showing its ability to infer intent and change device state appropriately with no fine-tuning or task-specific training. Our work hints at the promise of LLM-driven systems for context-awareness in smart environments, motivating future research in this area.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {King, Evan and Yu, Haoxiang and Lee, Sangsu and Julien, Christine},
	month = mar,
	year = {2023},
	note = {GSCC: 0000014 
remark: LLM赋能智能家居理解与响应。},
}

@misc{gemini_team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	language = {en-US},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and von Dincklage, Daniel and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and Girgin, Ser tan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M., Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and van de Kerkhof, Jan and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
	month = jun,
	year = {2024},
	note = {GSCC: 0002226 
arXiv:2312.11805 [cs]
remark: Gemini},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	doi = {10.48550/arXiv.2403.05530},
	abstract = {In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textgreater}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2024-12-25},
	publisher = {arXiv},
	author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and Mariooryad, Soroosh and Ding, Yifan and Geng, Xinyang and Alcober, Fred and Frostig, Roy and Omernick, Mark and Walker, Lexi and Paduraru, Cosmin and Sorokin, Christina and Tacchetti, Andrea and Gaffney, Colin and Daruki, Samira and Sercinoglu, Olcan and Gleicher, Zach and Love, Juliette and Voigtlaender, Paul and Jain, Rohan and Surita, Gabriela and Mohamed, Kareem and Blevins, Rory and Ahn, Junwhan and Zhu, Tao and Kawintiranon, Kornraphop and Firat, Orhan and Gu, Yiming and Zhang, Yujing and Rahtz, Matthew and Faruqui, Manaal and Clay, Natalie and Gilmer, Justin and Co-Reyes, J. D. and Penchev, Ivo and Zhu, Rui and Morioka, Nobuyuki and Hui, Kevin and Haridasan, Krishna and Campos, Victor and Mahdieh, Mahdis and Guo, Mandy and Hassan, Samer and Kilgour, Kevin and Vezer, Arpi and Cheng, Heng-Tze and Liedekerke, Raoul de and Goyal, Siddharth and Barham, Paul and Strouse, D. J. and Noury, Seb and Adler, Jonas and Sundararajan, Mukund and Vikram, Sharad and Lepikhin, Dmitry and Paganini, Michela and Garcia, Xavier and Yang, Fan and Valter, Dasha and Trebacz, Maja and Vodrahalli, Kiran and Asawaroengchai, Chulayuth and Ring, Roman and Kalb, Norbert and Soares, Livio Baldini and Brahma, Siddhartha and Steiner, David and Yu, Tianhe and Mentzer, Fabian and He, Antoine and Gonzalez, Lucas and Xu, Bibo and Kaufman, Raphael Lopez and Shafey, Laurent El and Oh, Junhyuk and Hennigan, Tom and Driessche, George van den and Odoom, Seth and Lucic, Mario and Roelofs, Becca and Lall, Sid and Marathe, Amit and Chan, Betty and Ontanon, Santiago and He, Luheng and Teplyashin, Denis and Lai, Jonathan and Crone, Phil and Damoc, Bogdan and Ho, Lewis and Riedel, Sebastian and Lenc, Karel and Yeh, Chih-Kuan and Chowdhery, Aakanksha and Xu, Yang and Kazemi, Mehran and Amid, Ehsan and Petrushkina, Anastasia and Swersky, Kevin and Khodaei, Ali and Chen, Gowoon and Larkin, Chris and Pinto, Mario and Yan, Geng and Badia, Adria Puigdomenech and Patil, Piyush and Hansen, Steven and Orr, Dave and Arnold, Sebastien M. R. and Grimstad, Jordan and Dai, Andrew and Douglas, Sholto and Sinha, Rishika and Yadav, Vikas and Chen, Xi and Gribovskaya, Elena and Austin, Jacob and Zhao, Jeffrey and Patel, Kaushal and Komarek, Paul and Austin, Sophia and Borgeaud, Sebastian and Friso, Linda and Goyal, Abhimanyu and Caine, Ben and Cao, Kris and Chung, Da-Woon and Lamm, Matthew and Barth-Maron, Gabe and Kagohara, Thais and Olszewska, Kate and Chen, Mia and Shivakumar, Kaushik and Agarwal, Rishabh and Godhia, Harshal and Rajwar, Ravi and Snaider, Javier and Dotiwalla, Xerxes and Liu, Yuan and Barua, Aditya and Ungureanu, Victor and Zhang, Yuan and Batsaikhan, Bat-Orgil and Wirth, Mateo and Qin, James and Danihelka, Ivo and Doshi, Tulsee and Chadwick, Martin and Chen, Jilin and Jain, Sanil and Le, Quoc and Kar, Arjun and Gurumurthy, Madhu and Li, Cheng and Sang, Ruoxin and Liu, Fangyu and Lamprou, Lampros and Munoz, Rich and Lintz, Nathan and Mehta, Harsh and Howard, Heidi and Reynolds, Malcolm and Aroyo, Lora and Wang, Quan and Blanco, Lorenzo and Cassirer, Albin and Griffith, Jordan and Das, Dipanjan and Lee, Stephan and Sygnowski, Jakub and Fisher, Zach and Besley, James and Powell, Richard and Ahmed, Zafarali and Paulus, Dominik and Reitter, David and Borsos, Zalan and Joshi, Rishabh and Pope, Aedan and Hand, Steven and Selo, Vittorio and Jain, Vihan and Sethi, Nikhil and Goel, Megha and Makino, Takaki and May, Rhys and Yang, Zhen and Schalkwyk, Johan and Butterfield, Christina and Hauth, Anja and Goldin, Alex and Hawkins, Will and Senter, Evan and Brin, Sergey and Woodman, Oliver and Ritter, Marvin and Noland, Eric and Giang, Minh and Bolina, Vijay and Lee, Lisa and Blyth, Tim and Mackinnon, Ian and Reid, Machel and Sarvana, Obaid and Silver, David and Chen, Alexander and Wang, Lily and Maggiore, Loren and Chang, Oscar and Attaluri, Nithya and Thornton, Gregory and Chiu, Chung-Cheng and Bunyan, Oskar and Levine, Nir and Chung, Timothy and Eltyshev, Evgenii and Si, Xiance and Lillicrap, Timothy and Brady, Demetra and Aggarwal, Vaibhav and Wu, Boxi and Xu, Yuanzhong and McIlroy, Ross and Badola, Kartikeya and Sandhu, Paramjit and Moreira, Erica and Stokowiec, Wojciech and Hemsley, Ross and Li, Dong and Tudor, Alex and Shyam, Pranav and Rahimtoroghi, Elahe and Haykal, Salem and Sprechmann, Pablo and Zhou, Xiang and Mincu, Diana and Li, Yujia and Addanki, Ravi and Krishna, Kalpesh and Wu, Xiao and Frechette, Alexandre and Eyal, Matan and Dafoe, Allan and Lacey, Dave and Whang, Jay and Avrahami, Thi and Zhang, Ye and Taropa, Emanuel and Lin, Hanzhao and Toyama, Daniel and Rutherford, Eliza and Sano, Motoki and Choe, HyunJeong and Tomala, Alex and Safranek-Shrader, Chalence and Kassner, Nora and Pajarskas, Mantas and Harvey, Matt and Sechrist, Sean and Fortunato, Meire and Lyu, Christina and Elsayed, Gamaleldin and Kuang, Chenkai and Lottes, James and Chu, Eric and Jia, Chao and Chen, Chih-Wei and Humphreys, Peter and Baumli, Kate and Tao, Connie and Samuel, Rajkumar and Santos, Cicero Nogueira dos and Andreassen, Anders and Rakićević, Nemanja and Grewe, Dominik and Kumar, Aviral and Winkler, Stephanie and Caton, Jonathan and Brock, Andrew and Dalmia, Sid and Sheahan, Hannah and Barr, Iain and Miao, Yingjie and Natsev, Paul and Devlin, Jacob and Behbahani, Feryal and Prost, Flavien and Sun, Yanhua and Myaskovsky, Artiom and Pillai, Thanumalayan Sankaranarayana and Hurt, Dan and Lazaridou, Angeliki and Xiong, Xi and Zheng, Ce and Pardo, Fabio and Li, Xiaowei and Horgan, Dan and Stanton, Joe and Ambar, Moran and Xia, Fei and Lince, Alejandro and Wang, Mingqiu and Mustafa, Basil and Webson, Albert and Lee, Hyo and Anil, Rohan and Wicke, Martin and Dozat, Timothy and Sinha, Abhishek and Piqueras, Enrique and Dabir, Elahe and Upadhyay, Shyam and Boral, Anudhyan and Hendricks, Lisa Anne and Fry, Corey and Djolonga, Josip and Su, Yi and Walker, Jake and Labanowski, Jane and Huang, Ronny and Misra, Vedant and Chen, Jeremy and Skerry-Ryan, R. J. and Singh, Avi and Rijhwani, Shruti and Yu, Dian and Castro-Ros, Alex and Changpinyo, Beer and Datta, Romina and Bagri, Sumit and Hrafnkelsson, Arnar Mar and Maggioni, Marcello and Zheng, Daniel and Sulsky, Yury and Hou, Shaobo and Paine, Tom Le and Yang, Antoine and Riesa, Jason and Rogozinska, Dominika and Marcus, Dror and Badawy, Dalia El and Zhang, Qiao and Wang, Luyu and Miller, Helen and Greer, Jeremy and Sjos, Lars Lowe and Nova, Azade and Zen, Heiga and Chaabouni, Rahma and Rosca, Mihaela and Jiang, Jiepu and Chen, Charlie and Liu, Ruibo and Sainath, Tara and Krikun, Maxim and Polozov, Alex and Lespiau, Jean-Baptiste and Newlan, Josh and Cankara, Zeyncep and Kwak, Soo and Xu, Yunhan and Chen, Phil and Coenen, Andy and Meyer, Clemens and Tsihlas, Katerina and Ma, Ada and Gottweis, Juraj and Xing, Jinwei and Gu, Chenjie and Miao, Jin and Frank, Christian and Cankara, Zeynep and Ganapathy, Sanjay and Dasgupta, Ishita and Hughes-Fitt, Steph and Chen, Heng and Reid, David and Rong, Keran and Fan, Hongmin and Amersfoort, Joost van and Zhuang, Vincent and Cohen, Aaron and Gu, Shixiang Shane and Mohananey, Anhad and Ilic, Anastasija and Tobin, Taylor and Wieting, John and Bortsova, Anna and Thacker, Phoebe and Wang, Emma and Caveness, Emily and Chiu, Justin and Sezener, Eren and Kaskasoli, Alex and Baker, Steven and Millican, Katie and Elhawaty, Mohamed and Aisopos, Kostas and Lebsack, Carl and Byrd, Nathan and Dai, Hanjun and Jia, Wenhao and Wiethoff, Matthew and Davoodi, Elnaz and Weston, Albert and Yagati, Lakshman and Ahuja, Arun and Gao, Isabel and Pundak, Golan and Zhang, Susan and Azzam, Michael and Sim, Khe Chai and Caelles, Sergi and Keeling, James and Sharma, Abhanshu and Swing, Andy and Li, YaGuang and Liu, Chenxi and Bostock, Carrie Grimes and Bansal, Yamini and Nado, Zachary and Anand, Ankesh and Lipschultz, Josh and Karmarkar, Abhijit and Proleev, Lev and Ittycheriah, Abe and Yeganeh, Soheil Hassas and Polovets, George and Faust, Aleksandra and Sun, Jiao and Rrustemi, Alban and Li, Pen and Shivanna, Rakesh and Liu, Jeremiah and Welty, Chris and Lebron, Federico and Baddepudi, Anirudh and Krause, Sebastian and Parisotto, Emilio and Soricut, Radu and Xu, Zheng and Bloxwich, Dawn and Johnson, Melvin and Neyshabur, Behnam and Mao-Jones, Justin and Wang, Renshen and Ramasesh, Vinay and Abbas, Zaheer and Guez, Arthur and Segal, Constant and Nguyen, Duc Dung and Svensson, James and Hou, Le and York, Sarah and Milan, Kieran and Bridgers, Sophie and Gworek, Wiktor and Tagliasacchi, Marco and Lee-Thorp, James and Chang, Michael and Guseynov, Alexey and Hartman, Ale Jakse and Kwong, Michael and Zhao, Ruizhe and Kashem, Sheleem and Cole, Elizabeth and Miech, Antoine and Tanburn, Richard and Phuong, Mary and Pavetic, Filip and Cevey, Sebastien and Comanescu, Ramona and Ives, Richard and Yang, Sherry and Du, Cosmo and Li, Bo and Zhang, Zizhao and Iinuma, Mariko and Hu, Clara Huiyi and Roy, Aurko and Bijwadia, Shaan and Zhu, Zhenkai and Martins, Danilo and Saputro, Rachel and Gergely, Anita and Zheng, Steven and Jia, Dawei and Antonoglou, Ioannis and Sadovsky, Adam and Gu, Shane and Bi, Yingying and Andreev, Alek and Samangooei, Sina and Khan, Mina and Kocisky, Tomas and Filos, Angelos and Kumar, Chintu and Bishop, Colton and Yu, Adams and Hodkinson, Sarah and Mittal, Sid and Shah, Premal and Moufarek, Alexandre and Cheng, Yong and Bloniarz, Adam and Lee, Jaehoon and Pejman, Pedram and Michel, Paul and Spencer, Stephen and Feinberg, Vladimir and Xiong, Xuehan and Savinov, Nikolay and Smith, Charlotte and Shakeri, Siamak and Tran, Dustin and Chesus, Mary and Bohnet, Bernd and Tucker, George and Glehn, Tamara von and Muir, Carrie and Mao, Yiran and Kazawa, Hideto and Slone, Ambrose and Soparkar, Kedar and Shrivastava, Disha and Cobon-Kerr, James and Sharman, Michael and Pavagadhi, Jay and Araya, Carlos and Misiunas, Karolis and Ghelani, Nimesh and Laskin, Michael and Barker, David and Li, Qiujia and Briukhov, Anton and Houlsby, Neil and Glaese, Mia and Lakshminarayanan, Balaji and Schucher, Nathan and Tang, Yunhao and Collins, Eli and Lim, Hyeontaek and Feng, Fangxiaoyu and Recasens, Adria and Lai, Guangda and Magni, Alberto and Cao, Nicola De and Siddhant, Aditya and Ashwood, Zoe and Orbay, Jordi and Dehghani, Mostafa and Brennan, Jenny and He, Yifan and Xu, Kelvin and Gao, Yang and Saroufim, Carl and Molloy, James and Wu, Xinyi and Arnold, Seb and Chang, Solomon and Schrittwieser, Julian and Buchatskaya, Elena and Radpour, Soroush and Polacek, Martin and Giordano, Skye and Bapna, Ankur and Tokumine, Simon and Hellendoorn, Vincent and Sottiaux, Thibault and Cogan, Sarah and Severyn, Aliaksei and Saleh, Mohammad and Thakoor, Shantanu and Shefey, Laurent and Qiao, Siyuan and Gaba, Meenu and Chang, Shuo-yiin and Swanson, Craig and Zhang, Biao and Lee, Benjamin and Rubenstein, Paul Kishan and Song, Gan and Kwiatkowski, Tom and Koop, Anna and Kannan, Ajay and Kao, David and Schuh, Parker and Stjerngren, Axel and Ghiasi, Golnaz and Gibson, Gena and Vilnis, Luke and Yuan, Ye and Ferreira, Felipe Tiengo and Kamath, Aishwarya and Klimenko, Ted and Franko, Ken and Xiao, Kefan and Bhattacharya, Indro and Patel, Miteyan and Wang, Rui and Morris, Alex and Strudel, Robin and Sharma, Vivek and Choy, Peter and Hashemi, Sayed Hadi and Landon, Jessica and Finkelstein, Mara and Jhakra, Priya and Frye, Justin and Barnes, Megan and Mauger, Matthew and Daun, Dennis and Baatarsukh, Khuslen and Tung, Matthew and Farhan, Wael and Michalewski, Henryk and Viola, Fabio and Quitry, Felix de Chaumont and Lan, Charline Le and Hudson, Tom and Wang, Qingze and Fischer, Felix and Zheng, Ivy and White, Elspeth and Dragan, Anca and Alayrac, Jean-baptiste and Ni, Eric and Pritzel, Alexander and Iwanicki, Adam and Isard, Michael and Bulanova, Anna and Zilka, Lukas and Dyer, Ethan and Sachan, Devendra and Srinivasan, Srivatsan and Muckenhirn, Hannah and Cai, Honglong and Mandhane, Amol and Tariq, Mukarram and Rae, Jack W. and Wang, Gary and Ayoub, Kareem and FitzGerald, Nicholas and Zhao, Yao and Han, Woohyun and Alberti, Chris and Garrette, Dan and Krishnakumar, Kashyap and Gimenez, Mai and Levskaya, Anselm and Sohn, Daniel and Matak, Josip and Iturrate, Inaki and Chang, Michael B. and Xiang, Jackie and Cao, Yuan and Ranka, Nishant and Brown, Geoff and Hutter, Adrian and Mirrokni, Vahab and Chen, Nanxin and Yao, Kaisheng and Egyed, Zoltan and Galilee, Francois and Liechty, Tyler and Kallakuri, Praveen and Palmer, Evan and Ghemawat, Sanjay and Liu, Jasmine and Tao, David and Thornton, Chloe and Green, Tim and Jasarevic, Mimi and Lin, Sharon and Cotruta, Victor and Tan, Yi-Xuan and Fiedel, Noah and Yu, Hongkun and Chi, Ed and Neitz, Alexander and Heitkaemper, Jens and Sinha, Anu and Zhou, Denny and Sun, Yi and Kaed, Charbel and Hulse, Brice and Mishra, Swaroop and Georgaki, Maria and Kudugunta, Sneha and Farabet, Clement and Shafran, Izhak and Vlasic, Daniel and Tsitsulin, Anton and Ananthanarayanan, Rajagopal and Carin, Alen and Su, Guolong and Sun, Pei and V, Shashank and Carvajal, Gabriel and Broder, Josef and Comsa, Iulia and Repina, Alena and Wong, William and Chen, Warren Weilun and Hawkins, Peter and Filonov, Egor and Loher, Lucia and Hirnschall, Christoph and Wang, Weiyi and Ye, Jingchen and Burns, Andrea and Cate, Hardie and Wright, Diana Gage and Piccinini, Federico and Zhang, Lei and Lin, Chu-Cheng and Gog, Ionel and Kulizhskaya, Yana and Sreevatsa, Ashwin and Song, Shuang and Cobo, Luis C. and Iyer, Anand and Tekur, Chetan and Garrido, Guillermo and Xiao, Zhuyun and Kemp, Rupert and Zheng, Huaixiu Steven and Li, Hui and Agarwal, Ananth and Ngani, Christel and Goshvadi, Kati and Santamaria-Fernandez, Rebeca and Fica, Wojciech and Chen, Xinyun and Gorgolewski, Chris and Sun, Sean and Garg, Roopal and Ye, Xinyu and Eslami, S. M. Ali and Hua, Nan and Simon, Jon and Joshi, Pratik and Kim, Yelin and Tenney, Ian and Potluri, Sahitya and Thiet, Lam Nguyen and Yuan, Quan and Luisier, Florian and Chronopoulou, Alexandra and Scellato, Salvatore and Srinivasan, Praveen and Chen, Minmin and Koverkathu, Vinod and Dalibard, Valentin and Xu, Yaming and Saeta, Brennan and Anderson, Keith and Sellam, Thibault and Fernando, Nick and Huot, Fantine and Jung, Junehyuk and Varadarajan, Mani and Quinn, Michael and Raul, Amit and Le, Maigo and Habalov, Ruslan and Clark, Jon and Jalan, Komal and Bullard, Kalesha and Singhal, Achintya and Luong, Thang and Wang, Boyu and Rajayogam, Sujeevan and Eisenschlos, Julian and Jia, Johnson and Finchelstein, Daniel and Yakubovich, Alex and Balle, Daniel and Fink, Michael and Agarwal, Sameer and Li, Jing and Dvijotham, Dj and Pal, Shalini and Kang, Kai and Konzelmann, Jaclyn and Beattie, Jennifer and Dousse, Olivier and Wu, Diane and Crocker, Remi and Elkind, Chen and Jonnalagadda, Siddhartha Reddy and Lee, Jong and Holtmann-Rice, Dan and Kallarackal, Krystal and Liu, Rosanne and Vnukov, Denis and Vats, Neera and Invernizzi, Luca and Jafari, Mohsen and Zhou, Huanjie and Taylor, Lilly and Prendki, Jennifer and Wu, Marcus and Eccles, Tom and Liu, Tianqi and Kopparapu, Kavya and Beaufays, Francoise and Angermueller, Christof and Marzoca, Andreea and Sarcar, Shourya and Dib, Hilal and Stanway, Jeff and Perbet, Frank and Trdin, Nejc and Sterneck, Rachel and Khorlin, Andrey and Li, Dinghua and Wu, Xihui and Goenka, Sonam and Madras, David and Goldshtein, Sasha and Gierke, Willi and Zhou, Tong and Liu, Yaxin and Liang, Yannie and White, Anais and Li, Yunjie and Singh, Shreya and Bahargam, Sanaz and Epstein, Mark and Basu, Sujoy and Lao, Li and Ozturel, Adnan and Crous, Carl and Zhai, Alex and Lu, Han and Tung, Zora and Gaur, Neeraj and Walton, Alanna and Dixon, Lucas and Zhang, Ming and Globerson, Amir and Uy, Grant and Bolt, Andrew and Wiles, Olivia and Nasr, Milad and Shumailov, Ilia and Selvi, Marco and Piccinno, Francesco and Aguilar, Ricardo and McCarthy, Sara and Khalman, Misha and Shukla, Mrinal and Galic, Vlado and Carpenter, John and Villela, Kevin and Zhang, Haibin and Richardson, Harry and Martens, James and Bosnjak, Matko and Belle, Shreyas Rammohan and Seibert, Jeff and Alnahlawi, Mahmoud and McWilliams, Brian and Singh, Sankalp and Louis, Annie and Ding, Wen and Popovici, Dan and Simicich, Lenin and Knight, Laura and Mehta, Pulkit and Gupta, Nishesh and Shi, Chongyang and Fatehi, Saaber and Mitrovic, Jovana and Grills, Alex and Pagadora, Joseph and Munkhdalai, Tsendsuren and Petrova, Dessie and Eisenbud, Danielle and Zhang, Zhishuai and Yates, Damion and Mittal, Bhavishya and Tripuraneni, Nilesh and Assael, Yannis and Brovelli, Thomas and Jain, Prateek and Velimirovic, Mihajlo and Akbulut, Canfer and Mu, Jiaqi and Macherey, Wolfgang and Kumar, Ravin and Xu, Jun and Qureshi, Haroon and Comanici, Gheorghe and Wiesner, Jeremy and Gong, Zhitao and Ruddock, Anton and Bauer, Matthias and Felt, Nick and GP, Anirudh and Arnab, Anurag and Zelle, Dustin and Rothfuss, Jonas and Rosgen, Bill and Shenoy, Ashish and Seybold, Bryan and Li, Xinjian and Mudigonda, Jayaram and Erdogan, Goker and Xia, Jiawei and Simsa, Jiri and Michi, Andrea and Yao, Yi and Yew, Christopher and Kan, Steven and Caswell, Isaac and Radebaugh, Carey and Elisseeff, Andre and Valenzuela, Pedro and McKinney, Kay and Paterson, Kim and Cui, Albert and Latorre-Chimoto, Eri and Kim, Solomon and Zeng, William and Durden, Ken and Ponnapalli, Priya and Sosea, Tiberiu and Choquette-Choo, Christopher A. and Manyika, James and Robenek, Brona and Vashisht, Harsha and Pereira, Sebastien and Lam, Hoi and Velic, Marko and Owusu-Afriyie, Denese and Lee, Katherine and Bolukbasi, Tolga and Parrish, Alicia and Lu, Shawn and Park, Jane and Venkatraman, Balaji and Talbert, Alice and Rosique, Lambert and Cheng, Yuchung and Sozanschi, Andrei and Paszke, Adam and Kumar, Praveen and Austin, Jessica and Li, Lu and Salama, Khalid and Perz, Bartek and Kim, Wooyeol and Dukkipati, Nandita and Baryshnikov, Anthony and Kaplanis, Christos and Sheng, XiangHai and Chervonyi, Yuri and Unlu, Caglar and Casas, Diego de Las and Askham, Harry and Tunyasuvunakool, Kathryn and Gimeno, Felix and Poder, Siim and Kwak, Chester and Miecnikowski, Matt and Mirrokni, Vahab and Dimitriev, Alek and Parisi, Aaron and Liu, Dangyi and Tsai, Tomy and Shevlane, Toby and Kouridi, Christina and Garmon, Drew and Goedeckemeyer, Adrian and Brown, Adam R. and Vijayakumar, Anitha and Elqursh, Ali and Jazayeri, Sadegh and Huang, Jin and Carthy, Sara Mc and Hoover, Jay and Kim, Lucy and Kumar, Sandeep and Chen, Wei and Biles, Courtney and Bingham, Garrett and Rosen, Evan and Wang, Lisa and Tan, Qijun and Engel, David and Pongetti, Francesco and Cesare, Dario de and Hwang, Dongseong and Yu, Lily and Pullman, Jennifer and Narayanan, Srini and Levin, Kyle and Gopal, Siddharth and Li, Megan and Aharoni, Asaf and Trinh, Trieu and Lo, Jessica and Casagrande, Norman and Vij, Roopali and Matthey, Loic and Ramadhana, Bramandia and Matthews, Austin and Carey, C. J. and Johnson, Matthew and Goranova, Kremena and Shah, Rohin and Ashraf, Shereen and Dasgupta, Kingshuk and Larsen, Rasmus and Wang, Yicheng and Vuyyuru, Manish Reddy and Jiang, Chong and Ijazi, Joana and Osawa, Kazuki and Smith, Celine and Boppana, Ramya Sree and Bilal, Taylan and Koizumi, Yuma and Xu, Ying and Altun, Yasemin and Shabat, Nir and Bariach, Ben and Korchemniy, Alex and Choo, Kiam and Ronneberger, Olaf and Iwuanyanwu, Chimezie and Zhao, Shubin and Soergel, David and Hsieh, Cho-Jui and Cai, Irene and Iqbal, Shariq and Sundermeyer, Martin and Chen, Zhe and Bursztein, Elie and Malaviya, Chaitanya and Biadsy, Fadi and Shroff, Prakash and Dhillon, Inderjit and Latkar, Tejasi and Dyer, Chris and Forbes, Hannah and Nicosia, Massimo and Nikolaev, Vitaly and Greene, Somer and Georgiev, Marin and Wang, Pidong and Martin, Nina and Sedghi, Hanie and Zhang, John and Banzal, Praseem and Fritz, Doug and Rao, Vikram and Wang, Xuezhi and Zhang, Jiageng and Patraucean, Viorica and Du, Dayou and Mordatch, Igor and Jurin, Ivan and Liu, Lewis and Dubey, Ayush and Mohan, Abhi and Nowakowski, Janek and Ion, Vlad-Doru and Wei, Nan and Tojo, Reiko and Raad, Maria Abi and Hudson, Drew A. and Keshava, Vaishakh and Agrawal, Shubham and Ramirez, Kevin and Wu, Zhichun and Nguyen, Hoang and Liu, Ji and Sewak, Madhavi and Petrini, Bryce and Choi, DongHyun and Philips, Ivan and Wang, Ziyue and Bica, Ioana and Garg, Ankush and Wilkiewicz, Jarek and Agrawal, Priyanka and Li, Xiaowei and Guo, Danhao and Xue, Emily and Shaik, Naseer and Leach, Andrew and Khan, Sadh MNM and Wiesinger, Julia and Jerome, Sammy and Chakladar, Abhishek and Wang, Alek Wenjiao and Ornduff, Tina and Abu, Folake and Ghaffarkhah, Alireza and Wainwright, Marcus and Cortes, Mario and Liu, Frederick and Maynez, Joshua and Terzis, Andreas and Samangouei, Pouya and Mansour, Riham and Kępa, Tomasz and Aubet, François-Xavier and Algymr, Anton and Banica, Dan and Weisz, Agoston and Orban, Andras and Senges, Alexandre and Andrejczuk, Ewa and Geller, Mark and Santo, Niccolo Dal and Anklin, Valentin and Merey, Majd Al and Baeuml, Martin and Strohman, Trevor and Bai, Junwen and Petrov, Slav and Wu, Yonghui and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeff and Vinyals, Oriol},
	month = dec,
	year = {2024},
	note = {GSCC: 0000769 
arXiv:2403.05530 [cs]
TLDR: Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.
remark: Gemini 1.5},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{qi_frustum_2018,
	title = {Frustum {PointNets} for {3D} {Object} {Detection} from {RGB}-{D} {Data}},
	url = {http://arxiv.org/abs/1711.08488},
	doi = {10.48550/arXiv.1711.08488},
	abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
	language = {en-US},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
	month = apr,
	year = {2018},
	note = {GSCC: 0002962 
arXiv:1711.08488 [cs]
Issue: arXiv:1711.08488
remark: 提出结合2D检测和3D学习的RGB-D目标检测方法。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{firoozi_foundation_2023,
	title = {Foundation {Models} in {Robotics}: {Applications}, {Challenges}, and the {Future}},
	shorttitle = {Foundation {Models} in {Robotics}},
	url = {https://arxiv.org/abs/2312.07843v1},
	abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models},
	language = {en},
	urldate = {2024-01-11},
	journal = {arXiv.org},
	author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
	month = dec,
	year = {2023},
	note = {GSCC: 0000108 
remark: 基础模型在机器人学中的应用及挑战。},
}

@misc{gong_figstep_2023,
	title = {{FigStep}: {Jailbreaking} {Large} {Vision}-language {Models} via {Typographic} {Visual} {Prompts}},
	shorttitle = {{FigStep}},
	url = {https://arxiv.org/abs/2311.05608v1},
	abstract = {Large vision-language models (VLMs) like GPT-4V represent an unprecedented revolution in the field of artificial intelligence (AI). Compared to single-modal large language models (LLMs), VLMs possess more versatile capabilities by incorporating additional modalities (e.g., images). Meanwhile, there's a rising enthusiasm in the AI community to develop open-source VLMs, such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety assessment. In this paper, to demonstrate that more modalities lead to unforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework against VLMs. FigStep feeds harmful instructions into VLMs through the image channel and then uses benign text prompts to induce VLMs to output contents that violate common AI safety policies. Our experimental results show that FigStep can achieve an average attack success rate of 94.8\% across 2 families of popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover, we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which already leverages several system-level mechanisms to filter harmful queries. Above all, our experimental results reveal that VLMs are vulnerable to jailbreaking attacks, which highlights the necessity of novel safety alignments between visual and textual modalities.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
	month = nov,
	year = {2023},
	note = {GSCC: 0000083 
remark: FigStep通过视觉提示破坏大型视觉语言模型的安全性。},
}

@misc{wang_fca_2021,
	title = {{FCA}: {Learning} a {3D} {Full}-coverage {Vehicle} {Camouflage} for {Multi}-view {Physical} {Adversarial} {Attack}},
	shorttitle = {{FCA}},
	url = {http://arxiv.org/abs/2109.07193},
	doi = {10.48550/arXiv.2109.07193},
	abstract = {Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors. The code of FCA will be available at: https://idrl-lab.github.io/Full-coverage-camouflage-adversarial-attack/.},
	language = {en-US},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Wang, Donghua and Jiang, Tingsong and Sun, Jialiang and Zhou, Weien and Zhang, Xiaoya and Gong, Zhiqiang and Yao, Wen and Chen, Xiaoqian},
	month = dec,
	year = {2021},
	note = {GSCC: 0000094 
arXiv:2109.07193 [cs]
remark: 全覆盖伪装攻击欺骗目标检测器。},
	keywords = {68-06, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.4.0, I.5.4},
}

@misc{zhao_fast_2023,
	title = {Fast {Segment} {Anything}},
	url = {http://arxiv.org/abs/2306.12156},
	doi = {10.48550/arXiv.2306.12156},
	abstract = {The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Zhao, Xu and Ding, Wenchao and An, Yongqi and Du, Yinglong and Yu, Tao and Li, Min and Tang, Ming and Wang, Jinqiao},
	month = jun,
	year = {2023},
	note = {GSCC: 0000220 
arXiv:2306.12156 [cs]
remark: FastSAM
TLDR: By reformulating the task as segments-generation and prompting, it is found that a regular CNN detector with an instance segmentation branch can also accomplish this task well and achieve a comparable performance with the SAM method at 50 times higher run-time speed.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sadasivan_fast_2024,
	title = {Fast {Adversarial} {Attacks} on {Language} {Models} {In} {One} {GPU} {Minute}},
	url = {https://arxiv.org/abs/2402.15570v1},
	abstract = {In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89\% when compared to a gradient-based baseline that takes over an hour to achieve 70\% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce {\textasciitilde}15\% more incorrect outputs when compared to LM outputs in the absence of our attack. We also learn that 22\% of the time, BEAST causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs. We believe that our fast attack, BEAST, has the potential to accelerate research in LM security and privacy. Our codebase is publicly available at https://github.com/vinusankars/BEAST.},
	language = {en},
	urldate = {2024-06-20},
	journal = {arXiv.org},
	author = {Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
	month = feb,
	year = {2024},
	note = {GSCC: 0000016 
remark: 基于BEAST的快速对抗攻击提升语言模型效率。},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en-US},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {GSCC: 0023367 
arXiv:1412.6572 [cs, stat]
remark: FGSM对抗样本攻击},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xiao_exorcising_2023,
	title = {Exorcising ''{Wraith}'': {Protecting} {LiDAR}-based {Object} {Detector} in {Automated} {Driving} {System} from {Appearing} {Attacks}},
	shorttitle = {Exorcising ''{Wraith}''},
	url = {http://arxiv.org/abs/2303.09731},
	doi = {10.48550/arXiv.2303.09731},
	abstract = {Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., appearing attack). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of recent appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object. At the core of our module is a local objectness predictor, which explicitly incorporates the depth information to model the relation between depth and point density, and predicts each local part of an obstacle with an objectness score. Extensive experiments show, our proposed defense eliminates at least 70\% cars forged by three known appearing attacks in most cases, while, for the best previous defense, less than 30\% forged cars are eliminated. Meanwhile, under the same circumstance, our defense incurs less overhead for AP/precision on cars compared with existing defenses. Furthermore, We validate the effectiveness of our proposed defense on simulation-based closed-loop control driving tests in the open-source system of Baidu's Apollo.},
	language = {en-US},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Xiao, Qifan and Pan, Xudong and Lu, Yifan and Zhang, Mi and Dai, Jiarun and Yang, Min},
	month = mar,
	year = {2023},
	note = {GSCC: 0000007 
arXiv:2303.09731 [cs]
TLDR: A novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object.
remark: 提出了一个防御模块用于保护LiDAR检测系统。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{guo_evaluating_2023,
	title = {Evaluating {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Evaluating {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.19736},
	doi = {10.48550/arXiv.2310.19736},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Guo, Zishan and Jin, Renren and Liu, Chuang and Huang, Yufei and Shi, Dan and Supryadi and Yu, Linhao and Liu, Yan and Li, Jiaxuan and Xiong, Bojian and Xiong, Deyi},
	month = nov,
	year = {2023},
	note = {GSCC: 0000113 
arXiv:2310.19736 [cs]
remark: 大语言模型的全面评估研究综述
TLDR: This survey endeavors to offer a panoramic perspective on the evaluation of LLMs, and categorizes the evaluated into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation, which will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {GSCC: 0003547 
arXiv:2107.03374 [cs]
remark: CodeX},
	keywords = {Computer Science - Machine Learning},
}

@article{sup1sup_esg_nodate,
	title = {{ESG视角下人工智能大模型风险识别与治理模型}},
	volume = {39},
	issn = {1000-3045},
	url = {http://old2022.bulletin.cas.cn/zgkxyyk/ch/reader/view_abstract.aspx?file_no=20241104&flag=1},
	abstract = {人工智能大模型应用生态快速拓展，环境、社会和治理面临新的挑战和机遇，探索构建大模型发展风险的治理框架，对于推进人工智能健康可持续发展具有重要的理论价值和实践意义。文章基于ESG（环境—社会—治理）和人工智能治理相关理论，分析ESG视角下人工智能大模型发展效益、典型风险，进而构建人工智能大模型风险治理框架和实施策略。研究认为技术治理和制度创新双管齐下是应对大模型发展风险的必要手段。遵循“切入视角→风险识别→治理机制→治理策略”的逻辑，文章探索性地提出了大模型风险治理框架，并根据模型风险影响周期和影响范围，提出“放开、管控、试点、迭代”4类差异化策略。;The application ecology of artificial intelligence foundation model is rapidly expanding. The environment, society, and governance are facing new challenges and opportunities. Exploring the construction of a governance framework for the development risks of foundation model has important theoretical value and practical significance for promoting the healthy and sustainable development of artificial intelligence. Based on the theories of ESG and artificial intelligence governance, this study analyzes the development benefits and typical risks of foundation model from the perspective of ESG and then constructs a risk governance framework and implementation strategies for artificial intelligence foundation models. This study shows that a dual approach of technological governance and institutional innovation is necessary to address the development risks of foundation models. Following the logic of “entry perspective → risk identification → governance mechanism → governance strategy”, the study exploringly proposes the risk governance framework of foundation model. Based on the impact cycle and scope of the foundation model risk, this study proposes four differentiated governance strategies, including “release, control, pilot, and iteration”.},
	language = {cn},
	number = {11},
	urldate = {2024-12-19},
	journal = {中国科学院院刊},
	author = {施锦诚$^{\textrm{1}}$ and 王国豫$^{\textrm{1 and 2 and 3*}}$ and 王迎春$^{\textrm{1}}$},
	note = {GSCC: 0000000},
	pages = {1845--1859},
}

@misc{huang_epnet_2020,
	title = {{EPNet}: {Enhancing} {Point} {Features} with {Image} {Semantics} for {3D} {Object} {Detection}},
	shorttitle = {{EPNet}},
	url = {http://arxiv.org/abs/2007.08856},
	doi = {10.48550/arXiv.2007.08856},
	abstract = {In this paper, we aim at addressing two critical issues in the 3D detection task, including the exploitation of multiple sensors{\textasciitilde}(namely LiDAR point cloud and camera image), as well as the inconsistency between the localization and classification confidence. To this end, we propose a novel fusion module to enhance the point features with semantic image features in a point-wise manner without any image annotations. Besides, a consistency enforcing loss is employed to explicitly encourage the consistency of both the localization and classification confidence. We design an end-to-end learnable framework named EPNet to integrate these two components. Extensive experiments on the KITTI and SUN-RGBD datasets demonstrate the superiority of EPNet over the state-of-the-art methods. Codes and models are available at: {\textbackslash}url\{https://github.com/happinesslz/EPNet\}.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Huang, Tengteng and Liu, Zhe and Chen, Xiwu and Bai, Xiang},
	month = jul,
	year = {2020},
	note = {GSCC: 0000470 
arXiv:2007.08856 [cs]
Issue: arXiv:2007.08856
remark: 提出EPNet框架，融合点云与图像特征改善3D检测。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en-US},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {GSCC: 0015146 
arXiv:2005.12872 [cs]
Issue: arXiv:2005.12872
remark: DETR},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wen_empowering_2023,
	title = {Empowering {LLM} to use {Smartphone} for {Intelligent} {Task} {Automation}},
	url = {https://arxiv.org/abs/2308.15272v3},
	abstract = {Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9\%, and complete tasks with a success rate of 71.3\%, outperforming the GPT-4-powered baselines by 36.4\% and 39.7\%. The demo, benchmark suites, and source code of AutoDroid will be released at url\{https://autodroid-sys.github.io/\}.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Wen, Hao and Li, Yuanchun and Liu, Guohong and Zhao, Shanhui and Yu, Tao and Li, Toby Jia-Jun and Jiang, Shiqi and Liu, Yunhao and Zhang, Yaqin and Liu, Yunxin},
	month = aug,
	year = {2023},
	note = {GSCC: 0000069 
remark: AutoDroid实现智能手机任务自动化。},
}

@misc{wang_empowering_2024,
	title = {Empowering {Autonomous} {Driving} with {Large} {Language} {Models}: {A} {Safety} {Perspective}},
	shorttitle = {Empowering {Autonomous} {Driving} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.00812},
	doi = {10.48550/arXiv.2312.00812},
	abstract = {Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles.},
	language = {en-US},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Wang, Yixuan and Jiao, Ruochen and Zhan, Sinong Simon and Lang, Chengtian and Huang, Chao and Wang, Zhaoran and Yang, Zhuoran and Zhu, Qi},
	month = mar,
	year = {2024},
	note = {GSCC: 0000020 
arXiv:2312.00812 [cs, eess]
remark: 引入大型语言模型提升自动驾驶安全性。
TLDR: This paper explores the integration of Large Language Models into AD systems, leveraging their robust common-sense knowledge and reasoning abilities, and presents an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{varley_embodied_2024,
	title = {Embodied {AI} with {Two} {Arms}: {Zero}-shot {Learning}, {Safety} and {Modularity}},
	shorttitle = {Embodied {AI} with {Two} {Arms}},
	url = {http://arxiv.org/abs/2404.03570},
	doi = {10.48550/arXiv.2404.03570},
	abstract = {We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.},
	language = {en-US},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Varley, Jake and Singh, Sumeet and Jain, Deepali and Choromanski, Krzysztof and Zeng, Andy and Chowdhury, Somnath Basu Roy and Dubey, Avinava and Sindhwani, Vikas},
	month = apr,
	year = {2024},
	note = {GSCC: 0000011 
arXiv:2404.03570 [cs]
Issue: arXiv:2404.03570
remark: 双臂AI系统零样本学习与模块化操控。
TLDR: An embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace over a large workspace is presented.},
	keywords = {Computer Science - Robotics},
}

@misc{zhou_easyjailbreak_2024,
	title = {{EasyJailbreak}: {A} {Unified} {Framework} for {Jailbreaking} {Large} {Language} {Models}},
	shorttitle = {{EasyJailbreak}},
	url = {http://arxiv.org/abs/2403.12171},
	doi = {10.48550/arXiv.2403.12171},
	abstract = {Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60\% under various jailbreaking attacks. Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57\% and 33\%, respectively. We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.},
	language = {en-US},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Zhou, Weikang and Wang, Xiao and Xiong, Limao and Xia, Han and Gu, Yingshuang and Chai, Mingxu and Zhu, Fukang and Huang, Caishuang and Dou, Shihan and Xi, Zhiheng and Zheng, Rui and Gao, Songyang and Zou, Yicheng and Yan, Hang and Le, Yifan and Wang, Ruohui and Li, Lijun and Shao, Jing and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
	month = mar,
	year = {2024},
	note = {GSCC: 0000010 
arXiv:2403.12171 [cs]
remark: 提出EasyJailbreak框架评估大语言模型安全性。
TLDR: This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs, which supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{khazatsky_droid_2024,
	title = {{DROID}: {A} {Large}-{Scale} {In}-{The}-{Wild} {Robot} {Manipulation} {Dataset}},
	shorttitle = {{DROID}},
	url = {http://arxiv.org/abs/2403.12945},
	doi = {10.48550/arXiv.2403.12945},
	abstract = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.},
	language = {en-US},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and Fagan, Peter David and Hejna, Joey and Itkina, Masha and Lepert, Marion and Ma, Yecheng Jason and Miller, Patrick Tree and Wu, Jimmy and Belkhale, Suneel and Dass, Shivin and Ha, Huy and Jain, Arhan and Lee, Abraham and Lee, Youngwoon and Memmel, Marius and Park, Sungjae and Radosavovic, Ilija and Wang, Kaiyuan and Zhan, Albert and Black, Kevin and Chi, Cheng and Hatch, Kyle Beltran and Lin, Shan and Lu, Jingpei and Mercat, Jean and Rehman, Abdul and Sanketi, Pannag R. and Sharma, Archit and Simpson, Cody and Vuong, Quan and Walke, Homer Rich and Wulfe, Blake and Xiao, Ted and Yang, Jonathan Heewon and Yavary, Arefeh and Zhao, Tony Z. and Agia, Christopher and Baijal, Rohan and Castro, Mateo Guaman and Chen, Daphne and Chen, Qiuyu and Chung, Trinity and Drake, Jaimyn and Foster, Ethan Paul and Gao, Jensen and Herrera, David Antonio and Heo, Minho and Hsu, Kyle and Hu, Jiaheng and Jackson, Donovon and Le, Charlotte and Li, Yunshuang and Lin, Kevin and Lin, Roy and Ma, Zehan and Maddukuri, Abhiram and Mirchandani, Suvir and Morton, Daniel and Nguyen, Tony and O'Neill, Abigail and Scalise, Rosario and Seale, Derick and Son, Victor and Tian, Stephen and Tran, Emi and Wang, Andrew E. and Wu, Yilin and Xie, Annie and Yang, Jingyun and Yin, Patrick and Zhang, Yunchu and Bastani, Osbert and Berseth, Glen and Bohg, Jeannette and Goldberg, Ken and Gupta, Abhinav and Gupta, Abhishek and Jayaraman, Dinesh and Lim, Joseph J. and Malik, Jitendra and Martín-Martín, Roberto and Ramamoorthy, Subramanian and Sadigh, Dorsa and Song, Shuran and Wu, Jiajun and Yip, Michael C. and Zhu, Yuke and Kollar, Thomas and Levine, Sergey and Finn, Chelsea},
	month = mar,
	year = {2024},
	note = {GSCC: 0000087 
arXiv:2403.12945
TLDR: This work introduces DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.
remark: 大型机器人操作数据集DROID的创建和应用。},
	keywords = {\# DROID, Computer Science - Robotics},
}

@misc{chen_driving_2023,
	title = {Driving with {LLMs}: {Fusing} {Object}-{Level} {Vector} {Modality} for {Explainable} {Autonomous} {Driving}},
	shorttitle = {Driving with {LLMs}},
	url = {http://arxiv.org/abs/2310.01957},
	doi = {10.48550/arXiv.2310.01957},
	abstract = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Chen, Long and Sinavski, Oleg and Hünermann, Jan and Karnsund, Alice and Willmott, Andrew James and Birch, Danny and Maund, Daniel and Shotton, Jamie},
	month = oct,
	year = {2023},
	note = {GSCC: 0000133 
arXiv:2310.01957 [cs]
remark: 融合对象级向量的自动驾驶LLM架构},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_driving_2023,
	title = {Driving into the {Future}: {Multiview} {Visual} {Forecasting} and {Planning} with {World} {Model} for {Autonomous} {Driving}},
	shorttitle = {Driving into the {Future}},
	url = {http://arxiv.org/abs/2311.17918},
	doi = {10.48550/arXiv.2311.17918},
	abstract = {In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road. To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models. Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes. Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time. Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards. Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning.},
	language = {en-US},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Wang, Yuqi and He, Jiawei and Fan, Lue and Li, Hongxin and Chen, Yuntao and Zhang, Zhaoxiang},
	month = nov,
	year = {2023},
	note = {GSCC: 0000070 
arXiv:2311.17918 [cs]
remark: 推出Drive-WM模型，助力自动驾驶规划。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tian_drivevlm_2024,
	title = {{DriveVLM}: {The} {Convergence} of {Autonomous} {Driving} and {Large} {Vision}-{Language} {Models}},
	shorttitle = {{DriveVLM}},
	url = {https://arxiv.org/abs/2402.12289v5},
	abstract = {A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and unpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a production vehicle, verifying it is effective in real-world autonomous driving environments.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang},
	month = feb,
	year = {2024},
	note = {GSCC: 0000082 
remark: DriveVLM提升自动驾驶的场景理解与规划能力。},
}

@misc{wang_drivemlm_2023,
	title = {{DriveMLM}: {Aligning} {Multi}-{Modal} {Large} {Language} {Models} with {Behavioral} {Planning} {States} for {Autonomous} {Driving}},
	shorttitle = {{DriveMLM}},
	url = {http://arxiv.org/abs/2312.09245},
	doi = {10.48550/arXiv.2312.09245},
	abstract = {Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs. Code and models shall be released at https://github.com/OpenGVLab/DriveMLM.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Wang, Wenhai and Xie, Jiangwei and Hu, ChuanYang and Zou, Haoming and Fan, Jianan and Tong, Wenwen and Wen, Yang and Wu, Silei and Deng, Hanming and Li, Zhiqi and Tian, Hao and Lu, Lewei and Zhu, Xizhou and Wang, Xiaogang and Qiao, Yu and Dai, Jifeng},
	month = dec,
	year = {2023},
	note = {GSCC: 0000087 
arXiv:2312.09245 [cs]
remark: 基于LLM的自动驾驶行为规划框架
TLDR: DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sima_drivelm_2023,
	title = {{DriveLM}: {Driving} with {Graph} {Visual} {Question} {Answering}},
	shorttitle = {{DriveLM}},
	url = {http://arxiv.org/abs/2312.14150},
	doi = {10.48550/arXiv.2312.14150},
	abstract = {We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Luo, Ping and Geiger, Andreas and Li, Hongyang},
	month = dec,
	year = {2023},
	note = {GSCC: 0000128 
arXiv:2312.14150 [cs]
remark: 集成VLM提升自动驾驶泛化能力。
TLDR: This work instantiate datasets built upon nuScenes and CARLA, and proposes a VLM-based baseline approach for jointly performing Graph VQA and end-to-end driving, demonstrating that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_drivegpt4_2024,
	title = {{DriveGPT4}: {Interpretable} {End}-to-end {Autonomous} {Driving} via {Large} {Language} {Model}},
	shorttitle = {{DriveGPT4}},
	url = {http://arxiv.org/abs/2310.01412},
	doi = {10.48550/arXiv.2310.01412},
	abstract = {Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V. The code and dataset will be publicly available.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K. and Li, Zhenguo and Zhao, Hengshuang},
	month = mar,
	year = {2024},
	note = {GSCC: 0000204 
arXiv:2310.01412 [cs]
remark: DriveGPT4：基于LLM的自动驾驶系统。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{fu_drive_2023,
	title = {Drive {Like} a {Human}: {Rethinking} {Autonomous} {Driving} with {Large} {Language} {Models}},
	shorttitle = {Drive {Like} a {Human}},
	url = {http://arxiv.org/abs/2307.07162},
	doi = {10.48550/arXiv.2307.07162},
	abstract = {In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https://github.com/PJLab-ADG/DriveLikeAHuman .},
	language = {en-US},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Fu, Daocheng and Li, Xin and Wen, Licheng and Dou, Min and Cai, Pinlong and Shi, Botian and Qiao, Yu},
	month = jul,
	year = {2023},
	note = {GSCC: 0000135 
arXiv:2307.07162 [cs]
remark: LLM以类似人类的方式理解驾驶环境的潜力，并分析了其在面对复杂场景时进行推理、解释和记忆的能力},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics},
}

@misc{yu_dont_2024,
	title = {Don't {Listen} {To} {Me}: {Understanding} and {Exploring} {Jailbreak} {Prompts} of {Large} {Language} {Models}},
	shorttitle = {Don't {Listen} {To} {Me}},
	url = {http://arxiv.org/abs/2403.17336},
	doi = {10.48550/arXiv.2403.17336},
	abstract = {Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited. Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbreak effectiveness empirically. Further, we conducted a user study involving 92 participants with diverse backgrounds to unveil the process of manually creating jailbreak prompts. We observed that users often succeeded in jailbreak prompts generation regardless of their expertise in LLMs. Building on the insights from the user study, we also developed a system using AI as the assistant to automate the process of jailbreak prompt generation.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Yu, Zhiyuan and Liu, Xiaogeng and Liang, Shunning and Cameron, Zach and Xiao, Chaowei and Zhang, Ning},
	month = mar,
	year = {2024},
	note = {GSCC: 0000046 
arXiv:2403.17336 [cs]
remark: 探索大语言模型的越狱提示。
TLDR: It is observed that users often succeeded in jailbreak prompts generation regardless of their expertise in LLMs, and a system using AI as the assistant to automate the process of jailbreak prompt generation was developed.},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@inproceedings{zhang_dolphinattack_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {{DolphinAttack}: {Inaudible} {Voice} {Commands}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {{DolphinAttack}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134052},
	doi = {10.1145/3133956.3134052},
	abstract = {Speech recognition (SR) systems such as Siri or Google Now have become an increasingly popular human-computer interaction method, and have turned various systems into voice controllable systems (VCS). Prior work on attacking VCS shows that the hidden voice commands that are incomprehensible to people can control the systems. Hidden voice commands, though "hidden", are nonetheless audible. In this work, we design a totally inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers (e.g., f \&gt; 20 kHz) to achieve inaudibility. By leveraging the nonlinearity of the microphone circuits, the modulated low-frequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the speech recognition systems. We validated DolphinAttack on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei HiVoice, Cortana and Alexa. By injecting a sequence of inaudible voice commands, we show a few proof-of-concept attacks, which include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions, and suggest to re-design voice controllable systems to be resilient to inaudible voice command attacks.},
	urldate = {2024-12-18},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wenyuan},
	year = {2017},
	note = {GSCC: 0000921 
TLDR: A totally inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers to achieve inaudibility and is validated on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei HiVoice, Cortana and Alexa.
remark: DolphinAttack利用超声波实现隐蔽语音攻击。},
	pages = {103--117},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	language = {en-US},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {GSCC: 0001978 
arXiv:2304.07193 [cs]
TLDR: This work revisits existing approaches and combines different techniques to scale the pretraining in terms of data and model size, and proposes an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature.
remark: 自监督学习产出强大视觉特征。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wen_dilu_2024,
	title = {{DiLu}: {A} {Knowledge}-{Driven} {Approach} to {Autonomous} {Driving} with {Large} {Language} {Models}},
	shorttitle = {{DiLu}},
	url = {http://arxiv.org/abs/2309.16292},
	doi = {10.48550/arXiv.2309.16292},
	abstract = {Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/},
	language = {en-US},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Wen, Licheng and Fu, Daocheng and Li, Xin and Cai, Xinyu and Ma, Tao and Cai, Pinlong and Dou, Min and Shi, Botian and He, Liang and Qiao, Yu},
	month = feb,
	year = {2024},
	note = {GSCC: 0000125 
arXiv:2309.16292 [cs]
remark: 利用大型语言模型提升自动驾驶决策。
TLDR: The DiLu framework is proposed, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously and is the first to leverage knowledge-driven capability in decision-making for autonomous vehicles.},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics},
}

@article{ji_detecting_2024,
	title = {Detecting {Inaudible} {Voice} {Commands} via {Acoustic} {Attenuation} by {Multi}-channel {Microphones}},
	issn = {1941-0018},
	url = {https://ieeexplore.ieee.org/abstract/document/10403816},
	doi = {10.1109/TDSC.2024.3355117},
	abstract = {DolphinAttacks (i.e., inaudible voice commands) modulate audible voices over ultrasounds to inject malicious commands silently into voice assistants and manipulate controlled systems (e.g., doors or smart speakers). Eliminating DolphinAttacks is challenging if ever possible since it requires to modify the microphone hardware. In this paper, we design EarArray, a lightweight method that can not only detect such attacks but also identify the direction of attackers without requiring any extra hardware or hardware modification. Essentially, inaudible voice commands are modulated on ultrasounds that inherently attenuate faster than the one of audible sounds. By inspecting the command sound signals via the built-in multiple microphones on smart devices, EarArray is able to estimate the attenuation rate and thus detect the attacks. We propose a model of the propagation of audible sounds and ultrasounds from the sound source to a voice assistant, e.g., a smart speaker, and illustrate the underlying principle and its feasibility. We implemented EarArray using two specially-designed microphone arrays and our experiments show that EarArray can detect inaudible voice commands with an accuracy of above 99\% and recognize the direction of the attackers with an accuracy of 97.89\% and can also detect the laser-based attack with an accuracy of 100\%.},
	language = {en-US},
	urldate = {2024-06-04},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Ji, Xiaoyu and Zhang, Guoming and Li, Xinfeng and Qu, Gang and Cheng, Xiuzhen and Xu, Wenyuan},
	year = {2024},
	note = {GSCC: 0000001 
Conference Name: IEEE Transactions on Dependable and Secure Computing
remark: 检测并定位不可听语音攻击的方法。},
	keywords = {Acoustic Attenuatio, Acoustics, Attenuation, Defense, Hardware, Inaudible Voice Command, Microphone arrays, Smart devices, Ultrasonic imaging, Ultrasonic variables measurement, Voice Assistant},
	pages = {1--17},
}

@inproceedings{wu_depthfake_2023,
	title = {{DepthFake}: {Spoofing} {3D} {Face} {Authentication} with a {2D} {Photo}},
	shorttitle = {{DepthFake}},
	url = {https://ieeexplore.ieee.org/abstract/document/10179429},
	doi = {10.1109/SP46215.2023.10179429},
	abstract = {Face authentication has been widely used in access control, and the latest 3D face authentication systems employ 3D liveness detection techniques to cope with the photo replay attacks, whereby an attacker uses a 2D photo to bypass the authentication. In this paper, we analyze the security of 3D liveness detection systems that utilize structured light depth cameras and discover a new attack surface against 3D face authentication systems. We propose DepthFake attacks that can spoof a 3D face authentication using only one single 2D photo. To achieve this goal, DepthFake first estimates the 3D depth information of a target victim’s face from his 2D photo. Then, DepthFake projects the carefully-crafted scatter patterns embedded with the face depth information, in order to empower the 2D photo with 3D authentication properties. We overcome a collection of practical challenges, e.g., depth estimation errors from 2D photos, depth images forgery based on structured light, the alignment of the RGB image and depth images for a face, and implemented DepthFake in laboratory setups. We validated DepthFake on 3 commercial face authentication systems (i.e., Tencent Cloud, Baidu Cloud, and 3DiVi) and one commercial access control device. The results over 50 users demonstrate that DepthFake achieves an overall Depth attack success rate of 79.4\% and RGB-D attack success rate of 59.4\% in the real world.},
	language = {en-US},
	urldate = {2024-06-04},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Wu, Zhihao and Cheng, Yushi and Yang, Jiahui and Ji, Xiaoyu and Xu, Wenyuan},
	month = may,
	year = {2023},
	note = {GSCC: 0000007 
ISSN: 2375-1207
remark: 提出一种利用2D照片欺骗3D人脸认证的方法。
TLDR: This paper proposes DepthFake attacks that can spoof a 3D face authentication using only one single 2D photo, and analyzes the security of 3D liveness detection systems that utilize structured light depth cameras and discovers a new attack surface against 3DFace authentication systems.},
	keywords = {Access control, Authentication, Cameras, Estimation error, Privacy, Three-dimensional displays, Web and internet services},
	pages = {917--933},
}

@inproceedings{zhou_dehirec_2023,
	title = {{DeHiREC}: {Detecting} {Hidden} {Voice} {Recorders} via {ADC} {Electromagnetic} {Radiation}},
	shorttitle = {{DeHiREC}},
	url = {https://ieeexplore.ieee.org/abstract/document/10179480},
	doi = {10.1109/SP46215.2023.10179480},
	abstract = {Unauthorized covert voice recording brings a remarkable threat to privacy-sensitive scenarios, such as confidential meetings and private conversations. Due to the miniaturization and disguise characteristics, hidden voice recorders are difficult to be noticed in their surroundings. In this paper, we present DeHiREC, the first proof-of-concept system that can detect offline hidden voice recorders from their electromagnetic radiations (EMR). We first characterize the unique patterns of the emanated EMR signals and then locate the EMR source, i.e., the analog-to-digital converter (ADC) module embedded in the mixed signal system-on-chips (MSoCs). Since these unintentional EMR signals can be extremely noisy and weak, accurately detecting them can be challenging. To address this challenge, we first design an EMR Catalyzing method to stimulate the EMR signals actively and then employ an adaptive-folding algorithm to improve the signal-to-noise ratio (SNR) of the sensed EMRs. Once the sensed EMR variation corresponds to our active stimulation, we can determine that there exists a hidden voice recorder. We evaluate the performance of DeHiREC on 13 commercial voice recorders under various impacts, including interference from other devices. Experimental results reveal that DeHiREC is effective in detecting all 13 voice recorders and achieves an overall success rate of 92.17\% and a recall rate of 86.14\% at a distance of 0.2 m.},
	language = {en-US},
	urldate = {2024-06-04},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Zhou, Ruochen and Ji, Xiaoyu and Yan, Chen and Chen, Yi-Chao and Xu, Wenyuan and Li, Chaohao},
	month = may,
	year = {2023},
	note = {GSCC: 0000009 
ISSN: 2375-1207
remark: 通过电磁辐射检测隐蔽录音设备。
TLDR: DeHiREC is the first proof-of-concept system that can detect offline hidden voice recorders from their electromagnetic radiations (EMR) and achieves an overall success rate of 92.17\% and a recall rate of 86.14\% under various impacts, including interference from other devices.},
	keywords = {Electromagnetic radiation, Oral communication, Performance evaluation, Privacy, System-on-chip, Wireless communication, Wireless sensor networks, analog-to-digital converter (ADC), electromagnetic radiation (EMR), hidden voice recorder},
	pages = {3113--3128},
}

@misc{lin_data_2024,
	title = {Data {Scaling} {Laws} in {Imitation} {Learning} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2410.18647},
	doi = {10.48550/arXiv.2410.18647},
	abstract = {Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\% success rates in novel environments with unseen objects.},
	language = {en-US},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Lin, Fanqi and Hu, Yingdong and Sheng, Pingyue and Wen, Chuan and You, Jiacheng and Gao, Yang},
	month = oct,
	year = {2024},
	note = {GSCC: 0000005 
arXiv:2410.18647 [cs]
TLDR: This paper investigates whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment.
remark: 研究机器人的Scaling Laws},
	keywords = {\# Scaling Laws, Computer Science - Robotics},
}

@misc{whang_data_2022,
	title = {Data {Collection} and {Quality} {Challenges} in {Deep} {Learning}: {A} {Data}-{Centric} {AI} {Perspective}},
	shorttitle = {Data {Collection} and {Quality} {Challenges} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2112.06409},
	doi = {10.48550/arXiv.2112.06409},
	abstract = {Data-centric AI is at the center of a fundamental shift in software engineering where machine learning becomes the new software, powered by big data and computing infrastructure. Here software engineering needs to be re-thought where data becomes a first-class citizen on par with code. One striking observation is that a significant portion of the machine learning process is spent on data preparation. Without good data, even the best machine learning algorithms cannot perform well. As a result, data-centric AI practices are now becoming mainstream. Unfortunately, many datasets in the real world are small, dirty, biased, and even poisoned. In this survey, we study the research landscape for data collection and data quality primarily for deep learning applications. Data collection is important because there is lesser need for feature engineering for recent deep learning approaches, but instead more need for large amounts of data. For data quality, we study data validation, cleaning, and integration techniques. Even if the data cannot be fully cleaned, we can still cope with imperfect data during model training using robust model training techniques. In addition, while bias and fairness have been less studied in traditional data management research, these issues become essential topics in modern machine learning applications. We thus study fairness measures and unfairness mitigation techniques that can be applied before, during, or after model training. We believe that the data management community is well poised to solve these problems.},
	language = {en},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Whang, Steven Euijong and Roh, Yuji and Song, Hwanjun and Lee, Jae-Gil},
	month = dec,
	year = {2022},
	note = {GSCC: 0000368 
arXiv:2112.06409 [cs]
remark: 深度学习中数据收集与质量挑战。},
	keywords = {Computer Science - Machine Learning},
}

@misc{arai_covla_2024,
	title = {{CoVLA}: {Comprehensive} {Vision}-{Language}-{Action} {Dataset} for {Autonomous} {Driving}},
	shorttitle = {{CoVLA}},
	url = {http://arxiv.org/abs/2408.10845},
	doi = {10.48550/arXiv.2408.10845},
	abstract = {Autonomous driving, particularly navigating complex and unanticipated scenarios, demands sophisticated reasoning and planning capabilities. While Multi-modal Large Language Models (MLLMs) offer a promising avenue for this, their use has been largely confined to understanding complex environmental contexts or generating high-level driving commands, with few studies extending their application to end-to-end path planning. A major research bottleneck is the lack of large-scale annotated datasets encompassing vision, language, and action. To address this issue, we propose CoVLA (Comprehensive Vision-Language-Action) Dataset, an extensive dataset comprising real-world driving videos spanning more than 80 hours. This dataset leverages a novel, scalable approach based on automated data processing and a caption generation pipeline to generate accurate driving trajectories paired with detailed natural language descriptions of driving environments and maneuvers. This approach utilizes raw in-vehicle sensor data, allowing it to surpass existing datasets in scale and annotation richness. Using CoVLA, we investigate the driving capabilities of MLLMs that can handle vision, language, and action in a variety of driving scenarios. Our results illustrate the strong proficiency of our model in generating coherent language and action outputs, emphasizing the potential of Vision-Language-Action (VLA) models in the field of autonomous driving. This dataset establishes a framework for robust, interpretable, and data-driven autonomous driving systems by providing a comprehensive platform for training and evaluating VLA models, contributing to safer and more reliable self-driving vehicles. The dataset is released for academic purpose.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Arai, Hidehisa and Miwa, Keita and Sasaki, Kento and Yamaguchi, Yu and Watanabe, Kohei and Aoki, Shunsuke and Yamamoto, Issei},
	month = aug,
	year = {2024},
	note = {GSCC: 0000004 
arXiv:2408.10845
TLDR: This dataset establishes a framework for robust, interpretable, and data-driven autonomous driving systems by providing a comprehensive platform for training and evaluating VLA models, contributing to safer and more reliable self-driving vehicles.
remark: 提出CoVLA数据集助力自动驾驶研究。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{luu_context-aware_2024,
	title = {Context-aware {LLM}-based {Safe} {Control} {Against} {Latent} {Risks}},
	url = {https://arxiv.org/abs/2403.11863v1},
	abstract = {It is challenging for autonomous control systems to perform complex tasks in the presence of latent risks. Motivated by this challenge, this paper proposes an integrated framework that involves Large Language Models (LLMs), stochastic gradient descent (SGD), and optimization-based control. In the first phrase, the proposed framework breaks down complex tasks into a sequence of smaller subtasks, whose specifications account for contextual information and latent risks. In the second phase, these subtasks and their parameters are refined through a dual process involving LLMs and SGD. LLMs are used to generate rough guesses and failure explanations, and SGD is used to fine-tune parameters. The proposed framework is tested using simulated case studies of robots and vehicles. The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.},
	language = {en},
	urldate = {2024-06-09},
	journal = {arXiv.org},
	author = {Luu, Quan Khanh and Deng, Xiyu and Van Ho, Anh and Nakahira, Yorie},
	month = mar,
	year = {2024},
	note = {GSCC: 0000007 
remark: LLM驱动的控制系统应对潜在风险。},
}

@misc{guo_cold-attack_2024,
	title = {{COLD}-{Attack}: {Jailbreaking} {LLMs} with {Stealthiness} and {Controllability}},
	shorttitle = {{COLD}-{Attack}},
	url = {http://arxiv.org/abs/2402.08679},
	doi = {10.48550/arXiv.2402.08679},
	abstract = {Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.},
	language = {en-US},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
	month = feb,
	year = {2024},
	note = {GSCC: 0000049 
arXiv:2402.08679 [cs]
remark: 提出了可控的大语言模型对抗攻击方法。
TLDR: The COLD-Attack framework is introduced, which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence, and leads to diverse new jailbreak scenarios.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{qin_cold_2022,
	title = {{COLD} {Decoding}: {Energy}-based {Constrained} {Text} {Generation} with {Langevin} {Dynamics}},
	shorttitle = {{COLD} {Decoding}},
	url = {http://arxiv.org/abs/2202.11705},
	doi = {10.48550/arXiv.2202.11705},
	abstract = {Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.},
	language = {en-US},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
	month = oct,
	year = {2022},
	note = {GSCC: 0000130 
arXiv:2202.11705 [cs]
remark: COLD解码通过能量函数实现受控文本生成。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_coherent_2024,
	title = {{COHERENT}: {Collaboration} of {Heterogeneous} {Multi}-{Robot} {System} with {Large} {Language} {Models}},
	shorttitle = {{COHERENT}},
	url = {http://arxiv.org/abs/2409.15146},
	doi = {10.48550/arXiv.2409.15146},
	abstract = {Leveraging the powerful reasoning capabilities of large language models (LLMs), recent LLM-based robot task planning methods yield promising results. However, they mainly focus on single or multiple homogeneous robots on simple tasks. Practically, complex long-horizon tasks always require collaborations among multiple heterogeneous robots especially with more complex action spaces, which makes these tasks more challenging. To this end, we propose COHERENT, a novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including quadrotors, robotic dogs, and robotic arms. Specifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is designed to decompose and assign actions for individual robots, where a centralized task assigner makes a task planning proposal to decompose the complex task into subtasks, and then assigns subtasks to robot executors. Each robot executor selects a feasible action to implement the assigned subtask and reports self-reflection feedback to the task assigner for plan adjustment. The PEFA loops until the task is completed. Moreover, we create a challenging heterogeneous multi-robot task planning benchmark encompassing 100 complex long-horizon tasks. The experimental results show that our work surpasses the previous methods by a large margin in terms of success rate and execution efficiency. The experimental videos, code, and benchmark are released at https://github.com/MrKeee/COHERENT.},
	language = {en-US},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Liu, Kehui and Tang, Zixin and Wang, Dong and Wang, Zhigang and Zhao, Bin and Li, Xuelong},
	month = sep,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2409.15146 [cs]
TLDR: This work proposes COHERENT, a novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including quadrotors, robotic dogs, and robotic arms and creates a challenging heterogeneous multi-robot task planning benchmark encompassing 100 complex long-horizon tasks.
remark: 异构多机器人系统协同任务规划框架COHERENT。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{pang_clocs_2020,
	title = {{CLOCs}: {Camera}-{LiDAR} {Object} {Candidates} {Fusion} for {3D} {Object} {Detection}},
	shorttitle = {{CLOCs}},
	url = {http://arxiv.org/abs/2009.00784},
	doi = {10.48550/arXiv.2009.00784},
	abstract = {There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Pang, Su and Morris, Daniel and Radha, Hayder},
	month = sep,
	year = {2020},
	note = {GSCC: 0000480 
arXiv:2009.00784 [cs]
Issue: arXiv:2009.00784
remark: 提出一种高效的相机-激光雷达融合网络CLOCs。},
	keywords = {CLOCs, Computer Science - Computer Vision and Pattern Recognition, Fusion Model},
}

@article{petrovic_chatgpt_2023,
	title = {{ChatGPT} in {IoT} {Systems}: {Arduino} {Case} {Studies}},
	shorttitle = {{ChatGPT} in {IoT} {Systems}},
	url = {https://rgdoi.net/10.13140/RG.2.2.10214.40004},
	doi = {10.13140/RG.2.2.10214.40004},
	abstract = {Since the beginning of this year, the novel large language model (LLM) based ChatGPT conversational agent has been in spotlight, due to its comprehensiveness across many fields – from novel writing to playing board games. In this paper, it is explored how it can be leveraged within IoT systems, taking into account both the novel scenarios enabled relying on ChatGPT’s power of question answering and software development as well. As example, two case studies related to Arduino platform are considered: 1) ChatGPT-based predictions on sensor data collected by Arduino 2) model-driven automated Arduino code generation.},
	language = {en},
	urldate = {2023-11-12},
	author = {Petrović, Nenad and Konicanin, Samir I and Suljović, Suad N},
	year = {2023},
	note = {GSCC: 0000006 
Publisher: Unpublished
remark: 探究ChatGPT在Arduino IoT系统应用。},
}

@article{wang_chatgpt_2023,
	title = {{ChatGPT} as {Your} {Vehicle} {Co}-{Pilot}: {An} {Initial} {Attempt}},
	volume = {8},
	issn = {2379-8904},
	shorttitle = {{ChatGPT} as {Your} {Vehicle} {Co}-{Pilot}},
	url = {https://ieeexplore.ieee.org/document/10286969},
	doi = {10.1109/TIV.2023.3325300},
	abstract = {One of the most challenging problems in human-machine co-work is the gap between human intention and the machine's understanding and execution. Large Language Models (LLMs) have been showing superior abilities in solving such issue. In this article, we design a universal framework that embeds LLMs as a vehicle “Co-Pilot” of driving, which can accomplish specific driving tasks with human intention satisfied based on the information provided. Meanwhile, a utilization workflow is defined to handle the interaction between humans and vehicles, and memory mechanism is introduced to organize the information involved in the tasks. Expert Oriented Black-Box tuning is proposed to improve the performance of the Co-Pilot without finetuning or training the LLMs. In the experiment, the Co-Pilot is applied to two different tasks, i.e., path tracking control and trajectory planning. The Co-Pilot adjusts vehicle operating conditions by selecting a proper controller or planning a certain trajectory to fit human intentions. Simulation tests are conducted to evaluate the performance and generality of the proposed module. The results show that the Co-Pilot can accomplish most of the tasks based on only natural language processing, although it is not flawless. Finally, a discussion about human-machine hybrid intelligence and further applications of LLMs in autonomous driving is made. We believe that such a framework has promising potential in further applications in the field of automous vehicles.},
	language = {en-US},
	number = {12},
	urldate = {2024-04-02},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Wang, Shiyi and Zhu, Yuxuan and Li, Zhiheng and Wang, Yutong and Li, Li and He, Zhengbing},
	month = dec,
	year = {2023},
	note = {GSCC: 0000050 
Conference Name: IEEE Transactions on Intelligent Vehicles
remark: LLM作为驾驶“副驾”，增强人车互动。
TLDR: A universal framework that embeds large Language Models as a vehicle “Co-Pilot” of driving, which can accomplish specific driving tasks with human intention satisfied based on the information provided, has promising potential in further applications in the field of automous vehicles.},
	keywords = {Adaptive control, Automation, Autonomous driving, Closed box, Cognition, Human factors, Human-machine systems, Task analysis, Training, Tuning, human-machine interaction, large language model, parallel learning},
	pages = {4706--4721},
}

@misc{zaboli_chatgpt_2023,
	title = {{ChatGPT} and other {Large} {Language} {Models} for {Cybersecurity} of {Smart} {Grid} {Applications}},
	url = {https://arxiv.org/abs/2311.05462v1},
	abstract = {Cybersecurity breaches targeting electrical substations constitute a significant threat to the integrity of the power grid, necessitating comprehensive defense and mitigation strategies. Any anomaly in information and communication technology (ICT) should be detected for secure communications between devices in digital substations. This paper proposes large language models (LLM), e.g., ChatGPT, for the cybersecurity of IEC 61850-based digital substation communications. Multicast messages such as generic object oriented substation event (GOOSE) and sampled value (SV) are used for case studies. The proposed LLM-based cybersecurity framework includes for the first time data pre-processing of communication systems and human-in-the-loop (HITL) training (considering the cybersecurity guidelines recommended by humans). The results show a comparative analysis of detected anomaly data carried out based on the performance evaluation metrics for different LLMs. A hardware-in-the-loop (HIL) testbed is used to generate and extract a dataset of IEC 61850 communications.},
	language = {en},
	urldate = {2023-11-15},
	journal = {arXiv.org},
	author = {Zaboli, Aydin and Choi, Seong Lok and Song, Tai-Jin and Hong, Junho},
	month = nov,
	year = {2023},
	note = {GSCC: 0000018 
remark: 利用ChatGPT提高电网通信安全。},
}

@misc{deng_catch_2023,
	title = {Catch {You} and {I} {Can}: {Revealing} {Source} {Voiceprint} {Against} {Voice} {Conversion}},
	shorttitle = {Catch {You} and {I} {Can}},
	url = {http://arxiv.org/abs/2302.12434},
	doi = {10.48550/arXiv.2302.12434},
	abstract = {Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.},
	language = {en-US},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Deng, Jiangyi and Chen, Yanjiao and Zhong, Yinan and Miao, Qianhao and Gong, Xueluan and Xu, Wenyuan},
	month = feb,
	year = {2023},
	note = {GSCC: 0000012 
arXiv:2302.12434 [cs, eess]
remark: 开发Revelio模型恢复转换语音源声纹。
TLDR: The first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit is made and Revelio, a representation learning model, is developed, which learns to effectively extract the voiceprint of the source speaker from converted audio samples.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{zhang_capatch_2023,
	title = {\{{CAPatch}\}: {Physical} {Adversarial} {Patch} against {Image} {Captioning} {Systems}},
	isbn = {978-1-939133-37-3},
	shorttitle = {\{{CAPatch}\}},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-shibo},
	language = {en},
	urldate = {2023-11-05},
	author = {Zhang, Shibo and Cheng, Yushi and Zhu, Wenjun and Ji, Xiaoyu and Xu, Wenyuan},
	year = {2023},
	note = {GSCC: 0000008 
remark: CAPatch攻击，针对图像字幕生成系统提出的一种对抗攻击技术},
	pages = {679--696},
}

@misc{mu_can_2023,
	title = {Can {LLMs} {Follow} {Simple} {Rules}?},
	url = {http://arxiv.org/abs/2311.04235},
	abstract = {As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as “do not generate abusive content”, but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developerprovided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RULES), a programmatic framework for measuring rule-following ability in LLMs. RULES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RULES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.},
	language = {en},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and Karamardian, David and Aljeraisy, Lulwa and Hendrycks, Dan and Wagner, David},
	month = nov,
	year = {2023},
	note = {GSCC: 0000022 
arXiv:2311.04235 [cs]
remark: 提出检测LLM遵循规则能力的RULES框架。},
}

@misc{zhu_campro_2023,
	title = {{CamPro}: {Camera}-based {Anti}-{Facial} {Recognition}},
	shorttitle = {{CamPro}},
	url = {http://arxiv.org/abs/2401.00151},
	doi = {10.48550/arXiv.2401.00151},
	abstract = {The proliferation of images captured from millions of cameras and the advancement of facial recognition (FR) technology have made the abuse of FR a severe privacy threat. Existing works typically rely on obfuscation, synthesis, or adversarial examples to modify faces in images to achieve anti-facial recognition (AFR). However, the unmodified images captured by camera modules that contain sensitive personally identifiable information (PII) could still be leaked. In this paper, we propose a novel approach, CamPro, to capture inborn AFR images. CamPro enables well-packed commodity camera modules to produce images that contain little PII and yet still contain enough information to support other non-sensitive vision applications, such as person detection. Specifically, CamPro tunes the configuration setup inside the camera image signal processor (ISP), i.e., color correction matrix and gamma correction, to achieve AFR, and designs an image enhancer to keep the image quality for possible human viewers. We implemented and validated CamPro on a proof-of-concept camera, and our experiments demonstrate its effectiveness on ten state-of-the-art black-box FR models. The results show that CamPro images can significantly reduce face identification accuracy to 0.3{\textbackslash}\% while having little impact on the targeted non-sensitive vision application. Furthermore, we find that CamPro is resilient to adaptive attackers who have re-trained their FR models using images generated by CamPro, even with full knowledge of privacy-preserving ISP parameters.},
	language = {en-US},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Zhu, Wenjun and Sun, Yuan and Liu, Jiani and Cheng, Yushi and Ji, Xiaoyu and Xu, Wenyuan},
	month = dec,
	year = {2023},
	note = {GSCC: 0000002 
arXiv:2401.00151 [cs]
remark: 通过调整相机ISP实现反人脸识别。
TLDR: CamPro enables well-packed commodity camera modules to produce images that contain little PII and yet still contain enough information to support other non-sensitive vision applications, such as person detection, and is resilient to adaptive attackers who have re-trained their FR models using images generated by CamPro.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{yoon_by_2024,
	title = {By {My} {Eyes}: {Grounding} {Multimodal} {Large} {Language} {Models} with {Sensor} {Data} via {Visual} {Prompting}},
	shorttitle = {By {My} {Eyes}},
	url = {http://arxiv.org/abs/2407.10385},
	doi = {10.48550/arXiv.2407.10385},
	abstract = {Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. We propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). We design a visual prompt that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10\% higher accuracy than text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes.},
	language = {en-US},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Yoon, Hyungjun and Tolera, Biniyam Aschalew and Gong, Taesik and Lee, Kimin and Lee, Sung-Ju},
	month = sep,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2407.10385
TLDR: A visual prompt is designed that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions, and a visualization generator is introduced that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge.
remark: 传感器数据转化为图像使用MMLLM处理。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sejnova_bridging_2024,
	title = {Bridging {Language}, {Vision} and {Action}: {Multimodal} {VAEs} in {Robotic} {Manipulation} {Tasks}},
	shorttitle = {Bridging {Language}, {Vision} and {Action}},
	url = {http://arxiv.org/abs/2404.01932},
	doi = {10.48550/arXiv.2404.01932},
	abstract = {In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55\%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Sejnova, Gabriela and Vavrecka, Michal and Stepanova, Karla},
	month = apr,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2404.01932
TLDR: This work explores whether and how multimodal VAEs can be employed in unsupervised robotic manipulation tasks in a simulated environment and proposes a model-invariant training alternative that improves the models' performance in a simulator by up to 55\%.
remark: 多模态VAEs提升机器人操作任务性能。},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_brant-x_2024,
	title = {Brant-{X}: {A} {Unified} {Physiological} {Signal} {Alignment} {Framework}},
	shorttitle = {Brant-{X}},
	url = {http://arxiv.org/abs/2409.00122},
	doi = {10.48550/arXiv.2409.00122},
	abstract = {Physiological signals serve as indispensable clues for understanding various physiological states of human bodies. Most existing works have focused on a single type of physiological signals for a range of application scenarios. However, as the body is a holistic biological system, the inherent interconnection among various physiological data should not be neglected. In particular, given the brain's role as the control center for vital activities, electroencephalogram (EEG) exhibits significant correlations with other physiological signals. Therefore, the correlation between EEG and other physiological signals holds potential to improve performance in various scenarios. Nevertheless, achieving this goal is still constrained by several challenges: the scarcity of simultaneously collected physiological data, the differences in correlations between various signals, and the correlation differences between various tasks. To address these issues, we propose a unified physiological signal alignment framework, Brant-X, to model the correlation between EEG and other signals. Our approach (1) employs the EEG foundation model to data-efficiently transfer the rich knowledge in EEG to other physiological signals, and (2) introduces the two-level alignment to fully align the semantics of EEG and other signals from different semantic scales. In the experiments, Brant-X achieves state-of-the-art performance compared with task-agnostic and task-specific baselines on various downstream tasks in diverse scenarios, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. Moreover, the analysis on the arrhythmia detection task and the visualization in case study further illustrate the effectiveness of Brant-X in the knowledge transfer from EEG to other physiological signals. The model's homepage is at https://github.com/zjunet/Brant-X/.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Zhang, Daoze and Yuan, Zhizhang and Chen, Junru and Chen, Kerui and Yang, Yang},
	month = aug,
	year = {2024},
	note = {GSCC: 0000001 
arXiv:2409.00122
remark: Brant-X框架提升生理信号语义对齐。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, ⏳},
}

@misc{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	doi = {10.48550/arXiv.2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	language = {en-US},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jun,
	year = {2023},
	note = {GSCC: 0004402 
arXiv:2301.12597 [cs]
remark: BLIP-2 图像-文本模型
TLDR: BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zheng_black-box_2024,
	title = {Black-box {Targeted} {Adversarial} {Attack} on {Segment} {Anything} ({SAM})},
	url = {http://arxiv.org/abs/2310.10010},
	doi = {10.48550/arXiv.2310.10010},
	abstract = {Deep recognition models are widely vulnerable to adversarial examples, which change the model output by adding quasi-imperceptible perturbation to the image input. Recently, Segment Anything Model (SAM) has emerged to become a popular foundation model in computer vision due to its impressive generalization to unseen data and tasks. Realizing flexible attacks on SAM is beneficial for understanding the robustness of SAM in the adversarial context. To this end, this work aims to achieve a targeted adversarial attack (TAA) on SAM. Specifically, under a certain prompt, the goal is to make the predicted mask of an adversarial example resemble that of a given target image. The task of TAA on SAM has been realized in a recent arXiv work in the white-box setup by assuming access to prompt and model, which is thus less practical. To address the issue of prompt dependence, we propose a simple yet effective approach by only attacking the image encoder. Moreover, we propose a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images. Extensive experiments verify the effectiveness of our proposed simple techniques to conduct a successful black-box TAA on SAM.},
	language = {en-US},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Zheng, Sheng and Zhang, Chaoning and Hao, Xinhong},
	month = feb,
	year = {2024},
	note = {GSCC: 0000007 
arXiv:2310.10010 [cs]
remark: 提出黑盒针对性对抗攻击SAM的方法。
TLDR: This work aims to achieve a targeted adversarial attack (TAA) on Segment Anything Model (SAM), and proposes a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gallegos_bias_2023,
	title = {Bias and {Fairness} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Bias and {Fairness} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.00770},
	doi = {10.48550/arXiv.2309.00770},
	abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
	language = {en},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
	month = sep,
	year = {2023},
	note = {GSCC: 0000338 
arXiv:2309.00770 [cs]
remark: 大语言模型偏见与公平性研究综述
TLDR: A comprehensive survey of bias evaluation and mitigation techniques for LLMs is presented, consolidating, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{yan_benchmarking_2023,
	title = {Benchmarking the {Robustness} of {LiDAR} {Semantic} {Segmentation} {Models}},
	url = {http://arxiv.org/abs/2301.00970},
	doi = {10.48550/arXiv.2301.00970},
	abstract = {When using LiDAR semantic segmentation models for safety-critical applications such as autonomous driving, it is essential to understand and improve their robustness with respect to a large range of LiDAR corruptions. In this paper, we aim to comprehensively analyze the robustness of LiDAR semantic segmentation models under various corruptions. To rigorously evaluate the robustness and generalizability of current approaches, we propose a new benchmark called SemanticKITTI-C, which features 16 out-of-domain LiDAR corruptions in three groups, namely adverse weather, measurement noise and cross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic segmentation models, especially spanning different input representations (e.g., point clouds, voxels, projected images, and etc.), network architectures and training schemes. Through this study, we obtain two insights: 1) We find out that the input representation plays a crucial role in robustness. Specifically, under specific corruptions, different representations perform variously. 2) Although state-of-the-art methods on LiDAR semantic segmentation achieve promising results on clean data, they are less robust when dealing with noisy data. Finally, based on the above observations, we design a robust LiDAR segmentation model (RLSeg) which greatly boosts the robustness with simple but effective modifications. It is promising that our benchmark, comprehensive analysis, and observations can boost future research in robust LiDAR semantic segmentation for safety-critical applications.},
	urldate = {2023-07-16},
	publisher = {arXiv},
	author = {Yan, Xu and Zheng, Chaoda and Li, Zhen and Cui, Shuguang and Dai, Dengxin},
	month = feb,
	year = {2023},
	note = {GSCC: 0000018 
arXiv:2301.00970 [cs]
Issue: arXiv:2301.00970
TLDR: A robust LiDAR segmentation model (RLSeg) is designed which greatly boosts the robustness ofLiDAR semantic segmentation models under various corruptions with simple but effective modifications.
remark: 提出衡量LiDAR语义分割模型鲁棒性的基准测试。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	doi = {10.48550/arXiv.1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	urldate = {2023-07-16},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {GSCC: 0003851 
arXiv:1903.12261 [cs, stat]
Issue: arXiv:1903.12261
remark: 建立图像分类器鲁棒性基准测试。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{loughry_basilisk_2024,
	title = {Basilisk: {Remote} {Code} {Execution} by {Laser} {Excitation} of \{{P}–{N}\} {Junctions} {Without} {Insider} {Assistance}},
	isbn = {978-1-939133-43-4},
	shorttitle = {Basilisk},
	url = {https://www.usenix.org/conference/woot24/presentation/loughry},
	language = {en},
	urldate = {2024-11-13},
	author = {Loughry, Joe and Rasmussen, Kasper},
	year = {2024},
	note = {GSCC: 0000000 
remark: 激光注入PN结从而修改代码},
	pages = {245--261},
}

@misc{zhang_avibench_2024,
	title = {{AVIBench}: {Towards} {Evaluating} the {Robustness} of {Large} {Vision}-{Language} {Model} on {Adversarial} {Visual}-{Instructions}},
	shorttitle = {{AVIBench}},
	url = {http://arxiv.org/abs/2403.09346},
	abstract = {Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs. The source code and benchmark will be made publicly available.},
	language = {en-US},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Zhang, Hao and Shao, Wenqi and Liu, Hong and Ma, Yongqiang and Luo, Ping and Qiao, Yu and Zhang, Kaipeng},
	month = mar,
	year = {2024},
	note = {GSCC: 0000017 
arXiv:2403.09346 [cs]
Issue: arXiv:2403.09346
remark: 评估视觉大模型对抗攻击鲁棒性框架。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shin_autoprompt_2020,
	title = {{AutoPrompt}: {Eliciting} {Knowledge} from {Language} {Models} with {Automatically} {Generated} {Prompts}},
	shorttitle = {{AutoPrompt}},
	url = {http://arxiv.org/abs/2010.15980},
	doi = {10.48550/arXiv.2010.15980},
	abstract = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
	language = {en},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
	month = nov,
	year = {2020},
	note = {GSCC: 0001809 
arXiv:2010.15980 [cs]
remark: 自动生成提示以揭示预训练语言模型知识。},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dai_automated_2024,
	title = {Automated {Creation} of {Digital} {Cousins} for {Robust} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2410.07408},
	doi = {10.48550/arXiv.2410.07408},
	abstract = {Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in digital twins, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.},
	language = {en-US},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Dai, Tianyuan and Wong, Josiah and Jiang, Yunfan and Wang, Chen and Gokmen, Cem and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li},
	month = oct,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2410.07408 [cs]
TLDR: It is found that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer.
remark: 数字堂兄简化虚拟环境生成与策略训练。},
	keywords = {Computer Science - Robotics},
}

@misc{guzhov_audioclip_2021,
	title = {{AudioCLIP}: {Extending} {CLIP} to {Image}, {Text} and {Audio}},
	shorttitle = {{AudioCLIP}},
	url = {http://arxiv.org/abs/2106.13043},
	doi = {10.48550/arXiv.2106.13043},
	abstract = {In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models. In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion. AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07\% on the UrbanSound8K and 97.15\% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets (68.78\% and 69.40\%, respectively). Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.},
	language = {en-US},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
	month = jun,
	year = {2021},
	note = {GSCC: 0000368 
arXiv:2106.13043 [cs]
remark: AudioCLIP扩展CLIP至音频，实现零样本分类。},
	keywords = {\# AudioCLIP, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{yi_audio_2023,
	title = {Audio {Deepfake} {Detection}: {A} {Survey}},
	shorttitle = {Audio {Deepfake} {Detection}},
	url = {http://arxiv.org/abs/2308.14970},
	doi = {10.48550/arXiv.2308.14970},
	abstract = {Audio deepfake detection is an emerging active topic. A growing number of literatures have aimed to study deepfake detection algorithms and achieved effective performance, the problem of which is far from being solved. Although there are some review literatures, there has been no comprehensive survey that provides researchers with a systematic overview of these developments with a unified evaluation. Accordingly, in this survey paper, we first highlight the key differences across various types of deepfake audio, then outline and analyse competitions, datasets, features, classifications, and evaluation of state-of-the-art approaches. For each aspect, the basic techniques, advanced developments and major challenges are discussed. In addition, we perform a unified comparison of representative features and classifiers on ASVspoof 2021, ADD 2023 and In-the-Wild datasets for audio deepfake detection, respectively. The survey shows that future research should address the lack of large scale datasets in the wild, poor generalization of existing detection methods to unknown fake attacks, as well as interpretability of detection results.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Yi, Jiangyan and Wang, Chenglong and Tao, Jianhua and Zhang, Xiaohui and Zhang, Chu Yuan and Zhao, Yan},
	month = aug,
	year = {2023},
	note = {GSCC: 0000057 
arXiv:2308.14970 [cs, eess]
remark: 音频深度伪造检测技术综述。
TLDR: The survey shows that future research should address the lack of large scale datasets in the wild, poor generalization of existing detection methods to unknown fake attacks, as well as interpretability of detection results.},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zhang_attack-sam_2023,
	title = {Attack-{SAM}: {Towards} {Attacking} {Segment} {Anything} {Model} {With} {Adversarial} {Examples}},
	shorttitle = {Attack-{SAM}},
	url = {http://arxiv.org/abs/2305.00866},
	doi = {10.48550/arXiv.2305.00866},
	abstract = {Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full white-box setting and transfer-based black-box settings. Beyond the basic goal of mask removal, we further investigate and find that it is possible to generate any desired mask by the adversarial attack.},
	language = {en-US},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Zhang, Chenshuang and Zhang, Chaoning and Kang, Taegoo and Kim, Donghun and Bae, Sung-Ho and Kweon, In So},
	month = may,
	year = {2023},
	note = {GSCC: 0000006 
arXiv:2305.00866 [cs]
remark: 对SAM模型进行对抗样本攻击研究。
TLDR: This work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples, and investigates the adversarial robustness of SAM in the full white-box setting and transfer-based black-box settings.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lukas_analyzing_2023,
	title = {Analyzing {Leakage} of {Personally} {Identifiable} {Information} in {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00539},
	doi = {10.48550/arXiv.2302.00539},
	abstract = {Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\${\textbackslash}times\$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3\% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing\_pii\_leakage.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-Béguelin, Santiago},
	month = apr,
	year = {2023},
	note = {GSCC: 0000190 
arXiv:2302.00539 [cs]
remark: 语言模型存在泄露个人身份信息的风险。},
	keywords = {Computer Science - Machine Learning},
}

@article{krishnamurthi_overview_2020,
	title = {An {Overview} of {IoT} {Sensor} {Data} {Processing}, {Fusion}, and {Analysis} {Techniques}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/21/6076},
	doi = {10.3390/s20216076},
	abstract = {In the recent era of the Internet of Things, the dominant role of sensors and the Internet provides a solution to a wide variety of real-life problems. Such applications include smart city, smart healthcare systems, smart building, smart transport and smart environment. However, the real-time IoT sensor data include several challenges, such as a deluge of unclean sensor data and a high resource-consumption cost. As such, this paper addresses how to process IoT sensor data, fusion with other data sources, and analyses to produce knowledgeable insight into hidden data patterns for rapid decision-making. This paper addresses the data processing techniques such as data denoising, data outlier detection, missing data imputation and data aggregation. Further, it elaborates on the necessity of data fusion and various data fusion methods such as direct fusion, associated feature extraction, and identity declaration data fusion. This paper also aims to address data analysis integration with emerging technologies, such as cloud computing, fog computing and edge computing, towards various challenges in IoT sensor network and sensor data analysis. In summary, this paper is the first of its kind to present a complete overview of IoT sensor data processing, fusion and analysis techniques.},
	language = {en},
	number = {21},
	urldate = {2024-03-21},
	journal = {Sensors},
	author = {Krishnamurthi, Rajalakshmi and Kumar, Adarsh and Gopinathan, Dhanalekshmi and Nayyar, Anand and Qureshi, Basit},
	month = oct,
	year = {2020},
	note = {GSCC: 0000354 
remark: IoT传感器数据处理与分析综述。
TLDR: This paper addresses the data processing techniques such as data denoising, data outlier detection, missing data imputation and data aggregation, and elaborates on the necessity of data fusion and various data fusion methods such as direct fusion, associated feature extraction, and identity declaration data fusion.},
	pages = {6076},
}

@inproceedings{sarabi_llm-based_2023,
	address = {New York, NY, USA},
	series = {{IMC} '23},
	title = {An {LLM}-based {Framework} for {Fingerprinting} {Internet}-connected {Devices}},
	isbn = {979-8-4007-0382-9},
	url = {https://dl.acm.org/doi/10.1145/3618257.3624845},
	doi = {10.1145/3618257.3624845},
	abstract = {In this paper we propose the use of large language models (LLMs) for characterizing, clustering, and fingerprinting raw text obtained from network measurements. To this end, We first train a transformer-based masked language model, namely RoBERTa, on a dataset containing hundreds of millions of banners obtained from Internet-wide scans. We further fine-tune this model using a contrastive loss function (driven by domain knowledge) to produce temporally stable numerical representations (embeddings) that can be used out-of-the-box for downstream learning tasks. Our embeddings are robust, resilient to small random changes in the content of a banner, and maintain proximity between embeddings of similar hardware/software products. We further cluster HTTP banners using a density-based approach (HDBSCAN), and examine the obtained clusters to generate text-based fingerprints for the purpose of labeling raw scan data. We compare our fingerprints to Recog, an existing database of manually curated fingerprints, and show that we can identify new IoT devices and server products that were not previously captured by Recog. Our proposed methodology poses an important direction for future research by utilizing state-of-the-art language models to automatically analyze, interpret, and label the large amounts of data generated by Internet scans.},
	urldate = {2023-11-20},
	booktitle = {Proceedings of the 2023 {ACM} on {Internet} {Measurement} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Sarabi, Armin and Yin, Tongxin and Liu, Mingyan},
	year = {2023},
	note = {GSCC: 0000013 
remark: 利用LLMs对网络设备进行指纹识别。
TLDR: This paper first trains a transformer-based masked language model, namely RoBERTa, on a dataset containing hundreds of millions of banners obtained from Internet-wide scans to produce temporally stable numerical representations (embeddings) that can be used out-of-the-box for downstream learning tasks.},
	keywords = {deep learning, device fingerprinting, internet scanning, large language models},
	pages = {478--484},
}

@misc{deng_ai_2024,
	title = {{AI} {Agents} {Under} {Threat}: {A} {Survey} of {Key} {Security} {Challenges} and {Future} {Pathways}},
	shorttitle = {{AI} {Agents} {Under} {Threat}},
	url = {http://arxiv.org/abs/2406.02630},
	doi = {10.48550/arXiv.2406.02630},
	abstract = {An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this paper highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.},
	language = {en-US},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Deng, Zehang and Guo, Yongjian and Han, Changzhou and Ma, Wanlun and Xiong, Junwu and Wen, Sheng and Xiang, Yang},
	month = jun,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2406.02630 [cs]
remark: 探讨AI代理面临的安全威胁及其研究进展。
TLDR: This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{paulus_advprompter_2024,
	title = {{AdvPrompter}: {Fast} {Adaptive} {Adversarial} {Prompting} for {LLMs}},
	shorttitle = {{AdvPrompter}},
	url = {http://arxiv.org/abs/2404.16873},
	doi = {10.48550/arXiv.2404.16873},
	abstract = {While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, \${\textbackslash}sim800{\textbackslash}times\$ faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.},
	language = {en-US},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Paulus, Anselm and Zharmagambetov, Arman and Guo, Chuan and Amos, Brandon and Tian, Yuandong},
	month = apr,
	year = {2024},
	note = {GSCC: 0000051 
arXiv:2404.16873 [cs]
version: 1
remark: 使用LLM生成快速对抗提示提升防御攻击。
TLDR: This paper presents a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, faster than existing optimization-based approaches, and demonstrates that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{cao_adversarial_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Adversarial {Sensor} {Attack} on {LiDAR}-based {Perception} in {Autonomous} {Driving}},
	isbn = {978-1-4503-6747-9},
	url = {https://dl.acm.org/doi/10.1145/3319535.3339815},
	doi = {10.1145/3319535.3339815},
	abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75\%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
	year = {2019},
	note = {GSCC: 0000635 
TLDR: This work performs the first security study of LiDAR-based perception in AV settings, and designs an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75\%.
remark: 对基于激光雷达感知的攻击方法研究。},
	keywords = {adversarial machine learning, autonomous driving, sensor attack},
	pages = {2267--2281},
}

@misc{zhang_adversarial_2024,
	title = {Adversarial {Illusions} in {Multi}-{Modal} {Embeddings}},
	url = {http://arxiv.org/abs/2308.11804},
	doi = {10.48550/arXiv.2308.11804},
	abstract = {Multi-modal embeddings encode texts, images, thermal images, sounds, and videos into a single embedding space, aligning representations across different modalities (e.g., associate an image of a dog with a barking sound). In this paper, we show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an image or a sound, an adversary can perturb it to make its embedding close to an arbitrary, adversary-chosen input in another modality. These attacks are cross-modal and targeted: the adversary can align any image or sound with any target of his choice. Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks and modalities, enabling a wholesale compromise of current and future tasks, as well as modalities not available to the adversary. Using ImageBind and AudioCLIP embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, zero-shot classification, and audio retrieval. We investigate transferability of illusions across different embeddings and develop a black-box version of our method that we use to demonstrate the first adversarial alignment attack on Amazon's commercial, proprietary Titan embedding. Finally, we analyze countermeasures and evasion attacks.},
	language = {en-US},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Zhang, Tingwei and Jha, Rishi and Bagdasaryan, Eugene},
	month = jun,
	year = {2024},
	note = {GSCC: 0000008 
arXiv:2308.11804 [cs]
TLDR: Using ImageBind embeddings, it is demonstrated how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
remark: 多模态嵌入易受跨模态对抗攻击。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{li_adv3d_2024,
	title = {{Adv3D}: {Generating} {3D} {Adversarial} {Examples} for {3D} {Object} {Detection} in {Driving} {Scenarios} with {NeRF}},
	shorttitle = {{Adv3D}},
	url = {http://arxiv.org/abs/2309.01351},
	doi = {10.48550/arXiv.2309.01351},
	abstract = {Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation. Project page: https://len-li.github.io/adv3d-web},
	language = {en-US},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Li, Leheng and Lian, Qing and Chen, Ying-Cong},
	month = aug,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2309.01351 [cs]
remark: 生成用于自动驾驶攻击的3D对抗样本。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⏳},
}

@misc{ma_actra_2024,
	title = {Actra: {Optimized} {Transformer} {Architecture} for {Vision}-{Language}-{Action} {Models} in {Robot} {Learning}},
	shorttitle = {Actra},
	url = {http://arxiv.org/abs/2408.01147},
	doi = {10.48550/arXiv.2408.01147},
	abstract = {Vision-language-action models have gained significant attention for their ability to model trajectories in robot learning. However, most existing models rely on Transformer models with vanilla causal attention, which we find suboptimal for processing segmented multi-modal sequences. Additionally, the autoregressive generation approach falls short in generating multi-dimensional actions. In this paper, we introduce Actra, an optimized Transformer architecture featuring trajectory attention and learnable action queries, designed for effective encoding and decoding of segmented vision-language-action trajectories in robot imitation learning. Furthermore, we devise a multi-modal contrastive learning objective to explicitly align different modalities, complementing the primary behavior cloning objective. Through extensive experiments conducted across various environments, Actra exhibits substantial performance improvement when compared to state-of-the-art models in terms of generalizability, dexterity, and precision.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Ma, Yueen and Chi, Dafeng and Wu, Shiguang and Liu, Yuecheng and Zhuang, Yuzheng and Hao, Jianye and King, Irwin},
	month = aug,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2408.01147
TLDR: Actra is introduced, an optimized Transformer architecture featuring trajectory attention and learnable action queries, designed for effective encoding and decoding of segmented vision-language-action trajectories in robot imitation learning and devise a multi-modal contrastive learning objective to explicitly align different modalities.
remark: 提出Actra模型提升机器人视觉语言动作学习。},
	keywords = {Computer Science - Robotics},
}

@article{fei_systematic_2023,
	title = {A {Systematic} {Review} of {IoT} {Security}: {Research} {Potential}, {Challenges}, and {Future} {Directions}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {A {Systematic} {Review} of {IoT} {Security}},
	url = {https://dl.acm.org/doi/10.1145/3625094},
	doi = {10.1145/3625094},
	abstract = {The Internet of Things (IoT) encompasses a network of physical objects embedded with sensors, software, and data processing technologies that can establish connections and exchange data with other devices and systems via the Internet. IoT devices are incorporated into various products, ranging from ordinary household items to complex industrial appliances. Despite the increasing demand for IoT, security concerns have impeded its development. This article systematically reviews IoT security research, focusing on vulnerabilities, challenges, technologies, and future directions. It surveys 171 recent publications in the field, providing a comprehensive discussion on the development status, challenges, and solutions in IoT. The article outlines IoT architecture patterns and typical features, evaluates existing limitations, and explores strategies for enhancing IoT security. Additionally, the article delves into known IoT attacks and discusses the security countermeasures and mechanisms to address these challenges. It explores the functional requirements of IoT security and explores related technologies and standards. Finally, the article discusses potential future research directions in IoT security.},
	number = {5},
	urldate = {2023-12-21},
	journal = {ACM Computing Surveys},
	author = {Fei, Wen and Ohno, Hiroyuki and Sampalli, Srinivas},
	year = {2023},
	note = {GSCC: 0000016 
remark: IoT安全研究的系统回顾与展望。},
	keywords = {Blockchain, Cloud Computing, Edge Computing, Internet of Things (IoT), IoT architecture, IoT security, IoT security challenges, IoT security goals, IoT security technology, IoT vulnerabilities, Machine Learning (ML)},
	pages = {111:1--111:40},
}

@article{yan_survey_2022,
	title = {A {Survey} on {Voice} {Assistant} {Security}: {Attacks} and {Countermeasures}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {A {Survey} on {Voice} {Assistant} {Security}},
	url = {https://dl.acm.org/doi/10.1145/3527153},
	doi = {10.1145/3527153},
	abstract = {Voice assistants (VA) have become prevalent on a wide range of personal devices such as smartphones and smart speakers. As companies build voice assistants with extra functionalities, attacks that trick a voice assistant into performing malicious behaviors can pose a significant threat to a user’s security, privacy, and even safety. However, the diverse attacks and stand-alone defenses in the literature often lack a systematic perspective, making it challenging for designers to properly identify, understand, and mitigate the security threats against voice assistants. To overcome this problem, this article provides a thorough survey of the attacks and countermeasures for voice assistants. We systematize a broad category of relevant but seemingly unrelated attacks by the vulnerable system components and attack methods, and categorize existing countermeasures based on the defensive strategies from a system designer’s perspective. To assist designers in planning defense based on their demands, we provide a qualitative comparison of existing countermeasures by the implementation cost, usability, and security and propose practical suggestions. We envision this work can help build more reliability into voice assistants and promote research in this fast-evolving area.},
	language = {en-US},
	number = {4},
	urldate = {2023-12-21},
	journal = {ACM Computing Surveys},
	author = {Yan, Chen and Ji, Xiaoyu and Wang, Kai and Jiang, Qinhong and Jin, Zizhi and Xu, Wenyuan},
	year = {2022},
	note = {GSCC: 0000063 
remark: 语音助手的安全攻防综述。},
	keywords = {Voice assistant, attack, defense, security, speech, voice interaction},
	pages = {84:1--84:36},
}

@misc{zhang_survey_2023,
	title = {A {Survey} on {Segment} {Anything} {Model} ({SAM}): {Vision} {Foundation} {Model} {Meets} {Prompt} {Engineering}},
	shorttitle = {A {Survey} on {Segment} {Anything} {Model} ({SAM})},
	url = {http://arxiv.org/abs/2306.06211},
	doi = {10.48550/arXiv.2306.06211},
	abstract = {Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Zhang, Chaoning and Puspitasari, Fachrina Dewi and Zheng, Sheng and Li, Chenghao and Qiao, Yu and Kang, Taegoo and Shan, Xinru and Zhang, Chenshuang and Qin, Caiyan and Rameau, Francois and Lee, Lik-Hang and Bae, Sung-Ho and Hong, Choong Seon},
	month = jul,
	year = {2023},
	note = {GSCC: 0000066 
arXiv:2306.06211 [cs]
remark: 综述SAM模型的性能及应用。
TLDR: This survey provides a comprehensive exploration of the SAM family, including SAM and SAM 2, highlighting their advancements in granularity and contextual understanding and suggests future research directions, including domain-specific adaptations and enhanced memory and propagation mechanisms.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yin_survey_2024,
	title = {A {Survey} on {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.13549},
	doi = {10.48550/arXiv.2306.13549},
	abstract = {Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	month = apr,
	year = {2024},
	note = {GSCC: 0000961 
arXiv:2306.13549
TLDR: The basic formulation of MLLM is presented and its related concepts, including architecture, training strategy and data, as well as evaluation are delineated, and research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios are introduced.
remark: 综述多模态大模型的进展与挑战。},
	keywords = {\# 结构 训练 数据 评估, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yao_survey_2024,
	title = {A {Survey} on {Large} {Language} {Model} ({LLM}) {Security} and {Privacy}: {The} {Good}, the {Bad}, and the {Ugly}},
	shorttitle = {A {Survey} on {Large} {Language} {Model} ({LLM}) {Security} and {Privacy}},
	url = {http://arxiv.org/abs/2312.02003},
	doi = {10.48550/arXiv.2312.02003},
	abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.},
	language = {en},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
	month = jan,
	year = {2024},
	note = {GSCC: 0000410 
arXiv:2312.02003 [cs]
remark: 探讨LLMs在安全隐私领域的影响。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{bansal_survey_2020,
	title = {A {Survey} on {IoT} {Big} {Data}: {Current} {Status}, 13 {V}’s {Challenges}, and {Future} {Directions}},
	volume = {53},
	issn = {0360-0300},
	shorttitle = {A {Survey} on {IoT} {Big} {Data}},
	url = {https://dl.acm.org/doi/10.1145/3419634},
	doi = {10.1145/3419634},
	abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
	number = {6},
	urldate = {2023-12-21},
	journal = {ACM Computing Surveys},
	author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobhán},
	year = {2020},
	note = {GSCC: 0000118 
remark: IoT大数据挑战与未来研究方向。},
	keywords = {IoT big data, IoT big data survey, V’s challenges for IoT big data, big data 2.0, cloud IoT services, cloud computing in IoT},
	pages = {131:1--131:59},
}

@article{lin_survey_2017,
	title = {A {Survey} on {Internet} of {Things}: {Architecture}, {Enabling} {Technologies}, {Security} and {Privacy}, and {Applications}},
	volume = {4},
	issn = {2327-4662},
	shorttitle = {A {Survey} on {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/7879243},
	doi = {10.1109/JIOT.2017.2683200},
	abstract = {Fog/edge computing has been proposed to be integrated with Internet of Things (IoT) to enable computing services devices deployed at network edge, aiming to improve the user's experience and resilience of the services in case of failures. With the advantage of distributed architecture and close to end-users, fog/edge computing can provide faster response and greater quality of service for IoT applications. Thus, fog/edge computing-based IoT becomes future infrastructure on IoT development. To develop fog/edge computing-based IoT infrastructure, the architecture, enabling techniques, and issues related to IoT should be investigated first, and then the integration of fog/edge computing and IoT should be explored. To this end, this paper conducts a comprehensive overview of IoT with respect to system architecture, enabling technologies, security and privacy issues, and present the integration of fog/edge computing and IoT, and applications. Particularly, this paper first explores the relationship between cyber-physical systems and IoT, both of which play important roles in realizing an intelligent cyber-physical world. Then, existing architectures, enabling technologies, and security and privacy issues in IoT are presented to enhance the understanding of the state of the art IoT development. To investigate the fog/edge computing-based IoT, this paper also investigate the relationship between IoT and fog/edge computing, and discuss issues in fog/edge computing-based IoT. Finally, several applications, including the smart grid, smart transportation, and smart cities, are presented to demonstrate how fog/edge computing-based IoT to be implemented in real-world applications.},
	language = {en},
	number = {5},
	urldate = {2024-02-24},
	journal = {IEEE Internet of Things Journal},
	author = {Lin, Jie and Yu, Wei and Zhang, Nan and Yang, Xinyu and Zhang, Hanlin and Zhao, Wei},
	month = oct,
	year = {2017},
	note = {GSCC: 0003242 
Conference Name: IEEE Internet of Things Journal
remark: 探讨基于雾/边缘计算的IoT架构及应用。
TLDR: The relationship between cyber-physical systems and IoT, both of which play important roles in realizing an intelligent cyber- physical world, are explored and existing architectures, enabling technologies, and security and privacy issues in IoT are presented to enhance the understanding of the state of the art IoT development.},
	keywords = {Applications, Computer architecture, Internet of Things, Privacy, Security, Smart cities, Smart grids, Smart transportation, enabling technologies, fog/edge computing, internet of Things (IoT), security and privacy},
	pages = {1125--1142},
}

@misc{kim_survey_2024,
	title = {A {Survey} on {Integration} of {Large} {Language} {Models} with {Intelligent} {Robots}},
	url = {http://arxiv.org/abs/2404.09228},
	abstract = {In recent years, the integration of large language models (LLMs) has revolutionized the field of robotics, enabling robots to communicate, understand, and reason with human-like proficiency. This paper explores the multifaceted impact of LLMs on robotics, addressing key challenges and opportunities for leveraging these models across various domains. By categorizing and analyzing LLM applications within core robotics elements -- communication, perception, planning, and control -- we aim to provide actionable insights for researchers seeking to integrate LLMs into their robotic systems. Our investigation focuses on LLMs developed post-GPT-3.5, primarily in text-based modalities while also considering multimodal approaches for perception and control. We offer comprehensive guidelines and examples for prompt engineering, facilitating beginners' access to LLM-based robotics solutions. Through tutorial-level examples and structured prompt construction, we illustrate how LLM-guided enhancements can be seamlessly integrated into robotics applications. This survey serves as a roadmap for researchers navigating the evolving landscape of LLM-driven robotics, offering a comprehensive overview and practical guidance for harnessing the power of language models in robotics development.},
	language = {en-US},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Kim, Yeseung and Kim, Dohyun and Choi, Jieun and Park, Jisang and Oh, Nayoung and Park, Daehyung},
	month = apr,
	year = {2024},
	note = {GSCC: 0000012 
arXiv:2404.09228 [cs]
Issue: arXiv:2404.09228
remark: 大语言模型在智能机器人中的应用综述。},
	keywords = {Computer Science - Robotics},
}

@misc{huang_survey_2023,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.05232},
	doi = {10.48550/arXiv.2311.05232},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
	language = {en-US},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = nov,
	year = {2023},
	note = {GSCC: 0000689 
arXiv:2311.05232 [cs]
remark: 大语言模型幻觉问题的综述研究。},
	keywords = {Computer Science - Computation and Language},
}

@misc{huang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.15769},
	doi = {10.48550/arXiv.2408.15769},
	abstract = {Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Huang, Jiaxing and Zhang, Jingyi},
	month = aug,
	year = {2024},
	note = {GSCC: 0000010 
arXiv:2408.15769
TLDR: This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: the background of MLLMs and their evaluation, and the background of MLLM evaluation steps and metrics.
remark: 多模态大模型的评估方法综述。},
	keywords = {\# 数据集, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	language = {en},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = dec,
	year = {2023},
	note = {GSCC: 0001800 
arXiv:2307.03109 [cs]
remark: 大语言模型评估方法综述},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{roh_survey_2019,
	title = {A {Survey} on {Data} {Collection} for {Machine} {Learning}: a {Big} {Data} -- {AI} {Integration} {Perspective}},
	shorttitle = {A {Survey} on {Data} {Collection} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.03402},
	doi = {10.48550/arXiv.1811.03402},
	abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.},
	language = {en},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
	month = aug,
	year = {2019},
	note = {GSCC: 0001166 
arXiv:1811.03402 [cs, stat]
remark: 数据收集在机器学习中的研究现状。},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_survey_2024,
	title = {A {Survey} on {Benchmarks} of {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.08632},
	doi = {10.48550/arXiv.2408.08632},
	abstract = {Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. For more details, please visit our GitHub repository: https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Li, Jian and Lu, Weiheng and Fei, Hao and Luo, Meng and Dai, Ming and Xia, Min and Jin, Yizhang and Gan, Zhenye and Qi, Ding and Fu, Chaoyou and Tai, Ying and Yang, Wankou and Wang, Yabiao and Wang, Chengjie},
	month = sep,
	year = {2024},
	note = {GSCC: 0000006 
arXiv:2408.08632
TLDR: A comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on perception and understanding, recognition and reasoning, specific domains, key capabilities, and other modalities, concludes that evaluation should be regarded as a crucial discipline to support the development of MLLMs better.
remark: 综述多模态大模型基准与评价方法。},
	keywords = {\# 数据集, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_survey_2023-1,
	title = {A {Survey} of {Safety} and {Trustworthiness} of {Large} {Language} {Models} through the {Lens} of {Verification} and {Validation}},
	url = {http://arxiv.org/abs/2305.11391},
	doi = {10.48550/arXiv.2305.11391},
	abstract = {Large Language Models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V\&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V\&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.},
	language = {en-US},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and Cai, Kaiwen and Zhang, Yanghao and Wu, Sihao and Xu, Peipei and Wu, Dengyu and Freitas, Andre and Mustafa, Mustafa A.},
	month = aug,
	year = {2023},
	note = {GSCC: 0000083 
arXiv:2305.11391 [cs]
remark: 审视LLMs的安全性与可信度验证。
TLDR: If and how the Verification and Validation techniques, which have been widely developed for traditional software and deep learning models as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications is considered.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bai_survey_2024,
	title = {A {Survey} of {Multimodal} {Large} {Language} {Model} from {A} {Data}-centric {Perspective}},
	url = {http://arxiv.org/abs/2405.16640},
	doi = {10.48550/arXiv.2405.16640},
	abstract = {Multimodal large language models (MLLMs) enhance the capabilities of standard large language models by integrating and processing data from multiple modalities, including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for the datasets and review the benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.},
	language = {en-US},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Bai, Tianyi and Liang, Hao and Wan, Binwang and Xu, Yanran and Li, Xi and Li, Shiyu and Yang, Ling and Li, Bozhou and Wang, Yifan and Cui, Bin and Huang, Ping and Shan, Jiulong and He, Conghui and Yuan, Binhang and Zhang, Wentao},
	month = jul,
	year = {2024},
	note = {GSCC: 0000027 
arXiv:2405.16640
TLDR: This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.
remark: 多模态大模型的数据准备与评估综述。MLLM 训练的数据收集、处理和选择的主要阶段、MLLM 的数据评估方法和现有评估数据集。},
	keywords = {\# 数据集, /done, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	language = {en},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = nov,
	year = {2023},
	note = {GSCC: 0003168 
arXiv:2303.18223 [cs]
remark: 大语言模型综述
TLDR: A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zheng_survey_2024,
	title = {A {Survey} of {Embodied} {Learning} for {Object}-{Centric} {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2408.11537},
	doi = {10.48550/arXiv.2408.11537},
	abstract = {Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM\_survey.},
	language = {en-US},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Zheng, Ying and Yao, Lei and Su, Yuejiao and Zhang, Yi and Wang, Yi and Zhao, Sicheng and Zhang, Yiyi and Chau, Lap-Pui},
	month = aug,
	year = {2024},
	note = {GSCC: 0000001 
arXiv:2408.11537 [cs]
物体中心机器人操作的具身学习综述。
TLDR: A comprehensive survey of the latest advancements in embodied learning for object-centric robotic manipulation and categorize the existing work into three main branches, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation.
remark: 体感学习是机器人操控领域的重要研究方向。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{duan_survey_2022,
	title = {A {Survey} of {Embodied} {AI}: {From} {Simulators} to {Research} {Tasks}},
	shorttitle = {A {Survey} of {Embodied} {AI}},
	url = {http://arxiv.org/abs/2103.04918},
	doi = {10.48550/arXiv.2103.04918},
	abstract = {There has been an emerging paradigm shift from the era of "internet AI" to "embodied AI", where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI -- visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field.},
	language = {en-US},
	urldate = {2024-01-12},
	publisher = {arXiv},
	author = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
	month = jan,
	year = {2022},
	note = {GSCC: 0000291 
arXiv:2103.04918 [cs]
Issue: arXiv:2103.04918
remark: 对化身AI模拟器与研究任务的综述。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{liu_survey_2024,
	title = {A {Survey} of {Attacks} on {Large} {Vision}-{Language} {Models}: {Resources}, {Advances}, and {Future} {Trends}},
	shorttitle = {A {Survey} of {Attacks} on {Large} {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2407.07403},
	doi = {10.48550/arXiv.2407.07403},
	abstract = {With the significant development of large models in recent years, Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding and reasoning tasks. Compared to traditional Large Language Models (LLMs), LVLMs present great potential and challenges due to its closer proximity to the multi-resource real-world applications and the complexity of multi-modal processing. However, the vulnerability of LVLMs is relatively underexplored, posing potential security risks in daily usage. In this paper, we provide a comprehensive review of the various forms of existing LVLM attacks. Specifically, we first introduce the background of attacks targeting LVLMs, including the attack preliminary, attack challenges, and attack resources. Then, we systematically review the development of LVLM attack methods, such as adversarial attacks that manipulate model outputs, jailbreak attacks that exploit model vulnerabilities for unauthorized actions, prompt injection attacks that engineer the prompt type and pattern, and data poisoning that affects model training. Finally, we discuss promising research directions in the future. We believe that our survey provides insights into the current landscape of LVLM vulnerabilities, inspiring more researchers to explore and mitigate potential safety issues in LVLM developments. The latest papers on LVLM attacks are continuously collected in https://github.com/liudaizong/Awesome-LVLM-Attack.},
	language = {en-US},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Liu, Daizong and Yang, Mingyu and Qu, Xiaoye and Zhou, Pan and Cheng, Yu and Hu, Wei},
	month = jul,
	year = {2024},
	note = {GSCC: 0000015 
arXiv:2407.07403
TLDR: This paper provides a comprehensive review of the various forms of existing LVLM attacks, and systematically review the development of LVLM attack methods, such as adversarial attacks that manipulate model outputs, jailbreak attacks that exploit model vulnerabilities for unauthorized actions, and data poisoning that affects model training.
remark: 综述大规模视觉语言模型攻击现状与未来趋势。},
	keywords = {\# VLM Attack, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{longpre_pretrainers_2023,
	title = {A {Pretrainer}'s {Guide} to {Training} {Data}: {Measuring} the {Effects} of {Data} {Age}, {Domain} {Coverage}, {Quality}, \& {Toxicity}},
	shorttitle = {A {Pretrainer}'s {Guide} to {Training} {Data}},
	url = {http://arxiv.org/abs/2305.13169},
	doi = {10.48550/arXiv.2305.13169},
	abstract = {Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.},
	language = {en},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and Ippolito, Daphne},
	month = nov,
	year = {2023},
	note = {GSCC: 0000108 
arXiv:2305.13169 [cs]
remark: 数据特性（时间、毒性、质量、领域）对预训练影响。
TLDR: These experiments constitute the single largest publicly documented empirical study of the effects of pretraining data, and validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development.},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lien_phenomenological_2024,
	title = {A {Phenomenological} {AI} {Foundation} {Model} for {Physical} {Signals}},
	url = {http://arxiv.org/abs/2410.14724},
	doi = {10.48550/arXiv.2410.14724},
	abstract = {The objective of this work is to develop an AI foundation model for physical signals that can generalize across diverse phenomena, domains, applications, and sensing apparatuses. We propose a phenomenological approach and framework for creating and validating such AI foundation models. Based on this framework, we developed and trained a model on 0.59 billion samples of cross-modal sensor measurements, ranging from electrical current to fluid flow to optical sensors. Notably, no prior knowledge of physical laws or inductive biases were introduced into the model. Through several real-world experiments, we demonstrate that a single foundation model could effectively encode and predict physical behaviors, such as mechanical motion and thermodynamics, including phenomena not seen in training. The model also scales across physical processes of varying complexity, from tracking the trajectory of a simple spring-mass system to forecasting large electrical grid dynamics. This work highlights the potential of building a unified AI foundation model for diverse physical world processes.},
	language = {en-US},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Lien, Jaime and Olascoaga, Laura I. Galindez and Dogan, Hasan and Gillian, Nicholas and Barbello, Brandon and Giusti, Leonardo and Poupyrev, Ivan},
	month = oct,
	year = {2024},
	note = {GSCC: 0000000 
arXiv:2410.14724
remark: 提出适用于多物理信号的AI基础模型牛顿。
TLDR: This work develops and trained a model that could effectively encode and predict physical behaviors, such as mechanical motion and thermodynamics, including phenomena not seen in training, and proposes a phenomenological approach and framework for creating and validating such AI foundation models.},
	keywords = {\# 牛顿模型, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{mao_language_2023,
	title = {A {Language} {Agent} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2311.10813},
	doi = {10.48550/arXiv.2311.10813},
	abstract = {Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods. Code will be released.},
	language = {en-US},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Mao, Jiageng and Ye, Junjie and Qian, Yuxi and Pavone, Marco and Wang, Yue},
	month = nov,
	year = {2023},
	note = {GSCC: 0000060 
arXiv:2311.10813 [cs]
remark: 利用LLM实现更人性化的自动驾驶。
TLDR: This paper proposes a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems, thus enabling a more nuanced, human-like approach to autonomous driving.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{niu_comprehensive_2024,
	title = {A {Comprehensive} {Survey} of {Cross}-{Domain} {Policy} {Transfer} for {Embodied} {Agents}},
	url = {http://arxiv.org/abs/2402.04580},
	doi = {10.48550/arXiv.2402.04580},
	abstract = {The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems. Lastly, we summarize the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field.},
	language = {en-US},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Niu, Haoyi and Hu, Jianming and Zhou, Guyue and Zhan, Xianyuan},
	month = aug,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2402.04580
TLDR: A systematic review of existing cross-domain policy transfer methods is conducted and a nuanced categorization of domain gaps is encapsulated, encapsulating the overarching insights and design considerations of each problem setting.
remark: 跨域策略转移方法综述与挑战分析。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{qian20223d,
	title = {{3D} {Object} {Detection} for {Autonomous} {Driving}: {A} {Survey}},
	volume = {130},
	issn = {00313203},
	shorttitle = {{3D} {Object} {Detection} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2106.10823},
	doi = {10.1016/j.patcog.2022.108796},
	abstract = {Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.},
	language = {en-US},
	urldate = {2023-03-06},
	journal = {Pattern Recognition},
	author = {Qian, Rui and Lai, Xin and Li, Xirong},
	month = oct,
	year = {2022},
	note = {GSCC: 0000338 
arXiv:2106.10823 [cs]
TLDR: A comprehensive survey of 3D object detection for autonomous driving, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons.
remark: 自动驾驶3D目标检测现状综述与未来展望。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {108796},
}

@misc{mao_3d_2022,
	title = {{3D} {Object} {Detection} for {Autonomous} {Driving}: {A} {Review} and {New} {Outlooks}},
	shorttitle = {{3D} {Object} {Detection} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2206.09474},
	doi = {10.48550/arXiv.2206.09474},
	abstract = {Autonomous driving, in recent years, has been receiving increasing attention for its potential to relieve drivers' burdens and improve the safety of driving. In modern autonomous driving pipelines, the perception system is an indispensable component, aiming to accurately estimate the status of surrounding environments and provide reliable observations for prediction and planning. 3D object detection, which intelligently predicts the locations, sizes, and categories of the critical 3D objects near an autonomous vehicle, is an important part of a perception system. This paper reviews the advances in 3D object detection for autonomous driving. First, we introduce the background of 3D object detection and discuss the challenges in this task. Second, we conduct a comprehensive survey of the progress in 3D object detection from the aspects of models and sensory inputs, including LiDAR-based, camera-based, and multi-modal detection approaches. We also provide an in-depth analysis of the potentials and challenges in each category of methods. Additionally, we systematically investigate the applications of 3D object detection in driving systems. Finally, we conduct a performance analysis of the 3D object detection approaches, and we further summarize the research trends over the years and prospect the future directions of this area.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Mao, Jiageng and Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	month = jun,
	year = {2022},
	note = {GSCC: 0000103 
arXiv:2206.09474 [cs]
Issue: arXiv:2206.09474
TLDR: This paper conducts a comprehensive survey of the progress in 3D object detection from the aspects of models and sensory inputs, including LiDAR-based, camera- based, and multi-modal detection approaches, and provides an in-depth analysis of the potentials and challenges in each category of methods.
remark: 自动驾驶3D对象检测综述与展望。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_voxact-b_2024,
	title = {{VoxAct}-{B}: {Voxel}-{Based} {Acting} and {Stabilizing} {Policy} for {Bimanual} {Manipulation}},
	shorttitle = {{VoxAct}-{B}},
	url = {https://arxiv.org/abs/2407.04152v2},
	abstract = {Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world \${\textbackslash}texttt\{Open Drawer\}\$ and \${\textbackslash}texttt\{Open Jar\}\$ tasks using two UR5s. Code, data, and videos are available at https://voxact-b.github.io.},
	language = {en},
	urldate = {2024-10-08},
	journal = {arXiv.org},
	author = {Liu, I.-Chun Arthur and He, Sicheng and Seita, Daniel and Sukhatme, Gaurav},
	month = jul,
	year = {2024},
	note = {GSCC: 0000004 
remark: 提出VoxAct-B方法提高双臂操作效率。},
	keywords = {\# VoxAct-B},
}

@misc{li_vision-language_2024,
	title = {Vision-{Language} {Foundation} {Models} as {Effective} {Robot} {Imitators}},
	url = {http://arxiv.org/abs/2311.01378},
	doi = {10.48550/arXiv.2311.01378},
	abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
	language = {en-US},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
	month = feb,
	year = {2024},
	note = {GSCC: 0000085 
arXiv:2311.01378 [cs]
Issue: arXiv:2311.01378
remark: 利用视觉语言模型进行机器人操作的框架RoboFlamingo。
TLDR: RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
	keywords = {\# RoboFlamingo, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jiang_vima_2023,
	title = {{VIMA}: {General} {Robot} {Manipulation} with {Multimodal} {Prompts}},
	shorttitle = {{VIMA}},
	url = {http://arxiv.org/abs/2210.03094},
	doi = {10.48550/arXiv.2210.03094},
	abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to \$2.9{\textbackslash}times\$ task success rate given the same training data. With \$10{\textbackslash}times\$ less training data, VIMA still performs \$2.7{\textbackslash}times\$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
	month = may,
	year = {2023},
	note = {GSCC: 0000196 
arXiv:2210.03094 [cs]
remark: 多模态机器人操控模型VIMA，实现任务广泛泛化。
TLDR: It is shown that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens, and designed a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.},
	keywords = {\# VIMA, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mullen_jr_towards_2024,
	title = {Towards {Robots} {That} {Know} {When} {They} {Need} {Help}: {Affordance}-{Based} {Uncertainty} for {Large} {Language} {Model} {Planners}},
	shorttitle = {Towards {Robots} {That} {Know} {When} {They} {Need} {Help}},
	url = {http://arxiv.org/abs/2403.13198},
	abstract = {Large language models (LLMs) showcase many desirable traits for intelligent and helpful robots. However, they are also known to hallucinate predictions. This issue is exacerbated in consumer robotics where LLM hallucinations may result in robots confidently executing plans that are contrary to user goals, relying more frequently on human assistance, or preventing the robot from asking for help at all. In this work, we present LAP, a novel approach for utilizing off-the-shelf LLM's, alongside scene and object Affordances, in robotic Planners that minimize harmful hallucinations and know when to ask for help. Our key finding is that calculating and leveraging a scene affordance score, a measure of whether a given action is possible in the provided scene, helps to mitigate hallucinations in LLM predictions and better align the LLM's confidence measure with the probability of success. We specifically propose and test three different affordance scores, which can be used independently or in tandem to improve performance across different use cases. The most successful of these individual scores involves prompting an LLM to determine if a given action is possible and safe in the given scene and uses the LLM's response to compute the score. Through experiments in both simulation and the real world, on tasks with a variety of ambiguities, we show that LAP significantly increases success rate and decreases the amount of human intervention required relative to prior art. For example, in our real-world testing paradigm, LAP decreases the human help rate of previous methods by over 33\% at a success rate of 70\%.},
	language = {en-US},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Mullen Jr., James F. and Manocha, Dinesh},
	month = mar,
	year = {2024},
	note = {GSCC: 0000005 
arXiv:2403.13198 [cs]
Issue: arXiv:2403.13198
remark: 提出了一种减少机器人预测错误的方法LAP。},
	keywords = {Computer Science - Robotics},
}

@article{lin_text2motion_2023,
	title = {{Text2Motion}: {From} {Natural} {Language} {Instructions} to {Feasible} {Plans}},
	volume = {47},
	issn = {0929-5593, 1573-7527},
	shorttitle = {{Text2Motion}},
	url = {http://arxiv.org/abs/2303.12153},
	doi = {10.1007/s10514-023-10131-7},
	abstract = {We propose Text2Motion, a language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Given a natural language instruction, our framework constructs both a task- and motion-level plan that is verified to reach inferred symbolic goals. Text2Motion uses feasibility heuristics encoded in Q-functions of a library of skills to guide task planning with Large Language Models. Whereas previous language-based planners only consider the feasibility of individual skills, Text2Motion actively resolves geometric dependencies spanning skill sequences by performing geometric feasibility planning during its search. We evaluate our method on a suite of problems that require long-horizon reasoning, interpretation of abstract goals, and handling of partial affordance perception. Our experiments show that Text2Motion can solve these challenging problems with a success rate of 82\%, while prior state-of-the-art language-based planning methods only achieve 13\%. Text2Motion thus provides promising generalization characteristics to semantically diverse sequential manipulation tasks with geometric dependencies between skills.},
	language = {en-US},
	number = {8},
	urldate = {2024-01-15},
	journal = {Autonomous Robots},
	author = {Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},
	month = dec,
	year = {2023},
	note = {GSCC: 0000265 
arXiv:2303.12153 [cs]
Number: 8
remark: 一种基于语言的机器人任务规划框架。
TLDR: Text2Motion provides promising generalization characteristics to semantically diverse sequential manipulation tasks with geometric dependencies between skills by actively resolving geometric dependencies spanning skill sequences by performing geometric feasibility planning during its search.},
	keywords = {Computer Science - Robotics},
	pages = {1345--1365},
}

@misc{zeng_socratic_2022,
	title = {Socratic {Models}: {Composing} {Zero}-{Shot} {Multimodal} {Reasoning} with {Language}},
	shorttitle = {Socratic {Models}},
	url = {http://arxiv.org/abs/2204.00598},
	doi = {10.48550/arXiv.2204.00598},
	abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
	language = {en-US},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
	month = may,
	year = {2022},
	note = {GSCC: 0000496 
arXiv:2204.00598 [cs]
remark: Socratic Models
TLDR: Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people.},
	keywords = {\# Socratic Models, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{mu_robocodex_2024,
	title = {{RoboCodeX}: {Multimodal} {Code} {Generation} for {Robotic} {Behavior} {Synthesis}},
	shorttitle = {{RoboCodeX}},
	url = {http://arxiv.org/abs/2402.16117},
	doi = {10.48550/arXiv.2402.16117},
	abstract = {Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.},
	language = {en-US},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Mu, Yao and Chen, Junting and Zhang, Qinglong and Chen, Shoufa and Yu, Qiaojun and Ge, Chongjian and Chen, Runjian and Liang, Zhixuan and Hu, Mengkang and Tao, Chaofan and Sun, Peize and Yu, Haibao and Yang, Chao and Shao, Wenqi and Wang, Wenhai and Dai, Jifeng and Qiao, Yu and Ding, Mingyu and Luo, Ping},
	month = feb,
	year = {2024},
	note = {GSCC: 0000009 
arXiv:2402.16117 [cs]
Issue: arXiv:2402.16117
remark: RoboCodeX框架用于通用机器人行为合成。
TLDR: A tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX, which decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{huang_rekep_2024,
	title = {{ReKep}: {Spatio}-{Temporal} {Reasoning} of {Relational} {Keypoint} {Constraints} for {Robotic} {Manipulation}},
	shorttitle = {{ReKep}},
	url = {http://arxiv.org/abs/2409.01652},
	doi = {10.48550/arXiv.2409.01652},
	abstract = {Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models. Website at https://rekep-robot.github.io/.},
	language = {en-US},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and Fei-Fei, Li},
	month = nov,
	year = {2024},
	note = {GSCC: 0000028 
arXiv:2409.01652 [cs]
TLDR: This work introduces Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation that can employ a hierarchical optimization procedure to solve for robot actions with a perception-action loop at a real-time frequency.
remark: 引入ReKep实现无需标注的机器人操作任务编码。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{gao_physically_2024,
	title = {Physically {Grounded} {Vision}-{Language} {Models} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2309.02561},
	doi = {10.48550/arXiv.2309.02561},
	abstract = {Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.},
	language = {en-US},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
	month = mar,
	year = {2024},
	note = {GSCC: 0000083 
arXiv:2309.02561 [cs]
remark: VLM在物理数据集上微调提升机器人视觉-语言模型的物理理解能力},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{ding_open6dor_2024,
	title = {{Open6DOR}: {Benchmarking} {Open}-instruction 6-{DoF} {Object} {Rearrangement} and {A} {VLM}-based {Approach}},
	shorttitle = {{Open6DOR}},
	url = {https://openreview.net/forum?id=axAmAy3Ghl},
	abstract = {The integration of large-scale Vision-Language Models (VLMs) with embodied AI can greatly enhance the generalizability and capacity to follow open instructions for robots. However, existing studies on object rearrangement are not up to full consideration of the 6-DoF requirements, let alone establishing a comprehensive benchmark. In this paper, we propel the pioneering construction of the benchmark and approach for table-top Open-instruction 6-DoF Object Rearrangement (Open6DOR). Specifically, we collect a synthetic dataset of 200+ objects and carefully design 2400+ Open6DOR tasks. These tasks are divided into the Position-track, Rotation-track, and 6-DoF-track for evaluating different embodied agents in predicting the positions and rotations of target objects. Besides, we also propose a VLM-based approach for Open6DOR, named Open6DOR-GPT, which empowers GPT-4V with 3D-awareness and simulation-assistance and exploits its strengths in generalizability and instruction-following for this task. We compare the existing embodied agents with our Open6DOR-GPT on the proposed Open6DOR benchmark and find that Open6DOR-GPT achieves state-of-the-art performance. We further show the impressive performance of Open6DOR-GPT in diverse real-world experiments. Our constructed benchmark and method will be released upon paper acceptance.},
	language = {en},
	urldate = {2024-06-10},
	author = {Ding, Yufei and Geng, Haoran and Xu, Chaoyi and Fang, Xiaomeng and Zhang, Jiazhao and Dai, Qiyu and Wei, Songlin and Zhang, Zhizheng and Wang, He},
	month = apr,
	year = {2024},
	note = {GSCC: 0000006 
remark: Open6DOR-GPT提升6自由度物体重排性能。},
}

@misc{sarch_open-ended_2023,
	title = {Open-{Ended} {Instructable} {Embodied} {Agents} with {Memory}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.15127},
	doi = {10.48550/arXiv.2310.15127},
	abstract = {Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.},
	language = {en-US},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Sarch, Gabriel and Wu, Yue and Tarr, Michael J. and Fragkiadaki, Katerina},
	month = nov,
	year = {2023},
	note = {GSCC: 0000023 
arXiv:2310.15127 [cs]
remark: HELPER一种通过记忆增强的大型语言模型解析人-机器人对话转化为行动程序的智能体。
TLDR: HELPER is introduced, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mikami_natural_2024,
	title = {Natural {Language} as {Polices}: {Reasoning} for {Coordinate}-{Level} {Embodied} {Control} with {LLMs}},
	shorttitle = {Natural {Language} as {Polices}},
	url = {http://arxiv.org/abs/2403.13801},
	doi = {10.48550/arXiv.2403.13801},
	abstract = {We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.},
	language = {en-US},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Mikami, Yusuke and Melnik, Andrew and Miura, Jun and Hautamäki, Ville},
	month = mar,
	year = {2024},
	note = {GSCC: 0000003 
arXiv:2403.13801 [cs]
Issue: arXiv:2403.13801
remark: 利用自然语言生成机器人控制命令。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, I.2.7, I.2.9},
}

@misc{li_manipllm_2023,
	title = {{ManipLLM}: {Embodied} {Multimodal} {Large} {Language} {Model} for {Object}-{Centric} {Robotic} {Manipulation}},
	shorttitle = {{ManipLLM}},
	url = {http://arxiv.org/abs/2312.16217},
	doi = {10.48550/arXiv.2312.16217},
	abstract = {Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
	language = {en-US},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao},
	month = dec,
	year = {2023},
	note = {GSCC: 0000028 
arXiv:2312.16217 [cs]
Issue: arXiv:2312.16217
remark: 大模型增强机器人抓取泛化能力。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{hu_look_2023,
	title = {Look {Before} {You} {Leap}: {Unveiling} the {Power} of {GPT}-{4V} in {Robotic} {Vision}-{Language} {Planning}},
	shorttitle = {Look {Before} {You} {Leap}},
	url = {http://arxiv.org/abs/2311.17842},
	doi = {10.48550/arXiv.2311.17842},
	abstract = {In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.},
	language = {en-US},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Hu, Yingdong and Lin, Fanqi and Zhang, Tong and Yi, Li and Gao, Yang},
	month = dec,
	year = {2023},
	note = {GSCC: 0000080 
arXiv:2311.17842 [cs]
remark: ViLa利用GPT4-V模型生成一系列可操作步骤，提升了机器人的任务规划能力。
TLDR: This study introduces Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps that demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.},
	keywords = {\# ViLa, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_language-conditioned_2024,
	title = {Language-{Conditioned} {Robotic} {Manipulation} with {Fast} and {Slow} {Thinking}},
	url = {http://arxiv.org/abs/2401.04181},
	doi = {10.48550/arXiv.2401.04181},
	abstract = {The language-conditioned robotic manipulation aims to transfer natural language instructions into executable actions, from simple pick-and-place to tasks requiring intent recognition and visual reasoning. Inspired by the dual process theory in cognitive science, which suggests two parallel systems of fast and slow thinking in human decision-making, we introduce Robotics with Fast and Slow Thinking (RFST), a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types. Our RFST consists of two key components: 1) an instruction discriminator to determine which system should be activated based on the current user instruction, and 2) a slow-thinking system that is comprised of a fine-tuned vision language model aligned with the policy networks, which allows the robot to recognize user intention or perform reasoning tasks. To assess our methodology, we built a dataset featuring real-world trajectories, capturing actions ranging from spontaneous impulses to tasks requiring deliberate contemplation. Our results, both in simulation and real-world scenarios, confirm that our approach adeptly manages intricate tasks that demand intent recognition and reasoning. The project is available at https://jlm-z.github.io/RSFT/},
	language = {en-US},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Zhu, Minjie and Zhu, Yichen and Li, Jinming and Wen, Junjie and Xu, Zhiyuan and Che, Zhengping and Shen, Chaomin and Peng, Yaxin and Liu, Dong and Feng, Feifei and Tang, Jian},
	month = feb,
	year = {2024},
	note = {GSCC: 0000012 
arXiv:2401.04181 [cs]
remark: 快速和慢速思维机器人（RFST）根据指令类型在两个系统上做出决策。},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kwon_language_2023,
	title = {Language {Models} as {Zero}-{Shot} {Trajectory} {Generators}},
	url = {http://arxiv.org/abs/2310.11604},
	doi = {10.48550/arXiv.2310.11604},
	abstract = {Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, code, and prompts are available at: https://www.robot-learning.uk/language-models-trajectory-generators.},
	language = {en-US},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Kwon, Teyun and Di Palo, Norman and Johns, Edward},
	month = oct,
	year = {2023},
	note = {GSCC: 0000029 
arXiv:2310.11604 [cs]
remark: 大型语言模型可生成机器人操作轨迹。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {https://arxiv.org/abs/2201.07207v2},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
	language = {en},
	urldate = {2024-01-10},
	journal = {arXiv.org},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = jan,
	year = {2022},
	note = {GSCC: 0001013 
remark: 利用LLMs的世界知识实现互动环境中的可执行任务计划。},
}

@misc{huang_instruct2act_2023,
	title = {{Instruct2Act}: {Mapping} {Multi}-modality {Instructions} to {Robotic} {Actions} with {Large} {Language} {Model}},
	shorttitle = {{Instruct2Act}},
	url = {http://arxiv.org/abs/2305.11176},
	doi = {10.48550/arXiv.2305.11176},
	abstract = {Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.},
	language = {en-US},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Huang, Siyuan and Jiang, Zhengkai and Dong, Hao and Qiao, Yu and Gao, Peng and Li, Hongsheng},
	month = may,
	year = {2023},
	note = {GSCC: 0000108 
arXiv:2305.11176 [cs]
remark: 利用大型语言模型将多模态指令映射到机器人操作的Instruct2Act框架。
TLDR: This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	doi = {10.48550/arXiv.2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	language = {en-US},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {GSCC: 0000836 
arXiv:2207.05608 [cs]
remark: 利用自然语言反馈，大型语言模型能丰富处理及规划机器人控制场景。
TLDR: This work proposes that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios, and finds that closed-loop language feedback significantly improves high-level instruction completion on three domains.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{huang_grounded_2023,
	title = {Grounded {Decoding}: {Guiding} {Text} {Generation} with {Grounded} {Models} for {Embodied} {Agents}},
	shorttitle = {Grounded {Decoding}},
	url = {http://arxiv.org/abs/2303.00855},
	doi = {10.48550/arXiv.2303.00855},
	abstract = {Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models. The project's website can be found at grounded-decoding.github.io.},
	language = {en-US},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = dec,
	year = {2023},
	note = {GSCC: 0000086 
arXiv:2303.00855 [cs]
remark: LLM与grounded模型结合控制机器人
TLDR: This guided decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shentu_llms_2024,
	title = {From {LLMs} to {Actions}: {Latent} {Codes} as {Bridges} in {Hierarchical} {Robot} {Control}},
	shorttitle = {From {LLMs} to {Actions}},
	url = {http://arxiv.org/abs/2405.04798},
	abstract = {Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. {\textbackslash}method{\textasciitilde}uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that {\textbackslash}method{\textasciitilde}outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.},
	language = {en-US},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Shentu, Yide and Wu, Philipp and Rajeswaran, Aravind and Abbeel, Pieter},
	month = may,
	year = {2024},
	note = {GSCC: 0000004 
arXiv:2405.04798 [cs]
Issue: arXiv:2405.04798
remark: 可学习潜码桥接层提升机器人分层控制。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{cheng_empowering_2024,
	title = {Empowering {Large} {Language} {Models} on {Robotic} {Manipulation} with {Affordance} {Prompting}},
	url = {http://arxiv.org/abs/2404.11027},
	abstract = {While large language models (LLMs) are successful in completing various language processing tasks, they easily fail to interact with the physical world by generating control sequences properly. We find that the main reason is that LLMs are not grounded in the physical world. Existing LLM-based approaches circumvent this problem by relying on additional pre-defined skills or pre-trained sub-policies, making it hard to adapt to new tasks. In contrast, we aim to address this problem and explore the possibility to prompt pre-trained LLMs to accomplish a series of robotic manipulation tasks in a training-free paradigm. Accordingly, we propose a framework called LLM+A(ffordance) where the LLM serves as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences). To ground these plans and control sequences on the physical world, we develop the affordance prompting technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects. Empirically, we evaluate the effectiveness of LLM+A in various language-conditioned robotic manipulation tasks, which show that our approach substantially improves performance by enhancing the feasibility of generated plans and control and can easily generalize to different environments.},
	language = {en-US},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Cheng, Guangran and Zhang, Chuheng and Cai, Wenzhe and Zhao, Li and Sun, Changyin and Bian, Jiang},
	month = apr,
	year = {2024},
	note = {GSCC: 0000007 
arXiv:2404.11027 [cs]
Issue: arXiv:2404.11027
remark: 通过可行性提示提高大型语言模型的机器人操控能力。},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{mu_embodiedgpt_2023,
	title = {{EmbodiedGPT}: {Vision}-{Language} {Pre}-{Training} via {Embodied} {Chain} of {Thought}},
	shorttitle = {{EmbodiedGPT}},
	url = {https://openreview.net/forum?id=IL5zJqfxAa},
	abstract = {Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.},
	language = {en},
	urldate = {2024-06-11},
	author = {Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
	month = nov,
	year = {2023},
	note = {GSCC: 0000176 
remark: EmbodiedGPT提升了机器人多模态理解与执行能力。},
}

@misc{ahn_as_2022,
	title = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
	shorttitle = {Do {As} {I} {Can}, {Not} {As} {I} {Say}},
	url = {https://arxiv.org/abs/2204.01691v2},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
	language = {en},
	urldate = {2024-01-10},
	journal = {arXiv.org},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
	month = apr,
	year = {2022},
	note = {GSCC: 0001398 
remark: 结合语言模型与技能提升机器人任务执行。},
	keywords = {\# Saycan},
}

@misc{huang_copa_2024,
	title = {{CoPa}: {General} {Robotic} {Manipulation} through {Spatial} {Constraints} of {Parts} with {Foundation} {Models}},
	shorttitle = {{CoPa}},
	url = {http://arxiv.org/abs/2403.08248},
	doi = {10.48550/arXiv.2403.08248},
	abstract = {Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/},
	language = {en-US},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Huang, Haoxu and Lin, Fanqi and Hu, Yingdong and Wang, Shengjie and Gao, Yang},
	month = mar,
	year = {2024},
	note = {GSCC: 0000027 
arXiv:2403.08248 [cs]
Issue: arXiv:2403.08248
remark: 利用基础模型中嵌入的常识知识来生成一系列用于开放世界机器人操纵的 6 自由度末端执行器姿势。
TLDR: This work introduces Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation.},
	keywords = {\# Copa, Computer Science - Robotics},
}

@misc{liang_code_2022,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {https://arxiv.org/abs/2209.07753v4},
	abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	language = {en},
	urldate = {2024-01-10},
	journal = {arXiv.org},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = sep,
	year = {2022},
	note = {GSCC: 0000782 
remark: 借助LLM生成策略代码，实现以自然语言指令编写机器人行为策略。},
	keywords = {\# Code as Policies},
}

@misc{zhi_closed-loop_2024,
	title = {Closed-{Loop} {Open}-{Vocabulary} {Mobile} {Manipulation} with {GPT}-{4V}},
	url = {http://arxiv.org/abs/2404.10220},
	doi = {10.48550/arXiv.2404.10220},
	abstract = {Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate ({\textasciitilde}25\%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.},
	language = {en-US},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Zhi, Peiyuan and Zhang, Zhiyuan and Han, Muzhi and Zhang, Zeyu and Li, Zhitian and Jiao, Ziyuan and Jia, Baoxiong and Huang, Siyuan},
	month = apr,
	year = {2024},
	note = {GSCC: 0000015 
arXiv:2404.10220 [cs]
Issue: arXiv:2404.10220
remark: GPT-4V实现移动机器人闭环开放语义操控。
TLDR: ComE-robot is presented, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios, and demonstrates a significant improvement in task success rate compared to state-of-the-art baseline methods.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhao_chat_2023,
	title = {Chat with the {Environment}: {Interactive} {Multimodal} {Perception} {Using} {Large} {Language} {Models}},
	shorttitle = {Chat with the {Environment}},
	url = {http://arxiv.org/abs/2303.08268},
	abstract = {Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. Matcha (Multimodal environment chatting) agent, an interactive perception framework, is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-agent.github.io.},
	language = {en-US},
	urldate = {2023-11-16},
	publisher = {arXiv},
	author = {Zhao, Xufeng and Li, Mengdi and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan},
	month = oct,
	year = {2023},
	note = {GSCC: 0000053 
arXiv:2303.08268 [cs, eess]
Issue: arXiv:2303.08268
remark: 大语言模型助机器人多模态互动感知与任务执行。},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{noauthor_claude_nodate,
	title = {Claude 3.5 {Sonnet}},
	url = {https://www.anthropic.com/claude/sonnet},
	abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
	language = {en},
	urldate = {2024-12-25},
	note = {remark: Claude 3.5 Sonnet},
}

@article{nan_jailbreak_2024,
	title = {Jailbreak {Attack} for {Large} {Language} {Models}: {A} {Survey}},
	volume = {61},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1000-1239},
	shorttitle = {Jailbreak {Attack} for {Large} {Language} {Models}},
	url = {https://crad.ict.ac.cn/en/article/doi/10.7544/issn1000-1239.202330962.pdf},
	doi = {10.7544/issn1000-1239.202330962},
	abstract = {{\textless}p{\textgreater}In recent years, large language models (LLMs) have been widely applied in a range of downstream tasks and have demonstrated remarkable text understanding, generation, and reasoning capabilities in various fields. However, jailbreak attacks are emerging as a new threat to LLMs. Jailbreak attacks can bypass the security mechanisms of LLMs, weaken the influence of safety alignment, and induce harmful outputs from aligned LLMs. Issues such as abuse, hijacking and leakage caused by jailbreak attacks have posed serious threats to both dialogue systems and applications based on LLMs. We present a systematic review of jailbreak attacks in recent years, categorize these attacks into three distinct types based on their underlying mechanism: manually designed attacks, LLM-generated attacks, and optimization-based attacks. We provide a comprehensive summary of the core principles, implementation methods, and research findings derived from relevant studies, thoroughly examine the evolutionary trajectory of jailbreak attacks on LLMs, offering a valuable reference for future research endeavors. Moreover, a concise overview of the existing security measures is offered. It introduces pertinent techniques from the perspectives of internal defense and external defense, which aim to mitigate jailbreak attacks and enhance the content security of LLM generation. Finally, we delve into the existing challenges and frontier directions in the field of jailbreak attacks on LLMs, examine the potential of multimodal approaches, model editing, and multi-agent methodologies in tackling jailbreak attacks, providing valuable insights and research prospects to further advance the field of LLM security.{\textless}/p{\textgreater}},
	language = {zh},
	number = {5},
	urldate = {2024-12-19},
	journal = {Journal of Computer Research and Development},
	author = {Nan, Li and Yidong, Ding and Haoyu, Jiang and Jiafei, Niu and Ping, Yi},
	month = may,
	year = {2024},
	note = {remark: 大型语言模型越狱攻击的综述与安全措施。},
	pages = {1156--1181},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Intelligence} {Security}: {Threats} and {Countermeasures} {\textbar} {ACM} {Computing} {Surveys}},
	url = {https://dl.acm.org/doi/10.1145/3487890},
	language = {en},
	urldate = {2024-02-28},
	note = {remark: AI系统在各阶段面临安全威胁与对策。},
}

@misc{noauthor_isodis_nodate,
	title = {{ISO}/{DIS} 13482},
	url = {https://www.iso.org/standard/83498.html},
	abstract = {Robotics — Safety requirements for service robots},
	language = {en},
	urldate = {2024-12-19},
	journal = {ISO},
}

@misc{noauthor_paralyzing_nodate,
	title = {Paralyzing {Drones} via {EMI} {Signal} {Injection} on {Sensory} {Communication} {Channels}},
	url = {https://www.ndss-symposium.org/ndss-paper/paralyzing-drones-via-emi-signal-injection-on-sensory-communication-channels/},
	language = {en-US},
	urldate = {2024-12-19},
	journal = {NDSS Symposium},
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {CRFM}},
	url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
	language = {en-US},
	urldate = {2024-08-24},
	note = {remark:  Alpaca数据集格式},
}

@misc{noauthor_addon_nodate,
	title = {Addon {Item}},
}

@misc{noauthor_addon_nodate-1,
	title = {Addon {Item}},
	keywords = {⏳},
}

@article{bai_embodied-ai_2024,
	title = {Embodied-{AI} with large models: research and challenges},
	volume = {54},
	issn = {1674-7267},
	shorttitle = {Embodied-{AI} with large models},
	url = {https://www.sciengine.com/10.1360/SSI-2024-0076},
	doi = {10.1360/SSI-2024-0076},
	abstract = {Embodied artificial intelligence (AI) driven by large-scale models is a cross-disciplinary field covering AI, robotics, and cognitive science, focusing on how to combine the perception, reasoning, and logical thinking abilities of large-scale models with embodied AI to improve the data efficiency and generalization ability of existing embodied AI frameworks such as imitation learning, reinforcement learning, and model predictive control. In recent years, with the continuous improvement of the capabilities of large-scale models and the continuous improvement of expert datasets, simulation platforms, and task sets in embodied robots, the combination of large-scale models and embodied AI will become the next wave of AI and is expected to become an important breakthrough for AI to move towards physical robots. This article focuses on the research field of embodied AI driven by large-scale foundation models (LFM), conducting systematic research, analysis, and prospects. Firstly, we review the relevant technical backgrounds of large models and embodied intelligence, as well as the existing learning frameworks of embodied intelligence. Secondly, according to how large models empower embodied intelligence, we divide the existing research into five paradigms: LFM-driven environmental perception, LFM-driven task planning, LFM-driven basic strategy, LFM-driven reward function, and LFM-driven data generation. Finally, we summarize the challenges in existing research, look forward to feasible technical routes, provide references for researchers, and further promote the national AI development strategy.},
	language = {en\_US},
	number = {9},
	urldate = {2024-10-25},
	journal = {SCIENTIA SINICA Informationis},
	author = {Bai, Chenjia and Xu, Huazhe and Li, Xuelong},
	month = aug,
	year = {2024},
	note = {Publisher: Science China Press
remark: 大模型驱动的具身智能研究与挑战。},
	keywords = {⏳},
	pages = {2035},
}

@misc{noauthor_vicuna_nodate,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality} {\textbar} {LMSYS} {Org}},
	shorttitle = {Vicuna},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	abstract = {{\textless}p{\textgreater}We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation ...},
	language = {en},
	urldate = {2024-08-28},
	note = {remark: Vicuna},
}

@misc{noauthor_hello_nodate,
	title = {Hello {GPT}-4o {\textbar} {OpenAI}},
	url = {https://openai.com/index/hello-gpt-4o/},
	language = {en-US},
	urldate = {2024-08-28},
	note = {remark: GPT-4o},
}

@misc{noauthor_gpt-4o_nodate,
	title = {{GPT}-4o mini: advancing cost-efficient intelligence},
	shorttitle = {{GPT}-4o mini},
	url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
	abstract = {Introducing the most cost-efficient small model in the market},
	language = {en-US},
	urldate = {2024-08-28},
	note = {remark: GPT-4o mini},
}
